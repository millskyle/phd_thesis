Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Silver2017,
abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:pdf},
isbn = {3013372370},
issn = {0028-0836},
journal = {Nature},
keywords = {AlphaGo Zero},
mendeley-tags = {AlphaGo Zero},
number = {7676},
pages = {354--359},
pmid = {29052630},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@article{Morningstar2017,
abstract = {It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.},
archivePrefix = {arXiv},
arxivId = {1708.04622},
author = {Morningstar, Alan and Melko, Roger G},
eprint = {1708.04622},
title = {{Deep Learning the Ising Model Near Criticality}},
url = {http://arxiv.org/abs/1708.04622},
year = {2017}
}
@article{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
isbn = {9781450330633},
issn = {10636919},
journal = {arXiv},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
pages = {675--678},
pmid = {18267787},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://arxiv.org/abs/1408.5093},
year = {2014}
}
@article{Jaramillo-Botero2014,
abstract = {First-principles-based force fields prepared from large quantum mechanical data sets are now the norm in predictive molecular dynamics simulations for complex chemical processes, as opposed to force fields fitted solely from phenomenological data. In principle, the former allow improved accuracy and transferability over a wider range of molecular compositions, interactions, and environmental conditions unexplored by experiments. That is, assuming they have been optimally prepared from a diverse training set. The trade-off has been force field engines that are functionally complex, with a large number of nonbonded and bonded analytical forms that give rise to rather large parameter search spaces. To address this problem, we have developed GARFfield (genetic algorithm- based reactive force field optimizer method), a hybrid multiobjective Pareto- optimal parameter development scheme based on genetic algorithms, hill- climbing routines and conjugate-gradient minimization. To demonstrate the capabilities of GARFfield we use it to develop two very different force fields: (1) the ReaxFF reactive force field for modeling the adiabatic reactive dynamics of silicon carbide growth from an methyltrichlorosilane precursor and (2) the SiC electron force field with effective core pseudopotentials for modeling nonadiabatic dynamic phenomena with highly excited electronic states. The flexible and open architecture of GARFfield enables efficient and fast parallel optimization of parameters from quantum mechanical data sets for demanding applications like ReaxFF, electronic fast forward (or electron force field), and others including atomistic reactive charge-optimized many-body interatomic potentials, Morse, and coarse-grain force fields},
author = {Jaramillo-Botero, Andres and Naserifar, Saber and Goddard, William A.},
doi = {10.1021/ct5001044},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Jaramillo-Botero, Naserifar, Goddard - 2014 - General multiobjective force field optimization framework, with application to reactive fo.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {4},
pages = {1426--1439},
pmid = {26580361},
title = {{General multiobjective force field optimization framework, with application to reactive force fields for silicon carbide}},
volume = {10},
year = {2014}
}
@article{Ferdinand1969,
abstract = {The critical-point anomaly of a plane square m×n Ising lattice with periodic boundary conditions (a torus) is analyzed asymptotically in the limit n→∞ with {\$}\xi{\$}=mn fixed. Among other results, it is shown that for fixed {\$}\tau{\$}=n(T−Tc)Tc, the specific heat per spin of a large lattice is given by Cmn(T)kBmn=A0lnn+B({\$}\tau{\$}, {\$}\xi{\$})+B1({\$}\tau{\$})(lnn)n+B2({\$}\tau{\$}, {\$}\xi{\$})n+O[(lnn)3n2], where explicit expressions can be given for A0 and for the functions B, B1, and B2. It follows that the specific-heat peak of the finite lattice is rounded on a scale {\$}\delta{\$}={\$}\Delta{\$}TTc∼1n, while the maximum in Cmn(T) is displaced from Tc by {\$}\epsilon{\$}=(Tc−Tmax)Tc∼1n. For {\$}\xi{\$}0{\textgreater}{\$}\xi{\$}{\textgreater}{\$}\xi{\$}0−1, where {\$}\xi{\$}0=3.13927⋯, the maximum lies above Tc; but for {\$}\xi{\$}{\textgreater}{\$}\xi{\$}0 or {\$}\xi{\$}{\textless}{\$}\xi{\$}0−1, the maximum is depressed below Tc; when {\$}\xi{\$}=∞, {\$}\xi{\$}0, or {\$}\xi{\$}0−1, the relative shift in the maximum from Tc is only of order (lnn)n2. Detailed graphs and numerical data are presented, and the results are compared with some for lattices with free edges. Some heuristic arguments are developed which indicate the possible nature of finite-size critical-point effects in more general systems.},
author = {Ferdinand, Arthur E and Fisher, Michael E},
doi = {10.1103/PhysRev.185.832},
issn = {0031-899X},
journal = {Physical Review},
month = {sep},
number = {2},
pages = {832--846},
pmid = {11268102},
title = {{Bounded and Inhomogeneous Ising Models. I. Specific-Heat Anomaly of a Finite Lattice}},
url = {https://link.aps.org/doi/10.1103/PhysRev.185.832},
volume = {185},
year = {1969}
}
@article{Huang2016a,
abstract = {In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.},
archivePrefix = {arXiv},
arxivId = {1612.04357},
author = {Huang, Xun and Li, Yixuan and Poursaeed, Omid and Hopcroft, John and Belongie, Serge},
eprint = {1612.04357},
month = {dec},
pages = {1--25},
pmid = {202927},
title = {{Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.04357},
year = {2016}
}
@article{Haarnoja2018,
abstract = {Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.},
archivePrefix = {arXiv},
arxivId = {1812.11103},
author = {Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
eprint = {1812.11103},
file = {::},
journal = {arXiv},
month = {dec},
publisher = {arXiv},
title = {{Learning to Walk via Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1812.11103},
year = {2018}
}
@article{stander1994temperature,
author = {Stander, Julian and Silverman, Bernard W},
journal = {Statistics and Computing},
number = {1},
pages = {21--32},
publisher = {Springer},
title = {{Temperature schedules for simulated annealing}},
volume = {4},
year = {1994}
}
@article{Sandfort2019,
abstract = {Labeled medical imaging data is scarce and expensive to generate. To achieve generalizable deep learning models large amounts of data are needed. Standard data augmentation is a method to increase generalizability and is routinely performed. Generative adversarial networks offer a novel method for data augmentation. We evaluate the use of CycleGAN for data augmentation in CT segmentation tasks. Using a large image database we trained a CycleGAN to transform contrast CT images into non-contrast images. We then used the trained CycleGAN to augment our training using these synthetic non-contrast images. We compared the segmentation performance of a U-Net trained on the original dataset compared to a U-Net trained on the combined dataset of original data and synthetic non-contrast images. We further evaluated the U-Net segmentation performance on two separate datasets: The original contrast CT dataset on which segmentations were created and a second dataset from a different hospital containing only non-contrast CTs. We refer to these 2 separate datasets as the in-distribution and out-of-distribution datasets, respectively. We show that in several CT segmentation tasks performance is improved significantly, especially in out-of-distribution (noncontrast CT) data. For example, when training the model with standard augmentation techniques, performance of segmentation of the kidneys on out-of-distribution non-contrast images was dramatically lower than for in-distribution data (Dice score of 0.09 vs. 0.94 for out-of-distribution vs. in-distribution data, respectively, p {\textless} 0.001). When the kidney model was trained with CycleGAN augmentation techniques, the out-of-distribution (non-contrast) performance increased dramatically (from a Dice score of 0.09 to 0.66, p {\textless} 0.001). Improvements for the liver and spleen were smaller, from 0.86 to 0.89 and 0.65 to 0.69, respectively. We believe this method will be valuable to medical imaging researchers to reduce manual segmentation effort and cost in CT imaging.},
author = {Sandfort, Veit and Yan, Ke and Pickhardt, Perry J and Summers, Ronald M},
doi = {10.1038/s41598-019-52737-x},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {16884},
title = {{Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks}},
url = {https://doi.org/10.1038/s41598-019-52737-x},
volume = {9},
year = {2019}
}
@article{VanMilligen1995,
author = {{Van Milligen}, B. Ph and Tribaldos, V. and Jim??nez, J. A.},
doi = {10.1103/PhysRevLett.75.3594},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/van Milligen, Tribaldos, Jim{\'{e}}nez - 1995 - Neural Network Differential Equation and Plasma Equilibrium Solver.pdf:pdf},
issn = {00319007},
journal = {Physical Review Letters},
number = {20},
pages = {3594--3597},
pmid = {10059679},
title = {{Neural network differential equation and plasma equilibrium solver}},
volume = {75},
year = {1995}
}
@article{Srivastava2017,
abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
archivePrefix = {arXiv},
arxivId = {1705.07761},
author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael and Sutton, Charles},
eprint = {1705.07761},
month = {may},
title = {{VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning}},
url = {http://arxiv.org/abs/1705.07761},
year = {2017}
}
@article{Dolgirev2016,
abstract = {We present a new method for a fast, unbiased and accurate representation of inter- atomic interactions. It is a combination of an artificial neural network and our new approach for pair potential reconstruction. The potential reconstruction method is simple and computationally cheap and gives rich information about interactions in crystals. This method can be combined with structure prediction and molecular dynamics simulations, providing accuracy similar to ab initio methods, but at a small fraction of the cost. We present applications to real systems and discuss the insight provided by our method.},
author = {Dolgirev, Pavel E. and Kruglov, Ivan A. and Oganov, Artem R.},
doi = {10.1063/1.4961886},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Dolgirev, Kruglov, Oganov - 2016 - Machine learning scheme for fast extraction of chemically interpretable interatomic potentials.pdf:pdf},
isbn = {0021-9606},
issn = {2158-3226},
journal = {AIP Advances},
month = {aug},
number = {8},
pages = {085318},
title = {{Machine learning scheme for fast extraction of chemically interpretable interatomic potentials}},
url = {http://dx.doi.org/10.1063/1.4961886 http://aip.scitation.org/doi/10.1063/1.4961886},
volume = {6},
year = {2016}
}
@article{Barahona1982,
abstract = {In a spin glass with Ising spins, the problems of computing the magnetic partition function and finding a ground state are studied. In a finite two-dimensional lattice these problems can be solved by algorithms that require a number of steps bounded by a polynomial function of the size of the lattice. In contrast to this fact, the same problems are shown to belong to the class of NP-hard problems, both in the two-dimensional case within a magnetic field, and in the three-dimensional case. NP-hardness of a problem suggests that it is very unlikely that a polynomial algorithm could exist to solve it. {\textcopyright} 1982 The Japan Society of Applied Physics.},
author = {Barahona, F.},
doi = {10.1088/0305-4470/15/10/028},
issn = {0305-4470},
journal = {Journal of Physics A: Mathematical and General},
month = {oct},
number = {10},
pages = {3241--3253},
title = {{On the computational complexity of Ising spin glass models}},
url = {http://stacks.iop.org/0305-4470/15/i=10/a=028?key=crossref.1fe57df6674a7c759374b69321415b44},
volume = {15},
year = {1982}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {Cell Biology,Life Sciences,Mathematical and Computational Biology,general},
month = {dec},
number = {4},
pages = {115--133},
publisher = {Kluwer Academic Publishers},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@article{Nguyen2015,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {427--436},
pmid = {24309266},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
volume = {07-12-June},
year = {2015}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Start with raw spin configurations of a prototypical Ising model, we use principle component analysis to extract relevant low dimensional representations the original data, and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor as indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
issn = {2469-9950},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@techreport{Hoffer2017,
abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance-known as the "generalization gap" phenomenon. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on a random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
pages = {1731--1741},
title = {{Train longer, generalize better: closing the generalization gap in large batch training of neural networks}},
volume = {30},
year = {2017}
}
@article{Mills2017d,
abstract = {We demonstrate that a generative adversarial network can be trained to produce Ising model configurations in distinct regions of phase space. In training a generative adversarial network, the discriminator neural network becomes very good a discerning examples from the training set and examples from the testing set. We demonstrate that this ability can be used as an anomaly detector, producing estimations of operator values along with a confidence in the prediction.},
archivePrefix = {arXiv},
arxivId = {1710.08053},
author = {Mills, Kyle and Tamblyn, Isaac},
eprint = {1710.08053},
file = {::},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{Phase space sampling and operator confidence with generative adversarial networks}},
url = {http://arxiv.org/abs/1710.08053},
year = {2017}
}
@techreport{Salakhutdinova,
abstract = {We present a new learning algorithm for Boltz-mann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer "pre-training" phase that allows variational inference to be initialized with a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
file = {::},
title = {{Deep Boltzmann Machines}}
}
@article{Gharaati2010,
abstract = {A new confinement potential for spherical quantum dots, called the modified Gaussian potential (MGP), is studied. In the present work, the following problems are investigated within the effective-mass approximation: (i) the one-electron energy spectra, (ii) wave functions, (iii) the problem of existence of a bound electron state, and (iv) the binding energy of center and off-center hydrogenic donor impurities. For zero angular momentum (l=0), the new confinement potential is sufficiently flexible to obtain analytically the spectral energy and wave functions. The results obtained from the present work show that (i) the new potential is suitable for predicting the spectral energy and wave functions, and (ii) the geometrical sizes of the quantum dot play the important roles on the energy levels, wave functions, the binding energy, and the existence of a bound electron state. ?? 2010 Elsevier Ltd. All rights reserved.},
author = {Gharaati, A. and Khordad, R.},
doi = {10.1016/j.spmi.2010.06.014},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Gharaati, Khordad - 2010 - A new confinement potential in spherical quantum dots Modified Gaussian potential.pdf:pdf},
issn = {07496036},
journal = {Superlattices and Microstructures},
keywords = {Binding energy,Confinement potential,Energy levels,Quantum dots},
number = {3},
pages = {276--287},
publisher = {Elsevier Ltd},
title = {{A new confinement potential in spherical quantum dots: Modified Gaussian potential}},
url = {http://dx.doi.org/10.1016/j.spmi.2010.06.014},
volume = {48},
year = {2010}
}
@article{Ciresan2010,
abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35{\%} error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.},
author = {Cireşan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/NECO_a_00052},
issn = {0899-7667},
journal = {Neural Computation},
month = {dec},
number = {12},
pages = {3207--3220},
title = {{Deep, Big, Simple Neural Nets for Handwritten Digit Recognition}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/NECO{\_}a{\_}00052},
volume = {22},
year = {2010}
}
@article{Gao2016,
abstract = {Non-covalent interactions (NCIs) play critical roles in supramolecular chemistries; however, they are difficult to measure. Currently, reliable computational methods are being pursued to meet this challenge, but the accuracy of calculations based on low levels of theory is not satisfactory and calculations based on high levels of theory are often too costly. Accordingly, to reduce the cost and increase the accuracy of low-level theoretical calculations to describe NCIs, an efficient approach is proposed to correct NCI calculations based on the benchmark databases S22, S66 and X40 (Hobza in Acc Chem Rev 45: 663–672, 2012; Řez{\'{a}}{\v{c}} et al. in J Chem Theory Comput 8:4285, 2012). A novel type of NCI correction is presented for density functional theory (DFT) methods. In this approach, the general regression neural network machine learning method is used to perform the correction for DFT methods on the basis of DFT calculations. Various DFT methods, including M06-2X, B3LYP, B3LYP-D3, PBE, PBE-D3 and $\omega$B97XD, with two small basis sets (i.e., 6-31G* and 6-31+G*) were investigated. Moreover, the conductor-like polarizable continuum model with two types of solvents (i.e., water and pentylamine, which mimics a protein environment with $\epsilon$ = 4.2) were considered in the DFT calculations. With the correction, the root mean square errors of all DFT calculations were improved by at least 70 {\%}. Relative to CCSD(T)/CBS benchmark values (used as experimental NCI values because of its high accuracy), the mean absolute error of the best result was 0.33 kcal/mol, which is comparable to high-level ab initio methods or DFT methods with fairly large basis sets. Notably, this level of accuracy is achieved within a fraction of the time required by other methods. For all of the correction models based on various DFT approaches, the validation parameters according to OECD principles (i.e., the correlation coefficient R 2, the predictive squared correlation coefficient q 2 and {\$}{\$}q{\_}{\{}cv{\}}{\^{}}{\{}2{\}}{\$}{\$} q c v 2 from cross-validation) were {\textgreater}0.92, which suggests that the correction model has good stability, robustness and predictive power. The correction can be added following DFT calculations. With the obtained molecular descriptors, the NCIs produced by DFT methods can be improved to achieve high-level accuracy. Moreover, only one parameter is introduced into the correction model, which makes it easily applicable. Overall, this work demonstrates that the correction model may be an alternative to the traditional means of correcting for NCIs. Graphical abstract A machine learning correction model efficiently improved the accuracy of non-covalent interactions(NCIs) calculated by DFT methods. The application of the correction model is easy and flexible, so it may be an alternative correction means for NCIs by first-principle calculations.},
author = {Gao, Ting and Li, Hongzhi and Li, Wenze and Li, Lin and Fang, Chao and Li, Hui and Hu, Lihong and Lu, Yinghua and Su, Zhong Min},
doi = {10.1186/s13321-016-0133-7},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Gao et al. - 2016 - A machine learning correction for DFT non-covalent interactions based on the S22, S66 and X40 benchmark databases.pdf:pdf},
issn = {17582946},
journal = {Journal of Cheminformatics},
keywords = {Computational accuracy,Density functional theory,Feature selection,Machine learning correction,Non-covalent interactions},
number = {1},
pages = {24},
pmid = {27148408},
publisher = {Springer International Publishing},
title = {{A machine learning correction for DFT non-covalent interactions based on the S22, S66 and X40 benchmark databases}},
url = {http://jcheminf.springeropen.com/articles/10.1186/s13321-016-0133-7},
volume = {8},
year = {2016}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Arsenault2014,
abstract = {Machine learning methods are applied to finding theGreen's function of the Anderson impurity model, a basic model system of quantum many-body condensed-matter physics.Different methods of parametrizing the Green's function are investigated; a representation in terms of Legendre polynomials is found to be superior due to its limited number of coefficients and its applicability to state of the art methods of solution. The dependence of the errors on the size of the training set is determined. The results indicate that a machine learning approach to dynamical mean-field theory may be feasible.},
archivePrefix = {arXiv},
arxivId = {1408.1143},
author = {Arsenault, Louis Fran{\c{c}}ois and Lopez-Bezanilla, Alejandro and von Lilienfeld, O. Anatole and Millis, Andrew J.},
doi = {10.1103/PhysRevB.90.155136},
eprint = {1408.1143},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Arsenault et al. - 2014 - Machine learning for many-body physics The case of the Anderson impurity model.pdf:pdf},
issn = {1098-0121},
journal = {Physical Review B},
month = {oct},
number = {15},
pages = {155136},
title = {{Machine learning for many-body physics: The case of the Anderson impurity model}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.90.155136},
volume = {90},
year = {2014}
}
@article{Farhi2019,
abstract = {The Quantum Approximate Optimization Algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization problems whose performance can only improve with the number of layers {\$}p{\$}. While QAOA holds promise as an algorithm that can be run on near-term quantum computers, its computational power has not been fully explored. In this work, we study the QAOA applied to the Sherrington-Kirkpatrick (SK) model, which can be understood as energy minimization of {\$}n{\$} spins with all-to-all random signed couplings. There is a recent classical algorithm by Montanari that can efficiently find an approximate solution for a typical instance of the SK model to within {\$}(1-\backslashbackslashepsilon){\$} times the ground state energy, so we can only hope to match its performance with the QAOA. Our main result is a novel technique that allows us to evaluate the typical-instance energy of the QAOA applied to the SK model. We produce a formula for the expected value of the energy, as a function of the {\$}2p{\$} QAOA parameters, in the infinite size limit that can be evaluated on a computer with {\$}O(16{\^{}}p){\$} complexity. We found optimal parameters up to {\$}p=8{\$} running on a laptop. Moreover, we show concentration: With probability tending to one as {\$}n\backslashbackslashto\backslashbackslashinfty{\{}\backslash{\$}{\}}, measurements of the QAOA will produce strings whose energies concentrate at our calculated value. As an algorithm running on a quantum computer, there is no need to search for optimal parameters on an instance-by-instance basis since we can determine them in advance. What we have here is a new framework for analyzing the QAOA, and our techniques can be of broad interest for evaluating its performance on more general problems.},
archivePrefix = {arXiv},
arxivId = {1910.08187},
author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam and Zhou, Leo},
eprint = {1910.08187},
pages = {1--31},
title = {{The Quantum Approximate Optimization Algorithm and the Sherrington-Kirkpatrick Model at Infinite Size}},
url = {http://arxiv.org/abs/1910.08187},
year = {2019}
}
@article{Metropolis1953,
author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
doi = {10.1063/1.1699114},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
month = {jun},
number = {6},
pages = {1087--1092},
title = {{Equation of State Calculations by Fast Computing Machines}},
url = {http://aip.scitation.org/doi/10.1063/1.1699114},
volume = {21},
year = {1953}
}
@article{Izotov2011,
author = {Izotov, P. Yu. and Kazanskiy, N. L. and Golovashkin, D. L. and Sukhanov, S. V.},
doi = {10.3103/S1060992X11020032},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Izotov et al. - 2011 - CUDA-enabled implementation of a neural network algorithm for handwritten digit recognition.pdf:pdf},
issn = {1060-992X},
journal = {Optical Memory and Neural Networks},
keywords = {back propaga,bpe,convolutional neural network,cuda,gpu,method,multiplication of matrices,neural network learning,nvidia,parallel computing,pattern recognition,tion of error},
number = {2},
pages = {98--106},
title = {{CUDA-enabled implementation of a neural network algorithm for handwritten digit recognition}},
url = {http://www.springerlink.com/index/10.3103/S1060992X11020032},
volume = {20},
year = {2011}
}
@article{DellaSala2016,
author = {{Della Sala}, Fabio and Fabiano, Eduardo and Constantin, Lucian A.},
doi = {10.1002/qua.25224},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
keywords = {density functional theory,kinetic energy,meta-GGA},
month = {nov},
number = {22},
pages = {1641--1694},
title = {{Kinetic-energy-density dependent semilocal exchange-correlation functionals}},
url = {http://doi.wiley.com/10.1002/qua.25224},
volume = {116},
year = {2016}
}
@article{Ikeda2019,
abstract = {Quantum annealing is a promising heuristic method to solve combinatorial optimization problems, and efforts to quantify performance on real-world problems provide insights into how this approach may be best used in practice. We investigate the empirical performance of quantum annealing to solve the Nurse Scheduling Problem (NSP) with hard constraints using the D-Wave 2000Q quantum annealing device. NSP seeks the optimal assignment for a set of nurses to shifts under an accompanying set of constraints on schedule and personnel. After reducing NSP to a novel Ising-type Hamiltonian, we evaluate the solution quality obtained from the D-Wave 2000Q against the constraint requirements as well as the diversity of solutions. For the test problems explored here, our results indicate that quantum annealing recovers satisfying solutions for NSP and suggests the heuristic method is potentially achievable for practical use. Moreover, we observe that solution quality can be greatly improved through the use of reverse annealing, in which it is possible to refine returned results by using the annealing process a second time. We compare the performance of NSP using both forward and reverse annealing methods and describe how this approach might be used in practice.},
archivePrefix = {arXiv},
arxivId = {1904.12139},
author = {Ikeda, Kazuki and Nakamura, Yuma and Humble, Travis S.},
doi = {10.1038/s41598-019-49172-3},
eprint = {1904.12139},
isbn = {4159801949172},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--10},
title = {{Application of Quantum Annealing to Nurse Scheduling Problem}},
volume = {9},
year = {2019}
}
@article{Farhi2014,
abstract = {We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.},
archivePrefix = {arXiv},
arxivId = {1411.4028},
author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
eprint = {1411.4028},
pages = {1--16},
title = {{A Quantum Approximate Optimization Algorithm}},
url = {http://arxiv.org/abs/1411.4028},
year = {2014}
}
@article{Onsager1944,
abstract = {The partition function of a two-dimensional "ferromagnetic" with scalar "spins" (Ising model) is computed rigorously for the case of vanishing field. The eigenwert problem involved in the corresponding computation for a long strip crystal of finite width (n atoms), joined straight to itself around a cylinder, is solved by direct product decomposition; in the special case n=∞ an integral replaces a sum. The choice of different interaction energies (±J,±J′) in the (0 1) and (1 0) directions does not complicate the problem. The two-way infinite crystal has an order-disorder transition at a temperature T=Tc given by the condition sinh(2J/kTc) sinh(2J′/kTc)=1. The energy is a continuous function of T; but the specific heat becomes infinite as -log |T-Tc|. For strips of finite width, the maximum of the specific heat increases linearly with log n. The order-converting dual transformation invented by Kramers and Wannier effects a simple automorphism of the basis of the quaternion algebra which is natural to the problem in hand. In addition to the thermodynamic properties of the massive crystal, the free energy of a (0 1) boundary between areas of opposite order is computed; on this basis the mean ordered length of a strip crystal is (exp (2J/kT) tanh(2J′/kT))n.},
author = {Onsager, Lars},
doi = {10.1103/PhysRev.65.117},
isbn = {0031-899X{\$}\backslash{\$}r1536-6065},
issn = {0031899X},
journal = {Physical Review},
number = {3-4},
pages = {117--149},
pmid = {18556531},
title = {{Crystal statistics. I. A two-dimensional model with an order-disorder transition}},
volume = {65},
year = {1944}
}
@article{Aramon2019,
abstract = {The Fujitsu Digital Annealer is designed to solve fully connected quadratic unconstrained binary optimization (QUBO) problems. It is implemented on application-specific CMOS hardware and currently solves problems of up to 1,024 variables. The Digital Annealer's algorithm is currently based on simulated annealing; however, it differs from it in its utilization of an efficient parallel-trial scheme and a dynamic escape mechanism. In addition, the Digital Annealer exploits the massive parallelization that custom application-specific CMOS hardware allows. We compare the performance of the Digital Annealer to simulated annealing and parallel tempering with isoenergetic cluster moves on two-dimensional and fully connected spin-glass problems with bimodal and Gaussian couplings. These represent the respective limits of sparse vs. dense problems, as well as high-degeneracy vs. low-degeneracy problems. Our results show that the Digital Annealer currently exhibits a time-to-solution speedup of roughly two orders of magnitude for fully connected spin-glass problems with bimodal or Gaussian couplings, over the single-core implementations of simulated annealing and parallel tempering Monte Carlo used in this study. The Digital Annealer does not appear to exhibit a speedup for sparse two-dimensional spin-glass problems, which we explain on theoretical grounds. We also benchmarked an early implementation of the Parallel Tempering Digital Annealer. Our results suggest an improved scaling over the other algorithms for fully connected problems of average difficulty with bimodal disorder. The next generation of the Digital Annealer is expected to be able to solve fully connected problems up to 8,192 variables in size. This would enable the study of fundamental physics problems and industrial applications that were previously inaccessible using standard computing hardware or special-purpose quantum annealing machines.},
archivePrefix = {arXiv},
arxivId = {1806.08815},
author = {Aramon, Maliheh and Rosenberg, Gili and Valiante, Elisabetta and Miyazawa, Toshiyuki and Tamura, Hirotaka and Katzgraber, Helmut G},
doi = {10.3389/fphy.2019.00048},
eprint = {1806.08815},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Benchmarking,Custom application-specific CMOS hardware,Digital Annealer,Monte Carlo simulation,Optimization},
number = {APR},
title = {{Physics-inspired optimization for quadratic unconstrained problems using a digital Annealer}},
volume = {7},
year = {2019}
}
@techreport{adadelta,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information , different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701v1},
author = {Zeiler, Matthew D},
eprint = {1212.5701v1},
file = {::},
keywords = {Gradient Descent,Index Terms-Adaptive Learning Rates,Machine Learn-ing,Neural Networks},
title = {{ADADELTA: AN ADAPTIVE LEARNING RATE METHOD}}
}
@article{Manzhos2009,
abstract = {We propose a neural network (NN) based algorithm for calculating vibrational energies and wave functions and apply it to problems in 2-, 4-, and 6-dimensions. By using neurons as basis functions and methods of nonlinear optimization, we are able to compute three states of a 6-D Hamiltonian using only 50 basis functions. In a standard direct product basis, thousands of basis functions would be necessary. Previous NN methods for solving the Schrodinger equation computed one level at a time and optimized all of the parameters using expensive nonlinear optimization methods. Using our approach, linear coefficients in the NN representation of wave functions are determined with methods of linear algebra and many levels are computed at the same time from one set of nonlinear NN parameters. In addition, we use radial basis function neurons to ensure the correct boundary conditions. The use of linear algebra methods makes it possible to treat systems of higher dimensionality.},
author = {Manzhos, S and Carrington, T},
doi = {10.1139/v09-025},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Manzhos, Carrington - 2009 - An improved neural network method for solving the Schrodinger equation(1).pdf:pdf},
isbn = {0008-4042},
issn = {0008-4042},
journal = {Canadian Journal of Chemistry-Revue Canadienne De Chimie},
keywords = {bounds,distributed gaussian bases,filter-diagonalization,lanczos,molecules,multilayer feedforward networks,neural networks,numerical methods,numerical-solution,universal approximation,vibrational spectroscopy,vibrational-energy levels,wave-functions},
pages = {864--871},
title = {{An improved neural network method for solving the Schrodinger equation(1)}},
volume = {87},
year = {2009}
}
@article{Hastings1970,
author = {Hastings, B Y W K},
pages = {97--109},
title = {{Monte Carlo sampling methods using Markov chains and their applications}},
year = {1970}
}
@article{Hen2012,
abstract = {We propose a method using a quantum annealer-an analog quantum computer based on the principles of quantum adiabatic evolution-to solve the graph isomorphism problem, in which one has to determine whether two graphs are isomorphic (i.e., can be transformed into each other simply by a relabeling of the vertices). We demonstrate the capabilities of the method by analyzing several types of graph families, focusing on graphs with particularly high symmetry called strongly regular graphs. We also show that our method is applicable, within certain limitations, to currently available quantum hardware such as D-Wave One. {\textcopyright} 2012 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1207.1712},
author = {Hen, Itay and Young, A. P.},
doi = {10.1103/PhysRevA.86.042310},
eprint = {1207.1712},
issn = {10502947},
journal = {Physical Review A - Atomic, Molecular, and Optical Physics},
number = {4},
pages = {1--8},
title = {{Solving the graph-isomorphism problem with a quantum annealer}},
volume = {86},
year = {2012}
}
@article{Linnainmaa1976,
abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed. {\textcopyright} 1976 BIT Foundations.},
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
issn = {15729125},
journal = {BIT},
keywords = {Computational Mathematics and Numerical Analysis,Mathematics,Numeric Computing,general},
number = {2},
pages = {146--160},
publisher = {Kluwer Academic Publishers},
title = {{Taylor expansion of the accumulated rounding error}},
url = {https://link.springer.com/article/10.1007/BF01931367},
volume = {16},
year = {1976}
}
@article{HohenbergKohn,
abstract = {This paper deals with the ground state of an interacting electron gas in an external potential v(r). It is proved that there exists a universal functional of the density, F[n(r)], independent of v(r), such that the expression E≡∫v(r)n(r)dr+F[n(r)] has as its minimum value the correct ground-state energy associated with v(r). The functional F[n(r)] is then discussed for two situations: (1) n(r)=n0+ñ(r), ñ/n0≪1, and (2) n(r)=ϕ(r/r0) with ϕ arbitrary and r0→∞. In both cases F can be expressed entirely in terms of the correlation energy and linear and higher order electronic polarizabilities of a uniform electron gas. This approach also sheds some light on generalized Thomas-Fermi methods and their limitations. Some new extensions of these methods are presented.},
author = {Hohenberg, P. and Kohn, W.},
doi = {10.1103/PhysRev.136.B864},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Hohenberg, Kohn - 1964 - Inhomogeneous Electron Gas(2).pdf:pdf},
isbn = {0163-1829},
issn = {0031-899X},
journal = {Physical Review},
month = {nov},
number = {3B},
pages = {B864--B871},
pmid = {14995397},
title = {{Inhomogeneous Electron Gas}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.7.1912 https://link.aps.org/doi/10.1103/PhysRev.136.B864},
volume = {136},
year = {1964}
}
@article{Luchak2017,
abstract = {We present a procedure for training and evaluating a deep neural network which can efficiently infer extensive parameters of arbitrarily large systems, doing so with O(N) complexity. We use a form of domain decomposition for training and inference, where each sub-domain (tile) is comprised of a non-overlapping focus region surrounded by an overlapping context region. The relative sizes of focus and context are physically motivated and depend on the locality length scale of the prob-lem. Extensive deep neural networks (EDNN) are a formulation of convolutional neural networks which provide a flexible and general approach, based on physical constraints, to describe multi-scale interactions. They are well suited to massively parallel inference, as no inter-thread communication is necessary during evaluation. Example uses for learning simple spin models, Laplacian (deriva-tive) operator, and approximating many-body quantum mechanical operators (within the density functional theory approach) are demonstrated.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.06686},
author = {Luchak, I and Mills, K and Ryczko, K and Domurad, A and Tamblyn, I},
eprint = {arXiv:1708.06686},
journal = {arXiv},
title = {{Extensive deep neural networks}},
url = {https://arxiv.org/pdf/1708.06686.pdf},
year = {2017}
}
@article{Schutt2017,
author = {Sch{\"{u}}tt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and M{\"{u}}ller, Klaus R. and Tkatchenko, Alexandre},
doi = {10.1038/ncomms13890},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{u}}tt et al. - 2017 - Quantum-chemical insights from deep tensor neural networks.pdf:pdf},
issn = {2041-1723},
journal = {Nature Communications},
month = {jan},
pages = {13890},
pmid = {28067221},
title = {{Quantum-chemical insights from deep tensor neural networks}},
url = {http://www.nature.com/doifinder/10.1038/ncomms13890},
volume = {8},
year = {2017}
}
@article{Snyder2013,
abstract = {Using a one-dimensional model, we explore the ability of machine learning to approximate the non-interacting kinetic energy density functional of diatomics. This nonlinear interpolation between Kohn-Sham reference calculations can (i) accurately dissociate a diatomic, (ii) be systematically improved with increased reference data and (iii) generate accurate self-consistent densities via a projection method that avoids directions with no data. With relatively few densities, the error due to the interpolation is smaller than typical errors in standard exchange-correlation functionals.},
archivePrefix = {arXiv},
arxivId = {1306.1812},
author = {Snyder, John C. and Rupp, Matthias and Hansen, Katja and Blooston, Leo and M{\"{u}}ller, Klaus Robert and Burke, Kieron},
doi = {10.1063/1.4834075},
eprint = {1306.1812},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Snyder et al. - 2013 - Orbital-free bond breaking via machine learning.pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {22},
pmid = {24329053},
title = {{Orbital-free bond breaking via machine learning}},
volume = {139},
year = {2013}
}
@article{Tiunov2019,
abstract = {The coherent Ising machine (CIM) enables efficient sampling of low-lying energy states of the Ising Hamiltonian with all-to-all connectivity by encoding the spins in the amplitudes of pulsed modes in an optical parametric oscillator (OPO). The interaction between the pulses is realized by means of measurement-based optoelectronic feedforward, which enhances the gain for lower-energy spin configurations. We present an efficient method of simulating the CIM on a classical computer that outperforms the CIM itself, as well as the noisy mean-field annealer in terms of both the quality of the samples and the computational speed. It is furthermore advantageous with respect to the CIM in that it can handle Ising Hamiltonians with arbitrary real-valued node coupling strengths. These results illuminate the nature of the faster performance exhibited by the CIM and may give rise to a new class of quantum-inspired algorithms of classical annealing that can successfully compete with existing methods.},
author = {Tiunov, Egor S and Ulanov, Alexander E and Lvovsky, A I},
doi = {10.1364/OE.27.010288},
journal = {Opt. Express},
keywords = {Computer simulation; Continuous variables; Neural},
month = {apr},
number = {7},
pages = {10288--10295},
publisher = {OSA},
title = {{Annealing by simulating the coherent Ising machine}},
url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-27-7-10288},
volume = {27},
year = {2019}
}
@article{Castro2000,
abstract = {In 1989 Hornik as well as Funahashi established that multilayer feedforward networks without the squashing function in the output layer are universal approximators. This result has been often used improperly because it has been applied to multilayer feedforward networks with the squashing function in the output layer. In this paper, we will prove that also this kind of neural networks are universal approximators, i.e. they are capable of approximating any Borel measurable function from one finite dimensional space into (0,1)(n) to any desired degree of accuracy, provided sufficiently many hidden units are available. (C) 2000 Elsevier Science Ltd.},
author = {Castro, J. L. and Mantas, C. J. and Ben{\'{i}}tez, J. M.},
doi = {10.1016/S0893-6080(00)00031-9},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Castro, Mantas, Ben{\'{i}}tez - 2000 - Neural networks with a continuous squashing function in the output are universal approximators.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Continuous functions,Feedforward networks,Squashing functions,Universal approximation},
number = {6},
pages = {561--563},
pmid = {10987509},
title = {{Neural networks with a continuous squashing function in the output are universal approximators}},
volume = {13},
year = {2000}
}
@article{Gomez2008,
abstract = {We have performed Hartree-Fock calculations of electronic structure of N $\backslash$le 10 electrons in a quantum dot modeled with a confining Gaussian potential well. We discuss the conditions for the stability of N bound electrons in the system. We show that the most relevant parameter determining the number of bound electrons is {\$}V{\_}0 R{\^{}}2{\$}. Such a property arises from widely valid scaling properties of the con ning potential. Gaussian Quantum dots having N = 2, 5 and 8 electrons are particularly stable in agreement with Hund rule. The shell structure becomes less and less noticeable as the well radius increases.},
archivePrefix = {arXiv},
arxivId = {0804.1961},
author = {Gomez, Sergio S. and Romero, Rodolfo H.},
doi = {10.2478/s11534-008-0132-z},
eprint = {0804.1961},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Gomez, Romero - 2008 - Few-electron semiconductor quantum dots with Gaussian confinement.pdf:pdf},
issn = {1895-1082},
keywords = {berlin heidelberg,electronic structure,gaussian potential,quantum dots,versita warsaw and springer-verlag},
number = {3400},
pages = {14},
title = {{Few-electron semiconductor quantum dots with Gaussian confinement}},
url = {http://arxiv.org/abs/0804.1961},
volume = {5500},
year = {2008}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, Juergen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf},
month = {apr},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://www.idsia.ch/˜juergen/DeepLearning8Oct2014.texCompleteBIBTEXfile http://arxiv.org/abs/1404.7828 http://dx.doi.org/10.1016/j.neunet.2014.09.003},
year = {2014}
}
@article{Mills2017a,
archivePrefix = {arXiv},
arxivId = {1706.09779v1},
author = {Mills, K and Tamblyn, I},
eprint = {1706.09779v1},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills, Tamblyn - 2017 - Deep neural networks for direct, featureless learning through observation the case of 2d spin models(2).pdf:pdf},
title = {{Deep neural networks for direct, featureless learning through observation: the case of 2d spin models}},
year = {2017}
}
@article{Gal2015a,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
isbn = {1506.02142},
issn = {1938-7228},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
volume = {48},
year = {2015}
}
@article{Lecun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L??on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@article{Ludwig2007,
abstract = {We outline a hybrid multiscale approach for the construction of ab initio potential energy surfaces (PESs) useful for performing six-dimensional (6D) classical or quantum mechanical molecular dynamics (MD) simulations of diatomic molecules reacting at single crystal surfaces. The algorithm implements concepts from the corrugation reduction procedure, which reduces energetic variation in the PES, and uses neural networks for interpolation of smoothed ab initio data. A novelty sampling scheme is implemented and used to identify configurations that are most likely to be predicted inaccurately by the neural network. This hybrid multiscale approach, which couples PES construction at the electronic structure level to MD simulations at the atomistic scale, reduces the number of density functional theory (DFT) calculations needed to specify an accurate PES. Due to the iterative nature of the novelty sampling algorithm, it is possible to obtain a quantitative measure of the convergence of the PES with respect to the number of ab initio calculations used to train the neural network. We demonstrate the algorithm by first applying it to two analytic potentials, which model the H2/Pt(111) and H2/Cu(111) systems. These potentials are of the corrugated London-Eyring-Polanyi-Sato form, which are based on DFT calculations, but are not globally accurate. After demonstrating the convergence of the PES using these simple potentials, we use DFT calculations directly and obtain converged semiclassical trajectories for the H2/Pt(111) system at the PW91/generalized gradient approximation level. We obtain a converged PES for a 6D hydrogen-surface dissociation reaction using novelty sampling coupled directly to DFT. These results, in excellent agreement with experiments and previous theoretical work, are compared to previous simulations in order to explore the sensitivity of the PES (and therefore MD) to the choice of exchange and correlation functional. Despite having a lower energetic corrugation in our PES, we obtain a broader reaction probability curve than previous simulations, which is attributed to increased geometric corrugation in the PES and the effect of nonparallel dissociation pathways.},
author = {Ludwig, Jeffery and Vlachos, Dionisios G},
doi = {10.1063/1.2794338},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ludwig, Vlachos - 2007 - Ab initio molecular dynamics of hydrogen dissociation on metal surfaces using neural networks and novelty sampl.pdf:pdf},
issn = {0021-9606},
journal = {The Journal of chemical physics},
number = {15},
pages = {154716},
pmid = {17949200},
title = {{Ab initio molecular dynamics of hydrogen dissociation on metal surfaces using neural networks and novelty sampling.}},
volume = {127},
year = {2007}
}
@article{Baldi2014,
abstract = {Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification problems, hence machine-learning approaches are often used. Standard approaches have relied on 'shallow' machine-learning models that have a limited capacity to learn complex nonlinear functions of the inputs, and rely on a painstaking search through manually constructed nonlinear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Here, using benchmark data sets, we show that deep-learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8{\%} over the best current approaches. This demonstrates that deep-learning approaches can improve the power of collider searches for exotic particles.},
archivePrefix = {arXiv},
arxivId = {1402.4735},
author = {Baldi, P and Sadowski, P and Whiteson, D},
doi = {10.1038/ncomms5308},
eprint = {1402.4735},
isbn = {2041-1723 (Electronic)2041-1723 (Linking)},
issn = {2041-1723},
journal = {Nature communications},
pages = {4308},
pmid = {24986233},
publisher = {Nature Publishing Group},
title = {{Searching for exotic particles in high-energy physics with deep learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24986233},
volume = {5},
year = {2014}
}
@techreport{Kingma,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differ-entiability conditions, even works in the intractable case. Our contributions is twofold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114v10},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114v10},
file = {::},
title = {{Auto-Encoding Variational Bayes}}
}
@misc{SwiftKey2015,
author = {SwiftKey},
booktitle = {SwiftKey Blog},
title = {{Introducing the world's first neural network keyboard}},
url = {https://blog.swiftkey.com/neural-networks-a-meaningful-leap-for-mobile-typing/},
year = {2015}
}
@book{deeplearningbook,
annote = {$\backslash$url{\{}http://www.deeplearningbook.org{\}}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@article{Yao2016,
abstract = {We demonstrate a convolutional neural network trained to reproduce the Kohn-Sham kinetic energy of hydrocarbons from electron density. The output of the network is used as a non-local correction to the conventional local and semi-local kinetic functionals. We show that this approximation qualitatively reproduces Kohn-Sham potential energy surfaces when used with conventional exchange correlation functionals. Numerical noise inherited from the non-linearity of the neural network is identified as the major challenge for the model. Finally we examine the features in the density learned by the neural network to anticipate the prospects of generalizing these models.},
archivePrefix = {arXiv},
arxivId = {1509.00062},
author = {Yao, Kun and Parkhill, John},
doi = {10.1021/acs.jctc.5b01011},
eprint = {1509.00062},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Yao, Parkhill - 2016 - Kinetic Energy of Hydrocarbons as a Function of Electron Density and Convolutional Neural Networks.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {3},
pages = {1139--1147},
pmid = {26812530},
title = {{Kinetic Energy of Hydrocarbons as a Function of Electron Density and Convolutional Neural Networks}},
volume = {12},
year = {2016}
}
@article{Haarnoja2018a,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
archivePrefix = {arXiv},
arxivId = {1812.05905},
author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
eprint = {1812.05905},
file = {::},
journal = {arXiv},
month = {dec},
publisher = {arXiv},
title = {{Soft Actor-Critic Algorithms and Applications}},
url = {http://arxiv.org/abs/1812.05905},
year = {2018}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashbackslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashbackslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashbackslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
eprint = {1703.10593},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@article{Kirkpatrick1984,
abstract = {Introduction: For detecting mould damage in indoor areas different methods are used. In contrast to traditional microbiological techniques the particular qualification of MVOC measurements was postulated. Thereyby the ability of MVOC to permeate barriers like wallpaper, panelling etc. was considered as the main advantage of this method. Aim of the study: A single-blinded study was performed to analyse whether indoor environments with and without mould infestation can be differentiated by MVOC analysis. Methods: In 40 dwellings with and in 44 dwellings without mould damage the concentrations of 8 selected MVOC considered as indicative, climatic parameters, airborne particles and the air exchange rate were determined and the characteristics of the dwellings were recorded. All collected data were analysed by multiple regression analysis. Results: For most of the analysed MVOC there was no correlation between the respective concentrations and the mould state. Only, for 2-methyl-1-butanol and 1-octen-3-ol a statistically significant, but weak correlation with the presence of a mould infestation was found. The concentrations of the "M"WOC were mainly influenced by other factors. 2-Methylfuran and 3-methylfuran, often used as main indicators for mould damage, had a highly significant correlation with the smoking state. The concentration of these compounds were also significantly correlated with the absolute humidity and the air exchange rate. Concerning 3-methyl-1-butanol, 2-hexanone, 3-heptanone and dimethyldisulphide there were found only weak correlations with other parameters, the air humidity being the strongest influencing factor. Discussion and conclusion: Based on the results of the study the MVOC investigated cannot be considered as mould indicators, since the parameter "mould infestation of the dwelling" explains only a small part of the total variability of the MVOC in the regression model. The major part of the total variability originated from unknown factors or could be explained in part by other factors included in the study as smoking state, air exchange rate, interior equipment etc. In spite of the statistically significant associations with 2-methyl-l-butanol and 1-octen-3-ol and the mould presence in the dwelling these compounds cannot be considered as indicator compounds, as only a small portion (10{\%} in this case) of the total variability can be attributed to the mould state. Such minor correlations lead to an excessive part of incorrect classifications, meaning that sensivity and specificity of these compounds are too low. Obviously the MVOC are not specific to bacteria or moulds, but also have other chemical and biological sources. The low specific emission rates found in lab tests are furthermore restricting the value of the MVOC as indicators.},
author = {Kirkpatrick, Scott},
doi = {10.1007/BF01009452},
issn = {0022-4715},
journal = {Journal of Statistical Physics},
keywords = {Indicator substances,Indoor moulds,MVOC,Microbial volatile organic compounds,Mouldy homes},
month = {mar},
number = {5-6},
pages = {975--986},
title = {{Optimization by simulated annealing: Quantitative studies}},
url = {http://link.springer.com/10.1007/BF01009452},
volume = {34},
year = {1984}
}
@article{Behler2014,
abstract = {The development of interatomic potentials employing artificial neural networks has seen tremendous progress in recent years. While until recently the applicability of neural network potentials (NNPs) has been restricted to low-dimensional systems, this limitation has now been overcome and high-dimensional NNPs can be used in large-scale molecular dynamics simulations of thousands of atoms. NNPs are constructed by adjusting a set of parameters using data from electronic structure calculations, and in many cases energies and forces can be obtained with very high accuracy. Therefore, NNP-based simulation results are often very close to those gained by a direct application of first-principles methods. In this review, the basic methodology of high-dimensional NNPs will be presented with a special focus on the scope and the remaining limitations of this approach. The development of NNPs requires substantial computational effort as typically thousands of reference calculations are required. Still, if the problem to be studied involves very large systems or long simulation times this overhead is regained quickly. Further, the method is still limited to systems containing about three or four chemical elements due to the rapidly increasing complexity of the configuration space, although many atoms of each species can be present. Due to the ability of NNPs to describe even extremely complex atomic configurations with excellent accuracy irrespective of the nature of the atomic interactions, they represent a general and therefore widely applicable technique, e.g. for addressing problems in materials science, for investigating properties of interfaces, and for studying solvation processes.},
author = {Behler, J{\"{o}}rg},
doi = {10.1088/0953-8984/26/18/183001},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Behler - 2014 - Representing potential energy surfaces by high-dimensional neural network potentials.pdf:pdf},
isbn = {0953-8984},
issn = {0953-8984},
journal = {Journal of Physics: Condensed Matter},
keywords = {in colour only in,interatomic potentials,molecular dynamics,neural networks,some figures may appear,the online journal},
number = {18},
pages = {183001},
pmid = {24758952},
title = {{Representing potential energy surfaces by high-dimensional neural network potentials}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24758952{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/24758952{\%}5Cnhttp://iopscience.iop.org/0953-8984/26/18/183001{\%}5Cnhttp://stacks.iop.org/0953-8984/26/i=18/a=183001?key=crossref.e2110f4e5f0e5ce5600d2eb9e27e4391},
volume = {26},
year = {2014}
}
@techreport{Huang,
abstract = {Deep learning based image-to-image translation methods aim at learning the joint distribution of the two domains and finding transformations between them. Despite recent GAN (Generative Adversarial Network) based methods have shown compelling results, they are prone to fail at preserving image-objects and maintaining translation consistency , which reduces their practicality on tasks such as generating large-scale training data for different domains. To address this problem, we purpose a structure-aware image-to-image translation network, which is composed of encoders, generators, discriminators and parsing nets for the two domains, respectively, in a unified framework. The purposed network generates more visually plausible images compared to competing methods on different image-translation tasks. In addition, we quantitatively evaluate different methods by training Faster-RCNN and YOLO with datasets generated from the image-translation results and demonstrate significant improvement on the detection accuracies by using the proposed image-object preserving network.},
author = {Huang, Sheng-Wei and Lin, Che-Tsung and Chen, Shu-Ping and Wu, Yen-Yi and Hsu, Po-Hao and Lai, Shang-Hong},
file = {::},
keywords = {Generative adversarial network,domain adaptation,image-to-image translation,object detection,semantic segmentation},
title = {{AugGAN: Cross Domain Adaptation with GAN-based Data Augmentation}}
}
@article{Ramakrishnan2015,
abstract = {Due to its favorable computational efficiency time-dependent (TD) density functional theory (DFT) enables the prediction of electronic spectra in high-throughput fashion across chemical space. Unfortunately, its predictions can be inaccurate. Machine learning models can resolve this issue when trained on deviations of reference coupled-cluster singles and doubles (CC2) spectra from TDDFT excitation energies, or even from DFT gap. Numerical evidence is produced for the low-lying singlet- singlet vertical electronic spectra of over 20 thousand diverse and synthetically feasible organic molecules with up to eight CONF atoms. Out-of-sample prediction errors decay monotonously as a function of training set size. For a training set of 10 thousand molecules, CC2 excitation energies can be reproduced within ±0.1 eV. Analysis of our spectral database with chromophore counting suggests that even higher accuracies can be achieved. We discuss open challenges associated with data-driven modeling of transition intensities.},
archivePrefix = {arXiv},
arxivId = {1504.01966},
author = {Ramakrishnan, Raghunathan and Hartmann, Mia and Tapavicza, Enrico and von Lilienfeld, O. Anatole},
doi = {10.1063/1.4928757},
eprint = {1504.01966},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ramakrishnan et al. - 2015 - Electronic Spectra from TDDFT and Machine Learning in Chemical Space.pdf:pdf},
issn = {0021-9606},
journal = {arXiv preprint arXiv:1504.01966},
number = {2015},
pages = {6},
pmid = {26328822},
title = {{Electronic Spectra from TDDFT and Machine Learning in Chemical Space}},
url = {http://dx.doi.org/10.1063/1.4928757},
volume = {084111},
year = {2015}
}
@article{Boixo2014,
abstract = {Quantum technology is maturing to the point where quantum devices, such as quantum communication systems, quantum random number generators and quantum simulators may be built with capabilities exceeding classical computers. A quantum annealer, in particular, solves optimization problems by evolving a known initial configuration at non-zero temperature towards the ground state of a Hamiltonian encoding a given problem. Here, we present results from tests on a 108 qubit D-Wave One device based on superconducting flux qubits. By studying correlations we find that the device performance is inconsistent with classical annealing or that it is governed by classical spin dynamics. In contrast, we find that the device correlates well with simulated quantum annealing. We find further evidence for quantum annealing in the form of small-gap avoided level crossings characterizing the hard problems. To assess the computational power of the device we compare it against optimized classical algorithms. {\textcopyright} 2014 Macmillan Publishers Limited.},
archivePrefix = {arXiv},
arxivId = {1304.4595},
author = {Boixo, Sergio and R{\o}nnow, Troels F. and Isakov, Sergei V. and Wang, Zhihui and Wecker, David and Lidar, Daniel A. and Martinis, John M. and Troyer, Matthias},
doi = {10.1038/nphys2900},
eprint = {1304.4595},
issn = {17452481},
journal = {Nature Physics},
number = {3},
pages = {218--224},
title = {{Evidence for quantum annealing with more than one hundred qubits}},
volume = {10},
year = {2014}
}
@techreport{Hinton,
author = {Hinton, G E and Srivastava, N and Krizhevsky, A and Sutskever, I and Salakhutdinov, R R},
file = {::},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}}
}
@article{Perez2007,
abstract = {We present here the parmbsc0 force field, a refinement of the AMBER parm99 force field, where emphasis has been made on the correct representation of the alpha/gamma concerted rotation in nucleic acids (NAs). The modified force field corrects overpopulations of the alpha/gamma = (g+,t) backbone that were seen in long (more than 10 ns) simulations with previous AMBER parameter sets (parm94-99). The force field has been derived by fitting to high-level quantum mechanical data and verified by comparison with very high-level quantum mechanical calculations and by a very extensive comparison between simulations and experimental data. The set of validation simulations includes two of the longest trajectories published to date for the DNA duplex (200 ns each) and the largest variety of NA structures studied to date (15 different NA families and 97 individual structures). The total simulation time used to validate the force field includes near 1 mus of state-of-the-art molecular dynamics simulations in aqueous solution.},
author = {P{\'{e}}rez, Alberto and March{\'{a}}n, Iv{\'{a}}n and Svozil, Daniel and Sponer, Jiri and Cheatham, Thomas E and Laughton, Charles a and Orozco, Modesto},
doi = {10.1529/biophysj.106.097782},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/P{\'{e}}rez et al. - 2007 - Refinement of the AMBER force field for nucleic acids improving the description of alphagamma conformers.pdf:pdf},
isbn = {0006-3495 (Print)$\backslash$n0006-3495 (Linking)},
issn = {0006-3495},
journal = {Biophysical journal},
keywords = {Computer Simulation,DNA,DNA: chemistry,Models, Molecular,Nucleic Acid Conformation,RNA,RNA: chemistry},
month = {jun},
number = {11},
pages = {3817--29},
pmid = {17351000},
title = {{Refinement of the AMBER force field for nucleic acids: improving the description of alpha/gamma conformers.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1868997{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {92},
year = {2007}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network (??net) package. The construction and application of ANN potentials using ??net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite (??-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
volume = {114},
year = {2016}
}
@article{Behler2016,
abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.},
author = {Behler, J??rg},
doi = {10.1063/1.4966192},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Behler - 2016 - Perspective Machine learning potentials for atomistic simulations(2).pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pages = {170901},
pmid = {27825224},
title = {{Perspective: Machine learning potentials for atomistic simulations}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966192},
volume = {145},
year = {2016}
}
@article{Keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say {\$}32{\$}-{\$}512{\$} data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
eprint = {1609.04836},
file = {::},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
url = {http://arxiv.org/abs/1609.04836},
year = {2016}
}
@article{pytorch,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
file = {::},
month = {dec},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
@article{Liers2005,
author = {Liers, Frauke and J{\"{u}}nger, Michael and Reinelt, Gerhard and Rinaldi, Giovanni},
doi = {10.1002/3527603794.ch4},
isbn = {3527404066},
journal = {New Optimization Algorithms in Physics},
pages = {47--69},
title = {{Computing Exact Ground States of Hard Ising Spin Glass Problems by Branch-and-Cut}},
year = {2005}
}
@techreport{rmsprop,
author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
file = {::},
title = {{Neural Networks for Machine Learning Lecture 6a Overview of mini-­-batch gradient descent}}
}
@article{Li2011,
abstract = {We propose a generalized regression neural network (GRNN) approach based on grey relational analysis (GRA) and principal component analysis (PCA) (GP-GRNN) to improve the accuracy of density functional theory (DFT) calculation for homolysis bond dissociation energies (BDE) of Y-NO bond. As a demonstration, this combined quantum chemistry calculation with the GP-GRNN approach has been applied to evaluate the homolysis BDE of 92 Y-NO organic molecules. The results show that the ull-descriptor GRNN without GRA and PCA (F-GRNN) and with GRA (G-GRNN) approaches reduce the root-mean-square (RMS) of the calculated homolysis BDE of 92 organic molecules from 5.31 to 0.49 and 0.39 kcal mol(-1) for the B3LYP/6-31G (d) calculation. Then the newly developed GP-GRNN approach further reduces the RMS to 0.31 kcal mol(-1). Thus, the GP-GRNN correction on top of B3LYP/6-31G (d) can improve the accuracy of calculating the homolysis BDE in quantum chemistry and can predict homolysis BDE which cannot be obtained experimentally.},
author = {Li, Hong Zhi and Tao, Wei and Gao, Ting and Li, Hui and Lu, Ying Hua and Su, Zhong Min},
doi = {10.3390/ijms12042242},
isbn = {1661-6596},
issn = {14220067},
journal = {International Journal of Molecular Sciences},
keywords = {Density functional theory,Generalized regression neural network,Grey relational analysis,Homolysis bond dissociation energy,Principal component analysis,Y-NO bond},
number = {4},
pages = {2242--2261},
pmid = {21731439},
title = {{Improving the accuracy of density functional theory (DFT) calculation for homolysis bond dissociation energies of Y-NO bond: Generalized regression neural network based on grey relational analysis and principal component analysis}},
volume = {12},
year = {2011}
}
@article{Snyder2012,
abstract = {Machine learning is used to approximate density functionals. For the model problem of the kinetic energy of noninteracting fermions in 1D, mean absolute errors below 1 kcal/mol on test densities similar to the training set are reached with fewer than 100 training densities. A predictor identifies if a test density is within the interpolation region. Via principal component analysis, a projected functional derivative finds highly accurate self-consistent densities. The challenges for application of our method to real electronic structure problems are discussed.},
author = {Snyder, John C. and Rupp, Matthias and Hansen, Katja and M{\"{u}}ller, Klaus-Robert and Burke, Kieron},
doi = {10.1103/PhysRevLett.108.253002},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Snyder et al. - 2012 - Finding Density Functionals with Machine Learning.pdf:pdf},
isbn = {8572533357217},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {jun},
number = {25},
pages = {253002},
pmid = {23004593},
title = {{Finding Density Functionals with Machine Learning}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.108.253002 https://link.aps.org/doi/10.1103/PhysRevLett.108.253002},
volume = {108},
year = {2012}
}
@inproceedings{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
keywords = {GoogLeNet},
mendeley-tags = {GoogLeNet},
month = {jun},
pages = {1--9},
pmid = {24920543},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
url = {http://ieeexplore.ieee.org/document/7298594/},
volume = {07-12-June},
year = {2015}
}
@techreport{Glorot,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
file = {::},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://www.iro.umontreal.}
}
@article{Snyder2013a,
abstract = {Using a one-dimensional model, we explore the ability of machine learning to approximate the non-interacting kinetic energy density functional of diatomics. This nonlinear interpolation between Kohn-Sham reference calculations can (i) accurately dissociate a diatomic, (ii) be systematically improved with increased reference data and (iii) generate accurate self-consistent densities via a projection method that avoids directions with no data. With relatively few densities, the error due to the interpolation is smaller than typical errors in standard exchange-correlation functionals.},
archivePrefix = {arXiv},
arxivId = {1306.1812},
author = {Snyder, John C. and Rupp, Matthias and Hansen, Katja and Blooston, Leo and M{\"{u}}ller, Klaus Robert and Burke, Kieron},
doi = {10.1063/1.4834075},
eprint = {1306.1812},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Snyder et al. - 2013 - Orbital-free bond breaking via machine learning(2).pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {22},
pmid = {24329053},
title = {{Orbital-free bond breaking via machine learning}},
volume = {139},
year = {2013}
}
@article{Huang2016,
abstract = {The predictive accuracy of Machine Learning (ML) models of molecular properties depends on the choice of the molecular representation. Based on the postulates of quantum mechanics, we introduce a hierarchy of representations which meet uniqueness and target similarity criteria. To systematically control target similarity, we rely on interatomic many body expansions, as implemented in universal force-fields, including Bonding, Angular, and higher order terms (BA). Addition of higher order contributions systematically increases similarity to the true potential energy and predictive accuracy of the resulting ML models. We report numerical evidence for the performance of BAML models trained on molecular properties pre-calculated at electron-correlated and density functional theory level of theory for thousands of small organic molecules. Properties studied include enthalpies and free energies of atomization, heatcapacity, zero-point vibrational energies, dipole-moment, polarizability, HOMO/LUMO energies and gap, ionization potential, electron affinity, and electronic excitations. After training, BAML predicts energies or electronic properties of out-of-sample molecules with unprecedented accuracy and speed.},
archivePrefix = {arXiv},
arxivId = {1608.06194},
author = {Huang, Bing and {Von Lilienfeld}, O. Anatole},
doi = {10.1063/1.4964627},
eprint = {1608.06194},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Von Lilienfeld - 2016 - Communication Understanding molecular representations in machine learning The role of uniqueness and targ.pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {16},
title = {{Communication: Understanding molecular representations in machine learning: The role of uniqueness and target similarity}},
url = {http://arxiv.org/abs/1608.06194},
volume = {145},
year = {2016}
}
@article{Lin2016,
abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that {\$}n{\$} variables cannot be multiplied using fewer than 2{\^{}}n neurons in a single hidden layer.},
archivePrefix = {arXiv},
arxivId = {1608.08225},
author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
eprint = {1608.08225},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Tegmark, Rolnick - 2016 - Why does deep and cheap learning work so well.pdf:pdf},
isbn = {9781627480031},
issn = {00189219},
journal = {arXiv},
month = {aug},
number = {11},
pages = {2278--2324},
pmid = {25246403},
title = {{Why does deep and cheap learning work so well?}},
url = {http://arxiv.org/abs/1608.08225 http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {2016}
}
@article{Jetchev2016,
abstract = {Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.},
archivePrefix = {arXiv},
arxivId = {1611.08207},
author = {Jetchev, Nikolay and Bergmann, Urs and Vollgraf, Roland},
eprint = {1611.08207},
number = {ii},
title = {{Texture Synthesis with Spatial Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.08207},
year = {2016}
}
@article{Burmeister1995,
abstract = {Abstract Go provides artificial intelligence (AI) and cognitive science researchers with an easily specified formal domain in which skills of human intelligence cannot be matched by currently known programming techniques. Go is a much more widely played game than  ...},
author = {Burmeister, J and Wiles, J},
doi = {10.1109/ANZIIS.1995.705737},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Burmeister, Wiles - 1995 - The challenge of Go as a domain for AI research a comparison between Go and chess.pdf:pdf},
isbn = {0-86422-430-3},
journal = {{\ldots} },
pages = {181--186},
title = {{The challenge of Go as a domain for AI research: a comparison between Go and chess}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=705737{\%}5Cnpapers3://publication/uuid/B00B0293-9C0A-4C08-8BD6-34ED130B0272},
year = {1995}
}
@article{Dieleman2015,
abstract = {Measuring the morphological parameters of galaxies is a key requirement for studying their formation and evolution. Surveys such as the Sloan Digital Sky Survey (SDSS) have resulted in the availability of very large collections of images, which have permitted population-wide analyses of galaxy morphology. Morphological analysis has traditionally been carried out mostly via visual inspection by trained experts, which is time-consuming and does not scale to large ({\$}\backslashgtrsim10{\^{}}4{\$}) numbers of images. Although attempts have been made to build automated classification systems, these have not been able to achieve the desired level of accuracy. The Galaxy Zoo project successfully applied a crowdsourcing strategy, inviting online users to classify images by answering a series of questions. Unfortunately, even this approach does not scale well enough to keep up with the increasing availability of galaxy images. We present a deep neural network model for galaxy morphology classification which exploits translational and rotational symmetry. It was developed in the context of the Galaxy Challenge, an international competition to build the best model for morphology classification based on annotated images from the Galaxy Zoo project. For images with high agreement among the Galaxy Zoo participants, our model is able to reproduce their consensus with near-perfect accuracy ({\$}{\textgreater} 99\backslash{\%}{\$}) for most questions. Confident model predictions are highly accurate, which makes the model suitable for filtering large collections of images and forwarding challenging images to experts for manual annotation. This approach greatly reduces the experts' workload without affecting accuracy. The application of these algorithms to larger sets of training data will be critical for analysing results from future surveys such as the LSST.},
archivePrefix = {arXiv},
arxivId = {1503.07077},
author = {Dieleman, Sander and Willett, Kyle W. and Dambre, Joni},
doi = {10.1093/mnras/stv632},
eprint = {1503.07077},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Dieleman, Willett, Dambre - 2015 - Rotation-invariant convolutional neural networks for galaxy morphology prediction.pdf:pdf},
isbn = {0035-8711},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Catalogues,Galaxies: general,Methods: data analysis,Techniques: image processing},
number = {2},
pages = {1441--1459},
pmid = {7491034},
title = {{Rotation-invariant convolutional neural networks for galaxy morphology prediction}},
volume = {450},
year = {2015}
}
@article{Behler2007,
abstract = {The accurate description of chemical processes often requires the use of computationally demanding methods like density-functional theory (DFT), making long simulations of large systems unfeasible. In this Letter we introduce a new kind of neural-network representation of DFT potential-energy surfaces, which provides the energy and forces as a function of all atomic positions in systems of arbitrary size and is several orders of magnitude faster than DFT. The high accuracy of the method is demonstrated for bulk silicon and compared with empirical potentials and DFT. The method is general and can be applied to all types of periodic and nonperiodic systems.},
author = {Behler, J??rg and Parrinello, Michele},
doi = {10.1103/PhysRevLett.98.146401},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Behler, Parrinello - 2007 - Generalized neural-network representation of high-dimensional potential-energy surfaces.pdf:pdf},
isbn = {0031-9007},
issn = {00319007},
journal = {Physical Review Letters},
number = {14},
pages = {1--4},
pmid = {17501293},
title = {{Generalized neural-network representation of high-dimensional potential-energy surfaces}},
volume = {98},
year = {2007}
}
@article{Tolstikhin2017,
abstract = {Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
archivePrefix = {arXiv},
arxivId = {1701.02386},
author = {Tolstikhin, Ilya and Gelly, Sylvain and Bousquet, Olivier and Simon-Gabriel, Carl-Johann and Sch{\"{o}}lkopf, Bernhard},
eprint = {1701.02386},
pages = {1--28},
title = {{AdaGAN: Boosting Generative Models}},
url = {http://arxiv.org/abs/1701.02386},
year = {2017}
}
@article{dirac,
author = {Dirac, Paul A. M.},
doi = {10.1098/rspa.1929.0094},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Dirac - 1929 - Quantum Mechanics of Many-Electron Systems.pdf:pdf},
issn = {1364-5021},
journal = {Royal Society},
month = {apr},
number = {792},
pages = {714 ---- 733},
title = {{Quantum Mechanics of Many-Electron Systems}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1983.0054 http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1929.0094},
volume = {123},
year = {1929}
}
@article{Shafaei2016b,
abstract = {Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision.},
archivePrefix = {arXiv},
arxivId = {1608.01745},
author = {Shafaei, Alireza and Little, James J. and Schmidt, Mark},
eprint = {1608.01745},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Shafaei, Little, Schmidt - 2016 - Play and Learn Using Video Games to Train Computer Vision Models.pdf:pdf},
journal = {Proceedings of the British Machine Vision Conference},
title = {{Play and Learn: Using Video Games to Train Computer Vision Models}},
url = {http://arxiv.org/abs/1608.01745},
year = {2016}
}
@article{Mnih2013,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://www.nature.com/doifinder/10.1038/nature14236 http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@article{Ray1989,
abstract = {The Sherrington-Kirkpatrick model under a transverse field is studied here employing the Suzuki-Trotter formula to map the model to an equivalent classical one. The effective Thouless-Anderson-Palmer free energy is used to study the stability of the system, and Monte Carlo computer simulations of the effective classical model are performed to obtain the phase diagram and the magnetization overlap distribution. Our results indicate a trivial overlap distribution due to quantum fluctuations. The phase diagram shows a slight initial increase in the glass transition temperature Tg as the transverse field is switched on, confirming that obtained by Yokota. {\textcopyright} 1989 The American Physical Society.},
author = {Ray, P. and Chakrabarti, B. K. and Chakrabarti, Arunava},
doi = {10.1103/PhysRevB.39.11828},
issn = {01631829},
journal = {Physical Review B},
number = {16},
pages = {11828--11832},
title = {{Sherrington-Kirkpatrick model in a transverse field: Absence of replica symmetry breaking due to quantum fluctuations}},
volume = {39},
year = {1989}
}
@article{Kadowaki1998,
abstract = {We introduce quantum fluctuations into the simulated annealing process of optimization problems, aiming at faster convergence to the optimal state. Quantum fluctuations cause transitions between states and thus play the same role as thermal fluctuations in the conventional approach. The idea is tested by the transverse Ising model, in which the transverse field is a function of time similar to the temperature in the conventional method. The goal is to find the ground state of the diagonal part of the Hamiltonian with high accuracy as quickly as possible. We have solved the time-dependent Schr{\"{o}}dinger equation numerically for small size systems with various exchange interactions. Comparison with the results of the corresponding classical (thermal) method reveals that the quantum annealing leads to the ground state with much larger probability in almost all cases if we use the same annealing schedule. {\textcopyright} 1998 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/9804280},
author = {Kadowaki, Tadashi and Nishimori, Hidetoshi},
doi = {10.1103/PhysRevE.58.5355},
eprint = {9804280},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {5},
pages = {5355--5363},
primaryClass = {cond-mat},
title = {{Quantum annealing in the transverse Ising model}},
volume = {58},
year = {1998}
}
@article{Tanaka2016,
abstract = {We design a Convolutional Neural Network (CNN) which studies correlation between discretized inverse temperature and spin configuration of 2D Ising model and show that it can find a feature of the phase transition without teaching any a priori information for it. We also define a new order parameter via the CNN and show that it provides well approximated critical inverse temperature. In addition, we compare the activation functions for convolution layer and find that the Rectified Linear Unit (ReLU) is important to detect the phase transition of 2D Ising model.},
archivePrefix = {arXiv},
arxivId = {1609.09087},
author = {Tanaka, Akinori and Tomiya, Akio},
doi = {10.7566/JPSJ.86.063001},
eprint = {1609.09087},
issn = {0031-9015},
number = {Table I},
pages = {1--7},
title = {{Detection of phase transition via convolutional neural network}},
url = {http://arxiv.org/abs/1609.09087},
year = {2016}
}
@article{Jasrasaria2016,
abstract = {A fundamental problem in applying machine learning techniques for chemical problems is to find suitable representations for molecular and crystal structures. While the structure representations based on atom connectivities are prevalent for molecules, two-dimensional descriptors are not suitable for describing molecular crystals. In this work, we introduce the SFC-M family of feature representations, which are based on Morton space-filling curves, as an alternative means of representing crystal structures. Latent Semantic Indexing (LSI) was employed in a novel setting to reduce sparsity of feature representations. The quality of the SFC-M representations were assessed by using them in combination with artificial neural networks to predict Density Functional Theory (DFT) single point, Ewald summed, lattice, and many-body dispersion energies of 839 organic molecular crystal unit cells from the Cambridge Structural Database that consist of the elements C, H, N, and O. Promising initial results suggest that the SFC-M representations merit further exploration to improve its ability to predict solid-state properties of organic crystal structures},
archivePrefix = {arXiv},
arxivId = {1608.05747},
author = {Jasrasaria, Dipti and Pyzer-Knapp, Edward O. and Rappoport, Dmitrij and Aspuru-Guzik, Alan},
eprint = {1608.05747},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Jasrasaria et al. - 2016 - Space-Filling Curves as a Novel Crystal Structure Representation for Machine Learning Models.pdf:pdf},
title = {{Space-Filling Curves as a Novel Crystal Structure Representation for Machine Learning Models}},
url = {http://arxiv.org/abs/1608.05747},
year = {2016}
}
@article{Wallach2015,
abstract = {Deep convolutional neural networks comprise a subclass of deep neural networks (DNN) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. Convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models. Although DNNs have been used in drug discovery for QSAR and ligand-based bioactivity predictions, none of these models have benefited from this powerful convolutional architecture. This paper introduces AtomNet, the first structure-based, deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications. We demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. In further contrast to existing DNN techniques, we show that AtomNet's application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators. Finally, we show that AtomNet outperforms previous docking approaches on a diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9 on 57.8{\%} of the targets in the DUDE benchmark.},
archivePrefix = {arXiv},
arxivId = {1510.02855},
author = {Wallach, Izhar and Dzamba, Michael and Heifets, Abraham},
doi = {10.1007/s10618-010-0175-9},
eprint = {1510.02855},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Wallach, Dzamba, Heifets - 2015 - AtomNet A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discove.pdf:pdf},
issn = {1384-5810},
journal = {arXiv preprint arXiv:1510.02855},
pages = {1--11},
pmid = {19477997},
title = {{AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery}},
year = {2015}
}
@article{Ryczko2018a,
abstract = {{\textcopyright} 2018 Elsevier B.V. We introduce a new method, called CNNAS (convolutional neural networks for atomistic systems), for calculating the total energy of atomic systems which rivals the computational cost of empirical potentials while maintaining the accuracy of ab initio calculations. This method uses deep convolutional neural networks (CNNs), where the input to these networks are simple representations of the atomic structure. We use this approach to predict energies obtained using density functional theory (DFT) for 2D hexagonal lattices of various types. Using a dataset consisting of graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures, with and without defects, we trained a deep CNN that is capable of predicting DFT energies to an extremely high accuracy, with a mean absolute error (MAE) of 0.198 meV/atom (maximum absolute error of 16.1 meV/atom). To explore our new methodology, we investigate the ability of a deep neural network (DNN) in predicting a Lennard-Jones energy and separation distance for a dataset of dimer molecules in both two and three dimensions. In addition, we systematically investigate the flexibility of the deep learning models by performing interpolation and extrapolation tests.},
author = {Ryczko, K. and Mills, K. and Luchak, I. and Homenick, C. and Tamblyn, I.},
doi = {10.1016/j.commatsci.2018.03.005},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {2D materials,Convolutional neural networks,Deep learning,Density functional theory,Dimer molecules},
title = {{Convolutional neural networks for atomistic systems}},
volume = {149},
year = {2018}
}
@article{Carrasquilla2016,
abstract = {Neural networks can be used to identify phases and phase transitions in condensed matter systems via supervised machine learning. Readily programmable through modern software libraries, we show that a standard feed-forward neural network can be trained to detect multiple types of order parameter directly from raw state configurations sampled with Monte Carlo. In addition, they can detect highly non-trivial states such as Coulomb phases, and if modified to a convolutional neural network, topological phases with no conventional order parameter. We show that this classification occurs within the neural network without knowledge of the Hamiltonian or even the general locality of interactions. These results demonstrate the power of machine learning as a basic research tool in the field of condensed matter and statistical physics.},
archivePrefix = {arXiv},
arxivId = {1605.01735},
author = {Carrasquilla, Juan and Melko, Roger G},
doi = {10.1038/nphys4035},
eprint = {1605.01735},
issn = {1745-2473},
journal = {Nature Physics},
month = {feb},
number = {5},
pages = {431--434},
title = {{Machine learning phases of matter}},
url = {http://arxiv.org/abs/1605.01735 http://www.nature.com/doifinder/10.1038/nphys4035},
volume = {13},
year = {2017}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961 http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Shirvany2008,
abstract = {We present a method to solve boundary value problems using artificial neural networks (ANN). A trial solution of the differential equation is written as a feed-forward neural network containing adjustable parameters (the weights and biases). From the differential equation and its boundary conditions we prepare the energy function which is used in the back-propagation method with momentum term to update the network parameters. We improved energy function of ANN which is derived from Schrodinger equation and the boundary conditions. With this improvement of energy function we can use unsupervised training method in the ANN for solving the equation. Unsupervised training aims to minimize a non-negative energy function. We used the ANN method to solve Schrodinger equation for few quantum systems. Eigenfunctions and energy eigenvalues are calculated. Our numerical results are in agreement with their corresponding analytical solution and show the efficiency of ANN method for solving eigenvalue problems. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Shirvany, Yazdan and Hayati, Mohsen and Moradian, Rostam},
doi = {10.1016/j.cnsns.2007.04.024},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Shirvany, Hayati, Moradian - 2008 - Numerical solution of the nonlinear Schrodinger equation by feedforward neural networks.pdf:pdf},
issn = {10075704},
journal = {Communications in Nonlinear Science and Numerical Simulation},
keywords = {Differential equation,Eigenfunction,Eigenvalue,Energy function,Feed-forward neural network,Schrodinger equation},
month = {dec},
number = {10},
pages = {2132--2145},
title = {{Numerical solution of the nonlinear Schrodinger equation by feedforward neural networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1007570407001037},
volume = {13},
year = {2008}
}
@article{Manzhos2006,
abstract = {We combine the high dimensional model representation (HDMR) idea of Rabitz and co-workers [J. Phys. Chem. 110, 2474 (2006)] with neural network (NN) fits to obtain an effective means of building multidimensional potentials. We verify that it is possible to determine an accurate many-dimensional potential by doing low dimensional fits. The final potential is a sum of terms each of which depends on a subset of the coordinates. This form facilitates quantum dynamics calculations. We use NNs to represent HDMR component functions that minimize error mode term by mode term. This NN procedure makes it possible to construct high-order component functions which in turn enable us to determine a good potential. It is shown that the number of available potential points determines the order of the HDMR which should be used.},
author = {Manzhos, Sergei and Carrington, Tucker},
doi = {10.1063/1.2336223},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Manzhos, Carrington - 2006 - A random-sampling high dimensional model representation neural network for building potential energy surfac.pdf:pdf},
issn = {0021-9606},
journal = {The Journal of chemical physics},
number = {8},
pages = {084109},
pmid = {16965003},
title = {{A random-sampling high dimensional model representation neural network for building potential energy surfaces.}},
url = {http://scitation.aip.org/content/aip/journal/jcp/125/8/10.1063/1.2336223},
volume = {125},
year = {2006}
}
@article{Oord2016a,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1606.05328},
file = {::},
month = {jun},
title = {{Conditional Image Generation with PixelCNN Decoders}},
url = {http://arxiv.org/abs/1606.05328},
year = {2016}
}
@book{Sutton2018,
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/TNN.1998.712192},
edition = {Second},
isbn = {9780262039246},
issn = {1045-9227},
month = {sep},
publisher = {The MIT Press},
title = {{Reinforcement Learning: An Introduction}},
url = {http://incompleteideas.net/book/the-book-2nd.html https://www.cambridge.org/core/product/identifier/S0263574799271172/type/journal{\_}article http://ieeexplore.ieee.org/document/712192/},
year = {2018}
}
@article{Vinyals2019,
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P and Jaderberg, Max and Vezhnevets, Alexander S and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
doi = {10.1038/s41586-019-1724-z},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {August},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {http://www.nature.com/articles/s41586-019-1724-z},
year = {2019}
}
@article{Metz2016,
abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
archivePrefix = {arXiv},
arxivId = {1611.02163},
author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
eprint = {1611.02163},
pages = {1--25},
pmid = {202927},
title = {{Unrolled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.02163},
year = {2016}
}
@techreport{Hron2018,
abstract = {Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for varia-tional Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.},
archivePrefix = {arXiv},
arxivId = {1807.01969v1},
author = {Hron, Jiri and De, Alexander G and Matthews, G and Ghahramani, Zoubin},
eprint = {1807.01969v1},
file = {::},
isbn = {1807.01969v1},
title = {{Variational Bayesian dropout: pitfalls and fixes}},
year = {2018}
}
@article{Seko2014,
abstract = {A combination of systematic density-functional theory (DFT) calculations and machine learning techniques has a wide range of potential applications. This study presents an application of the combination of systematic DFT calculations and regression techniques to the prediction of the melting temperature for single and binary compounds. Here we adopt the ordinary least-squares regression, partial least-squares regression, support vector regression, and Gaussian process regression. Among the four kinds of regression techniques, SVR provides the best prediction. The inclusion of physical properties computed by the DFT calculation to a set of predictor variables makes the prediction better. In addition, limitation of the predictive power is shown when extrapolation from the training dataset is required. Finally, a simulation to find the highest melting temperature toward the efficient materials design using kriging is demonstrated. The kriging design finds the compound with the highest melting temperature much faster than random designs. This result may stimulate the application of kriging to efficient materials design for a broad range of applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1310.1546v1},
author = {Seko, Atsuto and Maekawa, Tomoya and Tsuda, Koji and Tanaka, Isao},
doi = {10.1103/PhysRevB.89.054303},
eprint = {arXiv:1310.1546v1},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Seko et al. - 2014 - Machine learning with systematic density-functional theory calculations Application to melting temperatures of sing.pdf:pdf},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
keywords = {64.70.dj,89.20.Ff},
number = {5},
pages = {1--9},
title = {{Machine learning with systematic density-functional theory calculations: Application to melting temperatures of single- and binary-component solids}},
volume = {89},
year = {2014}
}
@article{Schulman2015a,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
eprint = {1506.02438},
pages = {1--14},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {http://arxiv.org/abs/1506.02438},
year = {2015}
}
@article{Chetlur2014,
abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36{\%} on a standard model while also reducing memory consumption.},
archivePrefix = {arXiv},
arxivId = {1410.0759},
author = {Chetlur, Sharan and Woolley, Cliff},
eprint = {1410.0759},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Chetlur, Woolley - 2014 - cuDNN Efficient Primitives for Deep Learning.pdf:pdf},
journal = {arXiv},
month = {oct},
pages = {1--9},
title = {{cuDNN: Efficient Primitives for Deep Learning}},
url = {http://arxiv.org/abs/1410.0759},
year = {2014}
}
@article{Chng2016,
abstract = {Machine learning offers an unprecedented perspective for the problem of classifying phases in condensed matter physics. We employ neural network machine learning techniques to distinguish finite-temperature phases of the strongly-correlated fermions on cubic lattices. We show that a three-dimensional convolutional network trained on auxiliary field configurations produced by quantum Monte Carlo simulations of the Hubbard model can correctly predict the magnetic phase diagram of the model at the average density of one (half filling). We then use the network, trained at half filling, to explore the trend in the transition temperature as the system is doped away from half filling. This transfer learning approach predicts that the instability to the magnetic phase extends to this region, albeit with a transition temperature that falls rapidly as a function of doping. Our results pave the way for other machine learning applications in correlated quantum many-body systems.},
archivePrefix = {arXiv},
arxivId = {1609.02552},
author = {Ch'ng, Kelvin and Carrasquilla, Juan and Melko, Roger G. and Khatami, Ehsan},
eprint = {1609.02552},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ch'ng et al. - 2016 - Machine Learning Phases of Strongly Correlated Fermions.pdf:pdf},
journal = {arXiv},
pages = {1--8},
title = {{Machine Learning Phases of Strongly Correlated Fermions}},
url = {http://arxiv.org/abs/1609.02552},
year = {2016}
}
@article{OpenAI2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
eprint = {1910.07113},
pages = {1--51},
title = {{Solving Rubik's Cube with a Robot Hand}},
url = {http://arxiv.org/abs/1910.07113},
year = {2019}
}
@article{Brooks1983,
abstract = {CHARMM (Chemistry at HARvard Macromolecular Mechanics) is a highly flexible computer program which uses empirical energy functions to model macromolecular systems. The program can read or model build structures, energy minimize them by first- or second-derivative techniques, perform a normal mode or molecular dynamics simulation, and analyze the structural, equilibrium, and dynamic properties determined in these calculations. The operations that CHARMM can perform are described, and some implementation details are given. A set of parameters for the empirical energy function and a sample run are included.},
author = {Brooks, Bernard R and Bruccoleri, Robert E and Olafson, Barry D and States, David J and Swaminathan, S and Karplus, Martin},
doi = {10.1002/jcc.540040211},
isbn = {0192-8651},
issn = {1096987X},
journal = {Journal of Computational Chemistry},
number = {2},
pages = {187--217},
pmid = {1},
title = {{CHARMM: A program for macromolecular energy, minimization, and dynamics calculations}},
volume = {4},
year = {1983}
}
@article{Lucas2014,
abstract = {We provide Ising formulations for many NP-complete and NP-hard problems, including all of Karp's 21 NP-complete problems. This collects and extends mappings to the Ising model from partitioning, covering and satisfiability. In each case, the required number of spins is at most cubic in the size of the problem. This work may be useful in designing adiabatic quantum optimization algorithms.},
author = {Lucas, Andrew},
doi = {10.3389/fphy.2014.00005},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Adiabatic quantum computation,Algorithms,Complexity theory,NP,Spin glasses},
number = {February},
pages = {1--14},
title = {{Ising formulations of many NP problems}},
volume = {2},
year = {2014}
}
@article{Mnih2016,
abstract = {Wc propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Lehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous methods for deep reinforcement learning.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2850--2869},
title = {{Asynchronous methods for deep reinforcement learning}},
volume = {4},
year = {2016}
}
@article{Wetzel2017,
abstract = {We employ unsupervised machine learning techniques to learn latent parameters which best describe states of the two-dimensional Ising model and the three-dimensional XY model. These methods range from principal component analysis to artificial neural network based variational autoencoders. The states are sampled using a Monte-Carlo simulation above and below the critical temperature. We find that the predicted latent parameters correspond to the known order parameters. The latent representation of the states of the models in question are clustered, which makes it possible to identify phases without prior knowledge of their existence or the underlying Hamiltonian. Furthermore, we find that the reconstruction loss function can be used as a universal identifier for phase transitions.},
archivePrefix = {arXiv},
arxivId = {1703.02435},
author = {Wetzel, Sebastian Johann},
doi = {10.1038/nphys4037},
eprint = {1703.02435},
issn = {1745-2473},
pages = {1--8},
title = {{Unsupervised learning of phase transitions: from principal component analysis to variational autoencoders}},
url = {http://arxiv.org/abs/1703.02435},
year = {2017}
}
@inproceedings{Raina2009,
abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton {\&} Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
address = {New York, New York, USA},
author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning ({\{}ICML{\}})},
doi = {http://doi.acm.org/10.1145/1553374.1553486},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Raina, Madhavan, Ng - 2009 - Large-scale deep unsupervised learning using graphics processors.pdf:pdf},
isbn = {978-1-60558-516-1},
issn = {12258687},
pages = {873--880},
pmid = {17394762},
publisher = {ACM Press},
title = {{Large-scale deep unsupervised learning using graphics processors}},
url = {http://dl.acm.org/citation.cfm?id=1553374.1553486 http://portal.acm.org/citation.cfm?doid=1553374.1553486},
year = {2009}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/kmills/Desktop/phd{\_}references/1701.07875 (1).pdf:pdf},
journal = {arXiv},
month = {jan},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{Faber2017,
abstract = {We investigate the impact of choosing regressors and molecular representations for the construction of fast machine learning (ML) models of thirteen electronic ground-state properties of organic molecules. The performance of each regressor/representation/property combination is assessed using learning curves which report out-of-sample errors as a function of training set size with up to {\$}\backslashsim{\$}117k distinct molecules. Molecular structures and properties at hybrid density functional theory (DFT) level of theory used for training and testing come from the QM9 database [Ramakrishnan et al, {\{}$\backslash$em Scientific Data{\}} {\{}$\backslash$bf 1{\}} 140022 (2014)] and include dipole moment, polarizability, HOMO/LUMO energies and gap, electronic spatial extent, zero point vibrational energy, enthalpies and free energies of atomization, heat capacity and the highest fundamental vibrational frequency. Various representations from the literature have been studied (Coulomb matrix, bag of bonds, BAML and ECFP4, molecular graphs (MG)), as well as newly developed distribution based variants including histograms of distances (HD), and angles (HDA/MARAD), and dihedrals (HDAD). Regressors include linear models (Bayesian ridge regression (BR) and linear regression with elastic net regularization (EN)), random forest (RF), kernel ridge regression (KRR) and two types of neural net works, graph convolutions (GC) and gated graph networks (GG). We present numerical evidence that ML model predictions deviate from DFT less than DFT deviates from experiment for all properties. Furthermore, our out-of-sample prediction errors with respect to hybrid DFT reference are on par with, or close to, chemical accuracy. Our findings suggest that ML models could be more accurate than hybrid DFT if explicitly electron correlated quantum (or experimental) data was available.},
archivePrefix = {arXiv},
arxivId = {1702.05532},
author = {Faber, Felix A and Hutchison, Luke and Huang, Bing and Gilmer, Justin and Schoenholz, Samuel S and Dahl, George E and Vinyals, Oriol and Kearnes, Steven and Riley, Patrick F and von Lilienfeld, O. Anatole},
eprint = {1702.05532},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Faber et al. - 2017 - Machine learning prediction errors better than DFT accuracy.pdf:pdf},
journal = {arXiv},
month = {feb},
pages = {1--12},
title = {{Machine learning prediction errors better than DFT accuracy}},
url = {http://arxiv.org/abs/1702.05532},
year = {2017}
}
@article{LeCun1998b,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Mills2020a,
author = {Mills, Kyle and Ronagh, Pooya and Tamblyn, Isaac},
doi = {10.1038/s42256-020-0226-x},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
month = {sep},
number = {9},
pages = {509--517},
title = {{Finding the ground state of spin Hamiltonians with reinforcement learning}},
url = {http://www.nature.com/articles/s42256-020-0226-x},
volume = {2},
year = {2020}
}
@techreport{Carreira-Perpinan,
abstract = {Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called "con-trastive divergence" (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution. Consider a probability distribution over a vector x (as-sumed discrete w.l.o.g.) and with parameters W p(x; W) = 1 Z(W) e −E(x;W) (1) where Z(W) = x e −E(x;W) is a normalisation constant and E(x; W) is an energy function. This class of random-field distributions has found many practical applications (Li, 2001; Winkler, 2002; Teh et al., 2003; He et al., 2004). Maximum-likelihood (ML) learning of the parameters W given an iid sample X = {\{}x n {\}} N n=1 can be done by gradient ascent: W ($\tau$ +1) = W ($\tau$) + $\eta$ ∂L(W; X) ∂W},
author = {Carreira-Perpi{\~{n}}{\'{a}}n, Migue{\'{i}} A and Hinton, Geoffrey E},
file = {::},
title = {{On Contrastive Divergence Learning}}
}
@article{GoogleResearch2015,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems(2).pdf:pdf},
journal = {None},
month = {mar},
number = {212},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf http://arxiv.org/abs/1603.04467},
volume = {1},
year = {2016}
}
@article{Kucharski1992,
abstract = {A general implementation of the coupled‐cluster (CC) single, double, triple, and quadruple excitation (CCSDTQ) method is presented and applied to several molecules, including BH, HF, H2O, and CO with DZP basis sets. Comparisons with full CI show average errors to be 14 $\mu$hartree at equilibrium and 26 $\mu$hartree at twice Re. CCSDTQ is exact for four electrons and is the first CC method correct through sixth order in perturbation theory.},
author = {Kucharski, Stanislaw A. and Bartlett, Rodney J.},
doi = {10.1063/1.463930},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kucharski, Bartlett - 1992 - The coupled‐cluster single, double, triple, and quadruple excitation method.pdf:pdf},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
keywords = {BORON HYDRIDES,CARBON MONOXIDE,COUPLING,ELECTRON CORRELATION,ERRORS,HYDROFLUORIC ACID,MANY-BODY PROBLEM,WATER,WAVE FUNCTIONS},
month = {sep},
number = {6},
pages = {4282--4288},
title = {{The coupled‐cluster single, double, triple, and quadruple excitation method}},
url = {http://scitation.aip.org/content/aip/journal/jcp/97/6/10.1063/1.463930 http://aip.scitation.org/doi/10.1063/1.463930},
volume = {97},
year = {1992}
}
@article{Battaglia2005,
abstract = {The path integral Monte Carlo simulated quantum annealing algorithm is applied to the optimization of a large hard instance of the random satisfiability problem (N=10000). The dynamical behavior of the quantum and the classical annealing are compared, showing important qualitative differences in the way of exploring the complex energy landscape of the combinatorial optimization problem. At variance with the results obtained for the Ising spin glass and for the traveling salesman problem, in the present case the linear-schedule quantum annealing performance is definitely worse than classical annealing. Nevertheless, a quantum cooling protocol based on field-cycling and able to outperform standard classical simulated annealing over short time scales is introduced. {\textcopyright} 2005 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0502468},
author = {Battaglia, Demian A. and Santoro, Giuseppe E. and Tosatti, Erio},
doi = {10.1103/PhysRevE.71.066707},
eprint = {0502468},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {6},
pages = {1--10},
primaryClass = {cond-mat},
title = {{Optimization by quantum annealing: Lessons from hard satisfiability problems}},
volume = {71},
year = {2005}
}
@article{Monterola2003b,
abstract = {We present a practical method for estimating the upper error bound in the neural network (NN) solution of the nonlinear Schr{\"{o}}dinger equation (NLSE) under different degrees of nonlinearity. The error bound is a function of the nonnegative energy E value that is minimized when the NN is trained to solve the NLSE. The form of E is derived from the NLSE expression and the NN solution becomes identical with the true NLSE solution only when the E value is reduced exactly to zero. In practice, machines with finite floating-point range and accuracy are used for training and E is not decreased exactly to zero. Knowledge of the error bound permits the estimation of the maximum average error in the NN solution without prior knowledge of the true NLSE solution - a crucial factor in the practical applications of the NN technique. The error bound is verified for both the linear time - independent Schr{\"{o}}dinger equation for a free particle, and the NLSE. We also discuss the conditions where the error bound formulation is valid. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
author = {Monterola, Christopher and Saloma, Caesar},
doi = {10.1016/S0030-4018(03)01570-0},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Monterola, Saloma - 2003 - Solving the nonlinear Schr{\"{o}}dinger equation with an unsupervised neural network estimation of error in sol(2).pdf:pdf},
issn = {00304018},
journal = {Optics Communications},
month = {jul},
number = {1-6},
pages = {331--339},
pmid = {19421275},
title = {{Solving the nonlinear Schr{\"{o}}dinger equation with an unsupervised neural network: estimation of error in solution}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0030401803015700},
volume = {222},
year = {2003}
}
@inproceedings{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
keywords = {GoogLeNet},
mendeley-tags = {GoogLeNet},
month = {jun},
pages = {1--9},
pmid = {24920543},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
url = {http://ieeexplore.ieee.org/document/7298594/},
volume = {07-12-June},
year = {2015}
}
@article{Haarnoja2018b,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {::},
journal = {35th International Conference on Machine Learning, ICML 2018},
month = {jan},
pages = {2976--2989},
publisher = {International Machine Learning Society (IMLS)},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://arxiv.org/abs/1801.01290},
volume = {5},
year = {2018}
}
@inproceedings{LeCun90,
author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R and Hubbard, Wayne and Jackel, Lawrence},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Touretzky, D},
pages = {396--404},
publisher = {Morgan-Kaufmann},
title = {{Handwritten Digit Recognition with a Back-Propagation Network}},
url = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
volume = {2},
year = {1990}
}
@article{Bian2014,
abstract = {This paper discusses techniques for solving discrete optimization problems using quantum annealing. Practical issues likely to affect the computation include precision limitations, finite temperature, bounded energy range, sparse connectivity, and small numbers of qubits. To address these concerns we propose a way of finding energy representations with large classical gaps between ground and first excited states, efficient algorithms for mapping non-compatible Ising models into the hardware, and the use of decomposition methods for problems that are too large to fit in hardware. We validate the approach by describing experiments with D-Wave quantum hardware for low density parity check decoding with up to 1000 variables.},
author = {Bian, Zhengbing and Chudak, Fabian and Israel, Robert and Lackey, Brad and Macready, William G. and Roy, Aidan},
doi = {10.3389/fphy.2014.00056},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Discrete optimization,Penalty functions,Quantum annealing,Sparse Ising model},
number = {September},
pages = {1--10},
title = {{Discrete optimization using quantum annealing on sparse Ising models}},
volume = {2},
year = {2014}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchan...},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Morawietz, Behler - 2013 - A density-functional theory-based neural network potential for water clusters including van der waals corr(2).pdf:pdf},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
volume = {117},
year = {2013}
}
@article{Portman2016,
abstract = {In this paper, we build and explore supervised learning models of ferromagnetic system behavior, using Monte-Carlo sampling of the spin configuration space generated by the 2D Ising model. Given the enormous size of the space of all possible Ising model realizations, the question arises as to how to choose a reasonable number of samples that will form physically meaningful and non-intersecting training and testing datasets. Here, we propose a sampling technique called ID-MH that uses the Metropolis-Hastings algorithm creating Markov process across energy levels within the predefined configuration subspace. We show that application of this method retains phase transitions in both training and testing datasets and serves the purpose of validation of a machine learning algorithm. For larger lattice dimensions, ID-MH is not feasible as it requires knowledge of the complete configuration space. As such, we develop a new "block-ID" sampling strategy: it decomposes the given structure into square blocks with lattice dimension no greater than 5 and uses ID-MH sampling of candidate blocks. Further comparison of the performance of commonly used machine learning methods such as random forests, decision trees, k nearest neighbors and artificial neural networks shows that the PCA-based Decision Tree regressor is the most accurate predictor of magnetizations of the Ising model. For energies, however, the accuracy of prediction is not satisfactory, highlighting the need to consider more algorithmically complex methods (e.g., deep learning).},
archivePrefix = {arXiv},
arxivId = {1611.05891},
author = {Portman, Nataliya and Tamblyn, Isaac},
doi = {10.1016/j.jcp.2017.06.045},
eprint = {1611.05891},
issn = {00219991},
journal = {Journal of Computational Physics},
month = {jul},
pages = {43},
title = {{Sampling algorithms for validation of supervised learning models for Ising-like systems}},
url = {http://arxiv.org/abs/1611.05891 http://linkinghub.elsevier.com/retrieve/pii/S0021999117304990},
year = {2017}
}
@article{Wang2016,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Start with raw spin configurations of a prototypical Ising model, we use principle component analysis to extract relevant low dimensional representations the original data, and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor as indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Wang - 2016 - Discovering phase transitions with unsupervised learning(3).pdf:pdf},
issn = {1550235X},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {http://arxiv.org/abs/1606.00318 http://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@article{Bowles2018,
abstract = {One of the biggest issues facing the use of machine learning in medical imaging is the lack of availability of large, labelled datasets. The annotation of medical images is not only expensive and time consuming but also highly dependent on the availability of expert observers. The limited amount of training data can inhibit the performance of supervised machine learning algorithms which often need very large quantities of data on which to train to avoid overfitting. So far, much effort has been directed at extracting as much information as possible from what data is available. Generative Adversarial Networks (GANs) offer a novel way to unlock additional information from a dataset by generating synthetic samples with the appearance of real images. This paper demonstrates the feasibility of introducing GAN derived synthetic data to the training datasets in two brain segmentation tasks, leading to improvements in Dice Similarity Coefficient (DSC) of between 1 and 5 percentage points under different conditions, with the strongest effects seen fewer than ten training image stacks are available.},
archivePrefix = {arXiv},
arxivId = {1810.10863},
author = {Bowles, Christopher and Chen, Liang and Guerrero, Ricardo and Bentley, Paul and Gunn, Roger and Hammers, Alexander and Dickie, David Alexander and Hern{\'{a}}ndez, Maria Vald{\'{e}}s and Wardlaw, Joanna and Rueckert, Daniel},
eprint = {1810.10863},
file = {::},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1810.10863},
year = {2018}
}
@phdthesis{Linnainmaa1970,
author = {Linnainmaa, Seppo},
file = {:Users/kmills/Desktop/phd{\_}references/linnainmaa1970thesis.pdf:pdf},
school = {University of Helsinki},
title = {{Alogritmin kumulatiivinen py{\"{o}}ristysvirhe yksitt{\"{a}}isten py{\"{o}}ristysvirheiden Taylor-kehitelm{\"{a}}n{\"{a}}}},
year = {1970}
}
@article{Carleo2016,
abstract = {The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the non-trivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form, for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with variable number of hidden neurons. A reinforcement-learning scheme is then demonstrated, capable of either finding the ground-state or describing the unitary time evolution of complex interacting quantum systems. We show that this approach achieves very high accuracy in the description of equilibrium and dynamical properties of prototypical interacting spins models in both one and two dimensions, thus offering a new powerful tool to solve the quantum many-body problem.},
author = {Carleo, Giuseppe and Troyer, Matthias},
doi = {10.1126/science.aag2302},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Carleo, Troyer - 2017 - Solving the quantum many-body problem with artificial neural networks.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {feb},
number = {6325},
pages = {602--606},
title = {{Solving the quantum many-body problem with artificial neural networks}},
url = {http://arxiv.org/abs/1606.02318 http://www.sciencemag.org/lookup/doi/10.1126/science.aag2302},
volume = {355},
year = {2017}
}
@article{Bounds1987,
abstract = {Research in spin-glass physics, population genetics, and neural network dynamics has provided powerful methods for finding near-global optima of functions that have many local optima. These techniques are being applied successfully to a wide variety of scientific and engineering problems. They may also give new insights into combinatorial optimization problems. {\textcopyright} 1987 Nature Publishing Group.},
author = {Bounds, David G.},
doi = {10.1038/329215a0},
issn = {00280836},
journal = {Nature},
number = {6136},
pages = {215--219},
title = {{New optimization methods from physics and biology}},
volume = {329},
year = {1987}
}
@article{Kim2017,
abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN},
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
eprint = {1703.05192},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.05192},
year = {2017}
}
@article{Behler2008,
abstract = {We present a combination of the metadynamics method for the investigation of pressure-induced phase transitions in sol- ids with a neural network representation of high-dimensional density-functional theory (DFT) potential-energy surfaces. In a recent illustration of the method for the complex high- pressure phase diagram of silicon [Behler et al., Phys. Rev. Lett. 100, 185501 (2008)] we have shown that the full sequence of phases can be reconstructed by a series of subsequent simulations. In the present paper we give a de- tailed account of the underlying methodology and discuss the scope and limitations of the approach, which promises to be a valuable tool for the investigation of a variety of inorganic materials. The method is several orders of magnitude faster than a direct coupling of metadynamics with electronic struc- ture calculations, while the accuracy is essentially maintained, thus providing access to extended simulations of large systems.},
author = {Behler, J{\"{o}}rg and Martoň{\'{a}}k, Roman and Donadio, Davide and Parrinello, Michele},
doi = {10.1002/pssb.200844219},
isbn = {0370-1972},
issn = {03701972},
journal = {physica status solidi (b)},
month = {dec},
number = {12},
pages = {2618--2629},
title = {{Pressure-induced phase transitions in silicon studied by neural network-based metadynamics simulations}},
url = {http://doi.wiley.com/10.1002/pssb.200844219},
volume = {245},
year = {2008}
}
@article{Funahashi1989,
abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations. ?? 1989.},
author = {Funahashi, Ken Ichi},
doi = {10.1016/0893-6080(89)90003-8},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Funahashi - 1989 - On the approximate realization of continuous mappings by neural networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Continuous mapping,Hidden layer,Neural network,Output function,Realization,Sigmoid function,Unit},
number = {3},
pages = {183--192},
title = {{On the approximate realization of continuous mappings by neural networks}},
volume = {2},
year = {1989}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Pedregosa et al. - 2012 - Scikit-learn Machine Learning in Python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
month = {jan},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490 http://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@phdthesis{Waals1873,
abstract = {De gewaarwordingen, die ik ondervind, nu ik geroepen ben dit proefschrift te verdedigen zijn van verschillenden aard. Heeft erkentelijkheid den boventoon jegens een Faculteit, aan welke ik mijn vorming heb te danken, en in het bijzonder jegens U, Hooggeleerde RIJKE, ..},
author = {van der Waals},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Waals - 1873 - De continuiteit van den gasen Vloeistoftoestand.pdf:pdf},
pages = {136},
title = {{De continuiteit van den gasen Vloeistoftoestand}},
year = {1873}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {::},
issn = {00280836},
journal = {Nature},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
number = {6088},
pages = {533--536},
publisher = {Nature Publishing Group},
title = {{Learning representations by back-propagating errors}},
url = {https://www-nature-com.uproxy.library.dc-uoit.ca/articles/323533a0},
volume = {323},
year = {1986}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network (??net) package. The construction and application of ANN potentials using ??net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite (??-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
volume = {114},
year = {2016}
}
@article{Dickson2013,
abstract = {Efforts to develop useful quantum computers have been blocked primarily by environmental noise. Quantum annealing is a scheme of quantum computation that is predicted to be more robust against noise, because despite the thermal environment mixing the system's state in the energy basis, the system partially retains coherence in the computational basis, and hence is able to establish well-defined eigenstates. Here we examine the environment's effect on quantum annealing using 16 qubits of a superconducting quantum processor. For a problem instance with an isolated small-gap anticrossing between the lowest two energy levels, we experimentally demonstrate that, even with annealing times eight orders of magnitude longer than the predicted single-qubit decoherence time, the probabilities of performing a successful computation are similar to those expected for a fully coherent system. Moreover, for the problem studied, we show that quantum annealing can take advantage of a thermal environment to achieve a speedup factor of up to 1,000 over a closed system. {\textcopyright} 2013 Macmillan Publishers Limited. All rights reserved.},
author = {Dickson, N. G. and Johnson, M. W. and Amin, M. H. and Harris, R. and Altomare, F. and Berkley, A. J. and Bunyk, P. and Cai, J. and Chapple, E. M. and Chavez, P. and Cioata, F. and Cirip, T. and Debuen, P. and Drew-Brook, M. and Enderud, C. and Gildert, S. and Hamze, F. and Hilton, J. P. and Hoskinson, E. and Karimi, K. and Ladizinsky, E. and Ladizinsky, N. and Lanting, T. and Mahon, T. and Neufeld, R. and Oh, T. and Perminov, I. and Petroff, C. and Przybysz, A. and Rich, C. and Spear, P. and Tcaciuc, A. and Thom, M. C. and Tolkacheva, E. and Uchaikin, S. and Wang, J. and Wilson, A. B. and Merali, Z. and Rose, G.},
doi = {10.1038/ncomms2920},
issn = {20411723},
journal = {Nature Communications},
number = {May},
pages = {1--6},
title = {{Thermally assisted quantum annealing of a 16-qubit problem}},
volume = {4},
year = {2013}
}
@article{Carr2016,
abstract = {Controlling molecule-surface interactions is key for chemical applications ranging from catalysis to gas sensing. We present a framework for accelerating the search for the global minimum on potential surfaces, corresponding to stable adsorbate-surface structures. We present a technique using Bayesian inference that enables us to predict converged density functional theory potential energies with fewer self-consistent field iterations. We then discuss how this technique fits in with the Bayesian Active Site Calculator, which applies Bayesian optimization to the problem. We demonstrate the performance of our framework using a hematite (Fe 2 O 3 ) surface and present the adsorption sites found by our global optimization method for various simple hydrocarbons on the rutile TiO 2 (110) surface.},
author = {Carr, S. F. and Garnett, R. and Lo, C. S.},
doi = {10.1063/1.4964671},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Carr, Garnett, Lo - 2016 - Accelerating the search for global minima on potential energy surfaces using machine learning.pdf:pdf},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
number = {15},
pages = {154106},
pmid = {27782478},
title = {{Accelerating the search for global minima on potential energy surfaces using machine learning}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/15/10.1063/1.4964671},
volume = {145},
year = {2016}
}
@article{Zhanga,
abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.},
archivePrefix = {arXiv},
arxivId = {1612.03242},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
eprint = {1612.03242},
file = {::},
month = {dec},
pages = {5908--5916},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.03242},
volume = {2017-Octob},
year = {2016}
}
@article{Weiss1992,
abstract = {Arthur Lee Samuel's (1901-90) early life, education, and career are described. Before World War II, at Bell Telephone Laboratories, he was a leading designer of microwave tubes, of which his TR radar switch, the Samuel tube, was the most widely used. At the University of Illinois he launched the ILLIAC team. He was one of those who guided IBM into computers and into real research, and he initiated its solid-state laboratory. He made a major improvement in the Williams storage tube. He invented hashing. He was chairman of the Defense Department Advisory Group on Electron Devices for 18 years. He started IBMs Zurich Laboratory and was instrumental in founding the IBM Journal of Research and Development},
author = {Weiss, E.A.},
doi = {10.1109/85.150082},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Weiss - 1992 - Biographies Eloge Arthur Lee Samuel (1901-90).pdf:pdf},
issn = {1058-6180},
journal = {IEEE Annals of the History of Computing},
number = {3},
pages = {55--69},
title = {{Biographies: Eloge: Arthur Lee Samuel (1901-90)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=150082},
volume = {14},
year = {1992}
}
@article{Heim215,
abstract = {Finding a solution to a problem often amounts to an optimization problem and thus can be recast in terms of the lowest-energy state of a system. To find such ground states, mathematical methods based on annealing were developed. To reach the ground state more quickly than with the earlier classical methods, a quantum-mechanical approach was proposed; however, the evidence for quantum speed-up is contradictory. Heim et al. show that the results depend on how the problem is described and how the optimization routine is implemented. This development should be valuable for benchmarking quantum machines.Science, this issue p. 215 Quantum annealers use quantum fluctuations to escape local minima and find low-energy configurations of a physical system. Strong evidence for superiority of quantum annealing (QA) has come from comparing QA implemented through quantum Monte Carlo (QMC) simulations to classical annealing. Motivated by recent experiments, we revisit the question of when quantum speedup may be expected. Although a better scaling is seen for QA in two-dimensional Ising spin glasses, this advantage is due to time discretization artifacts and measurements that are not possible on a physical quantum annealer. Simulations in the physically relevant continuous time limit, on the other hand, do not show superiority. Our results imply that care must be taken when using QMC simulations to assess the potential for quantum speedup.},
author = {Heim, Bettina and R{\o}nnow, Troels F and Isakov, Sergei V and Troyer, Matthias},
doi = {10.1126/science.aaa4170},
issn = {0036-8075},
journal = {Science},
number = {6231},
pages = {215--217},
publisher = {American Association for the Advancement of Science},
title = {{Quantum versus classical annealing of Ising spin glasses}},
url = {https://science.sciencemag.org/content/348/6231/215},
volume = {348},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Cireşan et al. - 2011 - High-Performance Neural Networks for Visual Object Classification.pdf:pdf},
isbn = {9781627480031},
issn = {2212-0173},
journal = {Procedia Technology},
keywords = {Deep learning,control,laser welding,prediction,reinforcement learning},
month = {feb},
number = {0},
pages = {474--483},
pmid = {7491034},
title = {{High-Performance Neural Networks for Visual Object Classification}},
url = {http://www.sciencedirect.com/science/article/pii/S2212017314001224 http://arxiv.org/abs/1102.0183},
volume = {15},
year = {2011}
}
@article{Cherukara2016,
abstract = {We introduce a bond order potential (BOP) for stanene based on an ab initio derived training data set. The potential is optimized to accurately describe the energetics, as well as thermal and mechanical properties of a free-standing sheet, and used to study diverse nanostructures of stanene, including tubes and ribbons. As a representative case study, using the potential, we perform molecular dynamics simulations to study stanene's structure and temperature-dependent thermal conductivity. We find that the structure of stanene is highly rippled, far in excess of other 2-D materials (e.g., graphene), owing to its low in-plane stiffness (stanene: ∼ 25 N/m; graphene: ∼ 480 N/m). The extent of stanene's rippling also shows stronger temperature dependence compared to that in graphene. Furthermore, we find that stanene based nanostructures have significantly lower thermal conductivity compared to graphene based structures owing to their softness (i.e., low phonon group velocities) and high anharmonic response. O...},
author = {Cherukara, Mathew J. and Narayanan, Badri and Kinaci, Alper and Sasikumar, Kiran and Gray, Stephen K. and Chan, Maria K.Y. and Sankaranarayanan, Subramanian K R S},
doi = {10.1021/acs.jpclett.6b01562},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Cherukara et al. - 2016 - Ab Initio -Based Bond Order Potential to Investigate Low Thermal Conductivity of Stanene Nanostructures.pdf:pdf},
issn = {1948-7185},
journal = {The Journal of Physical Chemistry Letters},
month = {oct},
number = {19},
pages = {3752--3759},
pmid = {27569053},
title = {{Ab Initio -Based Bond Order Potential to Investigate Low Thermal Conductivity of Stanene Nanostructures}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jpclett.6b01562},
volume = {7},
year = {2016}
}
@techreport{Zhang,
abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256×256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.},
archivePrefix = {arXiv},
arxivId = {1612.03242v2},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
eprint = {1612.03242v2},
file = {::},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {https://github.com/hanzhanggit/StackGAN.}
}
@article{Babin2012,
author = {Babin, Volodymyr and Medders, Gregory R. and Paesani, Francesco},
doi = {10.1021/jz3017733},
issn = {1948-7185},
journal = {The Journal of Physical Chemistry Letters},
month = {dec},
number = {24},
pages = {3765--3769},
title = {{Toward a Universal Water Model: First Principles Simulations from the Dimer to the Liquid Phase}},
url = {http://pubs.acs.org/doi/abs/10.1021/jz3017733},
volume = {3},
year = {2012}
}
@article{Khorshidi2016,
abstract = {Electronic structure calculations, such as those employing Kohn–Sham density functional theory or ab initio wavefunction theories, have allowed for atomistic-level understandings of a wide variety of phenomena and properties of matter at small scales. However, the computational cost of electronic structure methods drastically increases with length and time scales, which makes these methods difficult for long time-scale molecular dynamics simulations or large-sized systems. Machine-learning techniques can provide accurate potentials that can match the quality of electronic structure calculations, provided sufficient training data. These potentials can then be used to rapidly simulate large and long time-scale phenomena at similar quality to the parent electronic structure approach. Machine-learning potentials usually take a bias-free mathematical form and can be readily developed for a wide variety of systems. Electronic structure calculations have favorable properties–namely that they are noiseless and targeted training data can be produced on-demand–that make them particularly well-suited for machine learning. This paper discusses our modular approach to atomistic machine learning through the development of the open-source Atomistic Machine-learning Package (Amp), which allows for representations of both the total and atom-centered potential energy surface, in both periodic and non-periodic systems. Potentials developed through the atom-centered approach are simultaneously applicable for systems with various sizes. Interpolation can be enhanced by introducing custom descriptors of the local environment. We demonstrate this in the current work for Gaussian-type, bispectrum, and Zernike-type descriptors. Amp has an intuitive and modular structure with an interface through the python scripting language yet has parallelizable fortran components for demanding tasks; it is designed to integrate closely with the widely used Atomic Simulation Environment (ASE), which makes it compatible with a wide variety of commercial and open-source electronic structure codes. We finally demonstrate that the neural network model inside Amp can accurately interpolate electronic structure energies as well as forces of thousands of multi-species atomic systems. Program summary Program title: Amp Catalogue identifier: AFAK{\_}v1{\_}0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AFAK{\_}v1{\_}0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: yes No. of lines in distributed program, including test data, etc.: 21239 No. of bytes in distributed program, including test data, etc.: 1412975 Distribution format: tar.gz Programming language: Python, Fortran. Computer: PC, Mac. Operating system: Linux, Mac, Windows. Has the code been vectorized or parallelized?: Yes RAM: Variable, depending on the number and size of atomic systems. Classification: 16.1, 2.1. External routines: ASE, NumPy, SciPy, f2py, matplotlib Nature of problem: Atomic interactions within many-body systems typically have complicated functional forms, difficult to represent in simple pre-decided closed-forms. Solution method: Machine learning provides flexible functional forms that can be improved as new situations are encountered. Typically, interatomic potentials yield from machine learning simultaneously apply to different system sizes. Unusual features: Amp is as modular as possible, providing a framework for the user to create atomic environment descriptor and regression model at will. Moreover, it has Atomic Simulation Environment (ASE) interface, facilitating interactive collaboration with other electronic structure calculators within ASE. Running time: Variable, depending on the number and size of atomic systems.},
author = {Khorshidi, Alireza and Peterson, Andrew A},
doi = {10.1016/j.cpc.2016.05.010},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Atomic Simulation Environment (ASE),Density functional theory,Neural networks,Potential energy surface,Zernike polynomials},
pages = {310--324},
publisher = {Elsevier B.V.},
title = {{Amp: A modular approach to machine learning in atomistic simulations}},
url = {http://dx.doi.org/10.1016/j.cpc.2016.05.010},
volume = {207},
year = {2016}
}
@article{McGeoch2013,
abstract = {This paper describes an experimental study of a novel computing system (algorithm plus platform) that carries out quantum annealing, a type of adiabatic quantum computation, to solve optimization problems. We compare this system to three conventional software solvers, using instances from three NP-hard problem domains. We also describe experiments to learn how performance of the quantum annealing algorithm depends on input. Copyright 2013 ACM.},
author = {McGeoch, Catherine C. and Wang, Cong},
doi = {10.1145/2482767.2482797},
isbn = {9781450320535},
journal = {Proceedings of the ACM International Conference on Computing Frontiers, CF 2013},
keywords = {Adiabatic quantum computing,D-Wave,Heuristics,Quantum annealing},
title = {{Experimental evaluation of an adiabiatic quantum system for combinatorial optimization}},
year = {2013}
}
@article{Montavon2013,
abstract = {The combination of modern scientific computing with electronic structure theory can lead to an unprecedented amount of data amenable to intelligent data analysis for the identification of meaningful, novel and predictive structure–property relationships. Such relationships enable high-throughput screening for relevant properties in an exponentially growing pool of virtual compounds that are synthetically accessible. Here, we present a machine learning model, trained on a database of ab initio calculation results for thousands of organic molecules, that simultaneously predicts multiple electronic ground- and excited-state properties. The properties include atomization energy, polarizability, frontier orbital eigenvalues, ionization potential, electron affinity and excitation energies. The machine learning model is based on a deep multi-task artificial neural network, exploiting the underlying correlations between various molecular properties. The input is identical to ab initio methods, i.e. nuclear charges and Cartesian coordinates of all atoms. For small organic molecules, the accuracy of such a ‘quantum machine' is similar, and sometimes superior, to modern quantum-chemical methods—at negligible computational cost.},
archivePrefix = {arXiv},
arxivId = {1305.7074},
author = {Montavon, Gr{\'{e}}goire and Rupp, Matthias and Gobre, Vivekanand and Vazquez-Mayagoitia, Alvaro and Hansen, Katja and Tkatchenko, Alexandre and M{\"{u}}ller, Klaus Robert and {Anatole von Lilienfeld}, O},
doi = {10.1088/1367-2630/15/9/095003},
eprint = {1305.7074},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Montavon et al. - 2013 - Machine learning of molecular electronic properties in chemical compound space.pdf:pdf},
issn = {1367-2630},
journal = {New Journal of Physics},
month = {sep},
number = {9},
pages = {095003},
title = {{Machine learning of molecular electronic properties in chemical compound space}},
url = {http://stacks.iop.org/1367-2630/15/i=9/a=095003?key=crossref.c7515a05af17cccdbeec1c83340d4405},
volume = {15},
year = {2013}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {0028-0836},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Mnih2013,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://www.nature.com/doifinder/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Farimani2017,
abstract = {We have developed a new data-driven paradigm for the rapid inference, modeling and simulation of the physics of transport phenomena by deep learning. Using conditional generative adversarial networks (cGAN), we train models for the direct generation of solutions to steady state heat conduction and incompressible fluid flow purely on observation without knowledge of the underlying governing equations. Rather than using iterative numerical methods to approximate the solution of the constitutive equations, cGANs learn to directly generate the solutions to these phenomena, given arbitrary boundary conditions and domain, with high test accuracy (MAE{\$}{\textless}{\$}1{\$}\backslash{\$}{\%}) and state-of-the-art computational performance. The cGAN framework can be used to learn causal models directly from experimental observations where the underlying physical model is complex or unknown.},
archivePrefix = {arXiv},
arxivId = {1709.02432},
author = {Farimani, Amir Barati and Gomes, Joseph and Pande, Vijay S},
eprint = {1709.02432},
title = {{Deep Learning the Physics of Transport Phenomena}},
url = {http://arxiv.org/abs/1709.02432},
volume = {94305},
year = {2017}
}
@article{Curtarolo2003,
abstract = {Predicting and characterizing the crystal structure of materials is a key problem in materials research and development. It is typically addressed with highly accurate quantum mechanical computations on a small set of candidate structures, or with empirical rules that have been extracted from a large amount of experimental information, but have limited predictive power. In this Letter, we transfer the concept of heuristic rule extraction to a large library of ab initio calculated information, and we demonstrate that this can be developed into a tool for crystal structure prediction.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0307262},
author = {Curtarolo, Stefano and Morgan, Dane and Persson, Kristin and Rodgers, John and Ceder, Gerbrand},
doi = {10.1103/PhysRevLett.91.135503},
eprint = {0307262},
isbn = {0031-9007},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {sep},
number = {13},
pages = {135503},
pmid = {14525315},
primaryClass = {cond-mat},
title = {{Predicting crystal structures with data mining of quantum calculations.}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.91.135503},
volume = {91},
year = {2003}
}
@article{Farhi2001,
abstract = {A quantum system will stay near its instantaneous ground state if the Hamiltonian that governs its evolution varies slowly enough. This quantum adiabatic behavior is the basis of a new class of algorithms for quantum computing. We test one such algorithm by applying it to randomly generated, hard, instances of an NP-complete problem. For the small examples that we can simulate, the quantum adiabatic algorithm works well, and provides evidence that quantum computers (if large ones can be built) may be able to outperform ordinary computers on hard sets of instances of NP-complete problems.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0104129},
author = {Farhi, E and Goldstone, J and Gutmann, S and Lapan, J and Lundgren, A and Preda, D},
doi = {10.1126/science.1057726},
eprint = {0104129},
issn = {0036-8075},
journal = {Science},
number = {5516},
pages = {472--475},
primaryClass = {quant-ph},
title = {{A Quantum Adiabatic Evolution Algorithm Applied to Random Instances of an NP-Complete Problem}},
volume = {292},
year = {2001}
}
@techreport{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Ca, James Bergstra@umontreal and Ca, Yoshua Bengio@umontreal},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization Yoshua Bengio}},
url = {http://scikit-learn.sourceforge.net.},
volume = {13},
year = {2012}
}
@inproceedings{Krizhevsky2012,
author = {Alex, Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Neural Information Processing Systems (NIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Lundgaard2016,
abstract = {We propose a general-purpose semilocal/nonlocal exchange-correlation functional approximation, named mBEEF-vdW. The exchange is a meta generalized gradient approximation, and the correlation is a semilocal and nonlocal mixture, with the Rutgers-Chalmers approximation for van der Waals (vdW) forces. The functional is fitted within the Bayesian error estimation functional (BEEF) framework [J. Wellendorff et al., Phys. Rev. B 85, 235149 (2012); J. Wellendorff et al., J. Chem. Phys. 140, 144107 (2014)]. We improve the previously used fitting procedures by introducing a robust MM-estimator based loss function, reducing the sensitivity to outliers in the datasets. To more reliably determine the optimal model complexity, we furthermore introduce a generalization of the bootstrap 0.632 estimator with hierarchical bootstrap sampling and geometric mean estimator over the training datasets. Using this estimator, we show that the robust loss function leads to a 10{\%} improvement in the estimated prediction error over the previously used least-squares loss function. The mBEEF-vdW functional is benchmarked against popular density functional approximations over a wide range of datasets relevant for heterogeneous catalysis, including datasets that were not used for its training. Overall, we find that mBEEF-vdW has a higher general accuracy than competing popular functionals, and it is one of the best performing functionals on chemisorption systems, surface energies, lattice constants, and dispersion. We also show the potential-energy curve of graphene on the nickel(111) surface, where mBEEF-vdW matches the experimental binding length. mBEEF-vdW is currently available in gpaw and other density functional theory codes through Libxc, version 3.0.0.},
author = {Lundgaard, Keld T. and Wellendorff, Jess and Voss, Johannes and Jacobsen, Karsten W. and Bligaard, Thomas},
doi = {10.1103/PhysRevB.93.235162},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Lundgaard et al. - 2016 - MBEEF-vdW Robust fitting of error estimation density functionals.pdf:pdf},
isbn = {2469-9950},
issn = {1550235X},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {23},
pages = {1--16},
title = {{MBEEF-vdW: Robust fitting of error estimation density functionals}},
volume = {93},
year = {2016}
}
@article{Casert2020,
abstract = {Machine learning provides a novel avenue for the study of experimental realizations of many-body systems, and has recently been proven successful in analyzing properties of experimental data of ultracold quantum gases. We here show that deep learning succeeds in the more challenging task of modelling such an experimental data distribution. Our generative model (RUGAN) is able to produce snapshots of a doped two-dimensional Fermi-Hubbard model that are indistinguishable from previously reported experimental realizations. Importantly, it is capable of accurately generating snapshots at conditions for which it did not observe any experimental data, such as at higher doping values. On top of that, our generative model extracts relevant patterns from small-scale examples and can use these to construct new configurations at a larger size that serve as a precursor to observations at scales that are currently experimentally inaccessible. The snapshots created by our model-which come at effectively no cost-are extremely useful as they can be employed to quantitatively test new theoretical developments under conditions that have not been explored experimentally, parameterize phenomenological models, or train other, more data-intensive, machine learning methods. We provide predictions for experimental observables at unobserved conditions and benchmark these against modern theoretical frameworks. The deep learning method we develop here is broadly applicable and can be used for the efficient large-scale simulation of equilibrium and nonequilibrium physical systems.},
archivePrefix = {arXiv},
arxivId = {2002.07055},
author = {Casert, Corneel and Mills, Kyle and Vieijra, Tom and Ryckebusch, Jan and Tamblyn, Isaac},
eprint = {2002.07055},
file = {:Users/kmills/Desktop/external{\_}s/2002.07055.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--11},
title = {{Optical lattice experiments at unobserved conditions and scales through generative adversarial deep learning}},
year = {2020}
}
@article{Meredig2014,
abstract = {Typically, computational screens for new materials sharply constrain the compositional search space, structural search space, or both, for the sake of tractability. To lift these constraints, we construct a machine learning model from a database of thousands of density functional theory (DFT) calculations. The resulting model can predict the thermodynamic stability of arbitrary compositions without any other input and with six orders of magnitude less computer time than DFT. We use this model to scan roughly 1.6 million candidate compositions for novel ternary compounds (AxByCz), and predict 4500 new stable materials. Our method can be readily applied to other descriptors of interest to accelerate domain-specific materials discovery.},
author = {Meredig, B. and Agrawal, A. and Kirklin, S. and Saal, J. E. and Doak, J. W. and Thompson, A. and Zhang, K. and Choudhary, A. and Wolverton, C.},
doi = {10.1103/PhysRevB.89.094104},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Meredig et al. - 2014 - Combinatorial screening for new materials in unconstrained composition space with machine learning.pdf:pdf},
isbn = {1098-0121},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {9},
pages = {1--7},
title = {{Combinatorial screening for new materials in unconstrained composition space with machine learning}},
volume = {89},
year = {2014}
}
@phdthesis{Hochreiter1991,
author = {Hochreiter, Sepp},
school = {TU Munich},
title = {{Untersuchungen zu dynamischen neuronalen Netzen}},
type = {Diploma Thesis},
year = {1991}
}
@article{Hubel1968,
abstract = {SUMMARY 1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimu-lating the retinas with spots or patterns of light. Most cells can be cate-gorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded. 2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent. 3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other. 4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with) by guest on August 5, 2009 jp.physoc.org Downloaded from J Physiol (},
author = {Hubel, D H and Wiesel, T N},
doi = {10.1113/jphysiol.1968.sp008455},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Hubel, Wiesel - 1968 - Receptive fields and functional architecture of monkey striate cortex.pdf:pdf},
isbn = {0022-3077 (Print)$\backslash$r0022-3077 (Linking)},
issn = {00223751},
journal = {The Journal of Physiology},
keywords = {Classified neurons in area 17 of macaques as simpl,VC MP 102,XXa few color sens.cells found.Columns with cells,monkey,rfhb1},
month = {mar},
number = {1},
pages = {215--243},
pmid = {7931532},
title = {{Receptive fields and functional architecture of monkey striate cortex}},
url = {http://doi.wiley.com/10.1113/jphysiol.1968.sp008455},
volume = {195},
year = {1968}
}
@article{Bengio:2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
address = {Hanover, MA, USA},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
month = {jan},
number = {1},
pages = {1--127},
pmid = {17348934},
publisher = {Now Publishers Inc.},
title = {{Learning Deep Architectures for AI}},
url = {http://dx.doi.org/10.1561/2200000006 http://www.nowpublishers.com/article/Details/MAL-006},
volume = {2},
year = {2009}
}
@article{Albarr_n_Arriagada_2020,
abstract = {The characterization of an operator by its eigenvectors and eigenvalues allows us to know its action over any quantum state. Here, we propose a protocol to obtain an approximation of the eigenvectors of an arbitrary Hermitian quantum operator. This protocol is based on measurement and feedback processes, which characterize a reinforcement learning protocol. Our proposal is composed of two systems, a black box named environment and a quantum state named agent. The role of the environment is to change any quantum state by a unitary matrix where is a Hermitian operator, and $\tau$ is a real parameter. The agent is a quantum state which adapts to some eigenvector of by repeated interactions with the environment, feedback process, and semi-random rotations. With this proposal, we can obtain an approximation of the eigenvectors of a random qubit operator with average fidelity over 90{\%} in less than 10 iterations, and surpass 98{\%} in less than 300 iterations. Moreover, for the two-qubit cases, the four eigenvectors are obtained with fidelities above 89{\%} in 8000 iterations for a random operator, and fidelities of 99{\%} for an operator with the Bell states as eigenvectors. This protocol can be useful to implement semi-autonomous quantum devices which should be capable of extracting information and deciding with minimal resources and without human intervention.},
author = {Albarr{\'{a}}n-Arriagada, F and Retamal, J C and Solano, E and Lamata, L},
doi = {10.1088/2632-2153/ab43b4},
journal = {Machine Learning: Science and Technology},
month = {feb},
number = {1},
pages = {15002},
publisher = {{\{}IOP{\}} Publishing},
title = {{Reinforcement learning for semi-autonomous approximate quantum eigensolver}},
url = {https://doi.org/10.1088{\%}2F2632-2153{\%}2Fab43b4},
volume = {1},
year = {2020}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchan...},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Morawietz, Behler - 2013 - A density-functional theory-based neural network potential for water clusters including van der waals corr(2).pdf:pdf},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
month = {aug},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
url = {http://pubs.acs.org/doi/abs/10.1021/jp401225b},
volume = {117},
year = {2013}
}
@article{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
file = {::},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Continuous control with deep reinforcement learning}},
url = {https://goo.gl/J4PIAz http://arxiv.org/abs/1509.02971},
year = {2015}
}
@techreport{Larochelle,
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
file = {::},
title = {{An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation}}
}
@article{cole2007,
abstract = {We have developed a classical two- and three-body interaction potential to simulate the hydroxylated, natively oxidized Si surface in contact with water solutions, based on the combination and extension of the Stillinger-Weber potential and of a potential originally developed to simulate SiO(2) polymorphs. The potential parameters are chosen to reproduce the structure, charge distribution, tensile surface stress, and interactions with single water molecules of a natively oxidized Si surface model previously obtained by means of accurate density functional theory simulations. We have applied the potential to the case of hydrophilic silicon wafer bonding at room temperature, revealing maximum room temperature work of adhesion values for natively oxidized and amorphous silica surfaces of 97 and 90 mJm(2), respectively, at a water adsorption coverage of approximately 1 ML. The difference arises from the stronger interaction of the natively oxidized surface with liquid water, resulting in a higher heat of immersion (203 vs 166 mJm(2)), and may be explained in terms of the more pronounced water structuring close to the surface in alternating layers of larger and smaller densities with respect to the liquid bulk. The computed force-displacement bonding curves may be a useful input for cohesive zone models where both the topographic details of the surfaces and the dependence of the attractive force on the initial surface separation and wetting can be taken into account.},
archivePrefix = {arXiv},
arxivId = {0807.3215},
author = {Cole, Daniel J. and Payne, Mike C. and Cs{\'{a}}nyi, G{\'{a}}bor and {Mark Spearing}, S. and {Colombi Ciacchi}, Lucio},
doi = {10.1063/1.2799196},
eprint = {0807.3215},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Cole et al. - 2007 - Development of a classical force field for the oxidized Si surface Application to hydrophilic wafer bonding.pdf:pdf},
isbn = {0021-9606},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
month = {nov},
number = {20},
pages = {204704},
pmid = {18052443},
title = {{Development of a classical force field for the oxidized Si surface: Application to hydrophilic wafer bonding}},
url = {http://aip.scitation.org/doi/10.1063/1.2799196},
volume = {127},
year = {2007}
}
@article{Wang2001,
abstract = {A new, general, efficient Monte Carlo (MC) algorithm that offers substantial advantages over existing approaches is presented. For demonstration purposes, the application of the algorithm to the 2D ten state Potts model and Ising model is discussed.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0011174},
author = {Wang, Fugao and Landau, D P},
doi = {10.1103/PhysRevLett.86.2050},
eprint = {0011174},
issn = {00319007},
journal = {Physical Review Letters},
number = {10},
pages = {2050--2053},
primaryClass = {cond-mat},
title = {{Efficient, multiple-range random walk algorithm to calculate the density of states}},
volume = {86},
year = {2001}
}
@article{Rupp2012,
abstract = {We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schr{\"{o}}dinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10 kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.},
author = {Rupp, Matthias and Tkatchenko, Alexandre and M{\"{u}}ller, Klaus Robert and von Lilienfeld, O. Anatole},
doi = {10.1103/PhysRevLett.108.058301},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Rupp et al. - 2012 - Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning.pdf:pdf},
isbn = {1079-7114 (Electronic)$\backslash$r0031-9007 (Linking)},
issn = {0031-9007},
journal = {Physical Review Letters},
keywords = {Learning/Statistics {\&} Optimisation},
month = {jan},
number = {5},
pages = {058301},
pmid = {22400967},
title = {{Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning}},
url = {http://eprints.pascal-network.org/archive/00009418/ http://link.aps.org/doi/10.1103/PhysRevLett.108.058301 https://link.aps.org/doi/10.1103/PhysRevLett.108.058301},
volume = {108},
year = {2012}
}
@article{Tian2017,
author = {Tian, Wenliang and Meng, Fandi and Liu, Li and Li, Ying and Wang, Fuhui},
doi = {10.1038/srep40827},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Tian et al. - 2017 - Lifetime prediction for organic coating under alternating hydrostatic pressure by artificial neural network.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
month = {jan},
number = {January},
pages = {40827},
publisher = {Nature Publishing Group},
title = {{Lifetime prediction for organic coating under alternating hydrostatic pressure by artificial neural network}},
url = {http://www.nature.com/articles/srep40827},
volume = {7},
year = {2017}
}
@article{Sherrington1975,
abstract = {Amorphous Co2Ge was synthesized by ball milling crystalline Co2Ge, a state which cannot be obtained by traditional rapid solidification. It exhibits a single transition from the paramagnetic to the spin-glass state at 41 K (Tf) as defined by a sharp asymmetric cusp in the lowest field ac susceptibility. In fields 30 Oe the cusp in both ac and dc susceptibility curves becomes a rounded maximum and Tf decreases. Below Tf the magnetization reveals thermal hysteresis. Using the Sherrington-Kirkpatrick theory the spin-glass order parameter is derived and a spin-glass phase was synthesized by mechanical milling. This phase can be described by mean-field theory. {\textcopyright}1994 The American Physical Society.},
author = {Sherrington, David and Kirkpatrick, Scott},
doi = {10.1103/PhysRevLett.35.1792},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {dec},
number = {26},
pages = {1792--1796},
title = {{Solvable Model of a Spin-Glass}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792},
volume = {35},
year = {1975}
}
@techreport{Srivastava2014a,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Mehta2014,
abstract = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
archivePrefix = {arXiv},
arxivId = {1410.3831},
author = {Dieleman, Sander and Willett, Kyle W. and Dambre, Joni},
doi = {10.1093/mnras/stv632},
eprint = {1410.3831},
isbn = {1410.3831},
issn = {1365-2966},
journal = {Monthly Notices of the Royal Astronomical Society},
month = {jun},
number = {2},
pages = {1441--1459},
pmid = {7491034},
title = {{Rotation-invariant convolutional neural networks for galaxy morphology prediction}},
url = {http://arxiv.org/abs/1301.3124 http://arxiv.org/abs/1410.3831 http://academic.oup.com/mnras/article/450/2/1441/979677/Rotationinvariant-convolutional-neural-networks},
volume = {450},
year = {2015}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Start with raw spin configurations of a prototypical Ising model, we use principle component analysis to extract relevant low dimensional representations the original data, and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor as indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
issn = {2469-9950},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@article{Bergmann2017,
abstract = {This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribution by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scalable and it can generate output images of arbitrary large size.},
archivePrefix = {arXiv},
arxivId = {1705.06566},
author = {Bergmann, Urs and Jetchev, Nikolay and Vollgraf, Roland},
eprint = {1705.06566},
title = {{Learning Texture Manifolds with the Periodic Spatial GAN}},
url = {http://arxiv.org/abs/1705.06566},
year = {2017}
}
@article{Pozun2012,
abstract = {We present a method for optimizing transition state theory dividing surfaces with support vector machines. The resulting dividing surfaces require no a priori information or intuition about reaction mechanisms. To generate optimal dividing surfaces, we apply a cycle of machine-learning and refinement of the surface by molecular dynamics sampling. We demonstrate that the machine-learned surfaces contain the relevant low-energy saddle points. The mechanisms of reactions may be extracted from the machine-learned surfaces in order to identify unexpected chemically relevant processes. Furthermore, we show that the machine-learned surfaces significantly increase the transmission coefficient for an adatom exchange involving many coupled degrees of freedom on a (100) surface when compared to a distance-based dividing surface.},
author = {Pozun, Zachary D. and Hansen, Katja and Sheppard, Daniel and Rupp, Matthias and M{\"{u}}ller, Klaus Robert and Henkelman, Graeme},
doi = {10.1063/1.4707167},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Pozun et al. - 2012 - Optimizing transition states via kernel-based machine learning.pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pmid = {22583204},
title = {{Optimizing transition states via kernel-based machine learning}},
volume = {136},
year = {2012}
}
@article{Agostinelli2019,
author = {Agostinelli, Forest and McAleer, Stephen and Shmakov, Alexander and Baldi, Pierre},
doi = {10.1038/s42256-019-0070-z},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
month = {aug},
number = {8},
pages = {356--363},
title = {{Solving the Rubik's cube with deep reinforcement learning and search}},
url = {http://www.nature.com/articles/s42256-019-0070-z},
volume = {1},
year = {2019}
}
@book{minsky69perceptrons,
address = {Cambridge, MA, USA},
author = {Minsky, Marvin and Papert, Seymour},
keywords = {linear-classification neural-networks seminal},
publisher = {MIT Press},
title = {{Perceptrons: An Introduction to Computational Geometry}},
year = {1969}
}
@article{Shen2016,
abstract = {Molecular dynamics simulation with multiscale quantum mechanics / molecular mechanics (QM/MM) meth-ods is a very powerful tool for understanding the mechanism of chemical and biological processes in solution or enzymes. However, its computational cost can be too high for many biochemical systems because of the large number of ab initio QM calculations. Semi-empirical QM/MM simulations have much higher efficiency. Its accuracy can be improved with a correction to reach the ab initio QM/MM level. The computational cost on the ab initio calculation for the correction determines the efficiency. In this paper we developed a neural network method for QM/MM calculation as an extension of the neural-network representation reported by Behler and Parrinello. With this approach, the potential energy of any configuration along the reaction path for a given QM/MM system can be predicted at the ab initio QM/MM level based on the semi-empirical QM/MM simu-lations. We further applied this method to three reactions in water to calculate the free energy changes. The free-energy profile obtained from the semi-empirical QM/MM simulation is corrected to the ab initio QM/MM level with the potential energies predicted with the constructed neural network. The results are in excellent accordance with the reference data that are obtained from ab initio QM/MM molecular dynamics simulation or corrected with direct ab initio QM/MM potential energies. Compared with the correction using direct ab initio QM/MM potential energies, our method shows a speed-up of one or two order of magnitude. It demonstrates that neural network method combined with the semi-empirical QM/MM calculation can be an efficient and reliable strategy for chemical reaction simulations.},
author = {Shen, Lin and Wu, Jingheng and Yang, Weitao},
doi = {10.1021/acs.jctc.6b00663},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Shen, Wu, Yang - 2016 - Multiscale Quantum MechanicsMolecular Mechanics Simulations with Neural Networks.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {10},
pages = {4934--4946},
pmid = {27552235},
title = {{Multiscale Quantum Mechanics/Molecular Mechanics Simulations with Neural Networks}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jctc.6b00663},
volume = {12},
year = {2016}
}
@inproceedings{Frid-Adar2018,
abstract = {In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6{\%} sensitivity and 88.4{\%} specificity. By adding the synthetic data augmentation the results significantly increased to 85.7{\%} sensitivity and 92.4{\%} specificity.},
archivePrefix = {arXiv},
arxivId = {1801.02385},
author = {Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2018.8363576},
eprint = {1801.02385},
isbn = {9781538636367},
issn = {19458452},
keywords = {Data augmentation,Generative adversarial network,Image synthesis,Lesion classification,Liver lesions},
month = {may},
pages = {289--293},
publisher = {IEEE Computer Society},
title = {{Synthetic data augmentation using GAN for improved liver lesion classification}},
volume = {2018-April},
year = {2018}
}
@article{Baldi2015,
abstract = {The Higgs boson is thought to provide the interaction that imparts mass to the fundamental fermions, but while measurements at the Large Hadron Collider (LHC) are consistent with this hypothesis, current analysis techniques lack the statistical power to cross the traditional 5$\sigma$ significance barrier without more data. Deep learning techniques have the potential to increase the statistical power of this analysis by automatically learning complex, high-level data representations. In this work, deep neural networks are used to detect the decay of the Higgs boson to a pair of tau leptons. A Bayesian optimization algorithm is used to tune the network architecture and training algorithm hyperparameters, resulting in a deep network of eight nonlinear processing layers that improves upon the performance of shallow classifiers even without the use of features specifically engineered by physicists for this application. The improvement in discovery significance is equivalent to an increase in the accumulated data set of 25{\%}.},
archivePrefix = {arXiv},
arxivId = {1410.3469},
author = {Baldi, P. and Sadowski, P. and Whiteson, D.},
doi = {10.1103/PhysRevLett.114.111801},
eprint = {1410.3469},
issn = {10797114},
journal = {Physical Review Letters},
number = {11},
pages = {1--5},
pmid = {25839260},
title = {{Enhanced Higgs boson to ??+??- search with deep learning}},
volume = {114},
year = {2015}
}
@article{Bunyk2014,
abstract = {We have developed a quantum annealing processor, based on an array of tunable coupled rf-SQUID flux qubits, fabricated in a superconducting integrated circuit process. Implementing this type of processor at a scale of 512 qubits and 1472 programmable interqubit couplers and operating at ∼ 20 mK has required attention to a number of considerations that one may ignore at the smaller scale of a few dozen or so devices. Here, we discuss some of these considerations, and the delicate balance necessary for the construction of a practical processor that respects the demanding physical requirements imposed by a quantum algorithm. In particular, we will review some of the design tradeoffs at play in the floor planning of the physical layout, driven by the desire to have an algorithmically useful set of interqubit couplers, and the simultaneous need to embed programmable control circuitry into the processor fabric. In this context, we have developed a new ultralow-power embedded superconducting digital-to-analog flux converter (DAC) used to program the processor with zero static power dissipation, optimized to achieve maximum flux storage density per unit area. The 512 single-stage, 3520 two-stage, and 512 three-stage flux DACs are controlled with an XYZ addressing scheme requiring 56 wires. Our estimate of on-chip dissipated energy for worst-case reprogramming of the whole processor is ∼65 fJ. Several chips based on this architecture have been fabricated and operated successfully at our facility, as well as two outside facilities (see, for example, the recent reporting by Jones).},
archivePrefix = {arXiv},
arxivId = {1401.5504},
author = {Bunyk, P. I. and Hoskinson, Emile M. and Johnson, Mark W. and Tolkacheva, Elena and Altomare, Fabio and Berkley, Andrew J. and Harris, Richard and Hilton, Jeremy P. and Lanting, Trevor and Przybysz, Anthony J. and Whittaker, Jed},
doi = {10.1109/TASC.2014.2318294},
eprint = {1401.5504},
issn = {10518223},
journal = {IEEE Transactions on Applied Superconductivity},
keywords = {Computational physics,quantum computing,superconducting integrated circuits},
number = {4},
pages = {1--10},
publisher = {IEEE},
title = {{Architectural Considerations in the Design of a Superconducting Quantum Annealing Processor}},
volume = {24},
year = {2014}
}
@article{VanMilligen1995,
author = {van Milligen, B. Ph. and Tribaldos, V. and Jim{\'{e}}nez, J. A.},
doi = {10.1103/PhysRevLett.75.3594},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/van Milligen, Tribaldos, Jim{\'{e}}nez - 1995 - Neural Network Differential Equation and Plasma Equilibrium Solver.pdf:pdf},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {nov},
number = {20},
pages = {3594--3597},
pmid = {10059679},
title = {{Neural Network Differential Equation and Plasma Equilibrium Solver}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.75.3594},
volume = {75},
year = {1995}
}
@article{Salimans2016,
author = {Salimans, Tim and Goodfellow, Ian and Cheung, Vicki and Radford, Alec and Chen, Xi},
number = {Nips},
pages = {1--9},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@article{Berner2019,
author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\c{e}}biak, Przemys{\l}aw Psyho and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'{o}}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pond{\'{e}}, Henrique and Pinto, De Oliveira and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
title = {{Dota 2 with Large Scale Deep Reinforcement Learning}},
year = {2019}
}
@article{Chen2016,
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
number = {Nips},
title = {{InfoGAN : Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
year = {2016}
}
@article{Mills2017a,
abstract = {We train a deep convolutional neural network to accurately predict the energies and magnetizations of Ising model configurations, using both the traditional nearest-neighbour Hamiltonian, as well as a long-range screened Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbour energy of the 4x4 Ising model. Using its success at this task, we motivate the study of the larger 8x8 Ising model, showing that the deep neural network can learn the nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. Finally, we teach the convolutional deep neural network to accurately predict a long-range interaction through a screened Coulomb Hamiltonian. In this case, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, 1600 times faster than a CUDA-optimized "exact" calculation.},
archivePrefix = {arXiv},
arxivId = {1706.09779},
author = {Mills, Kyle and Tamblyn, Isaac},
eprint = {1706.09779},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills, Tamblyn - 2017 - Deep neural networks for direct, featureless learning through observation the case of 2d spin models(2).pdf:pdf},
month = {jun},
title = {{Deep neural networks for direct, featureless learning through observation: the case of 2d spin models}},
url = {http://arxiv.org/abs/1706.09779},
year = {2017}
}
@article{Mills2017b,
author = {Mills, Kyle and Spanner, Michael and Tamblyn, Isaac},
doi = {10.1103/PhysRevA.96.042113},
issn = {2469-9926},
journal = {Physical Review A},
month = {oct},
number = {4},
pages = {042113},
title = {{Deep learning and the Schr{\"{o}}dinger equation}},
url = {https://link.aps.org/doi/10.1103/PhysRevA.96.042113},
volume = {96},
year = {2017}
}
@article{Balabin2009,
abstract = {Artificial neural network (ANN) approach has been applied to estimate the density functional theory (DFT) energy with large basis set using lower-level energy values and molecular descriptors. A total of 208 different molecules were used for the ANN training, cross validation, and testing by applying BLYP, B3LYP, and BMK density functionals. Hartree-Fock results were reported for comparison. Furthermore, constitutional molecular descriptor (CD) and quantum-chemical molecular descriptor (QD) were used for building the calibration model. The neural network structure optimization, leading to four to five hidden neurons, was also carried out. The usage of several low-level energy values was found to greatly reduce the prediction error. An expected error, mean absolute deviation, for ANN approximation to DFT energies was 0.6+/-0.2 kcal mol(-1). In addition, the comparison of the different density functionals with the basis sets and the comparison of multiple linear regression results were also provided. The CDs were found to overcome limitation of the QD. Furthermore, the effective ANN model for DFT/6-311G(3df,3pd) and DFT/6-311G(2df,2pd) energy estimation was developed, and the benchmark results were provided.},
author = {Balabin, Roman M. and Lomakina, Ekaterina I.},
doi = {10.1063/1.3206326},
isbn = {1089-7690 (Electronic)$\backslash$r0021-9606 (Linking)},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {7},
pmid = {19708729},
title = {{Neural network approach to quantum-chemistry data: Accurate prediction of density functional theory energies}},
volume = {131},
year = {2009}
}
@article{Weiner1984,
abstract = {We present the development of a force field for simulation of nucleic acids and proteins. Our approach began by obtaining equilibrium bond lengths and angles from microwave, neutron diffraction, and prior molecular mechanical calculations, torsional constants from microwave, NMR, and molecular mechanical studies, nonbonded parameters from crystal packing calculations, and atomic charges from the fit of a partial charge model to electrostatic potentials calculated by ab initio quantum mechanical theory. The parameters were then refined with molecular mechanical studies on the structures and energies of model compounds. For nucleic acids, we focused on methyl ethyl ether, tetrahydrofuran, deoxyadenosine, dimethyl phosphate, 9-methylguanine-l -methylcytosine hydrogen-bonded complex, 9-methyladenine-1 -methylthymine hydrogen-bonded complex, and 1,3-dimethyluracil base-stacked dimer. Bond, angle, torsional, nonbonded, and hydrogen-bond parameters were varied to optimize the agreement between calculated and experimental values for sugar pucker energies and structures, vibrational frequencies of dimethyl phosphate and tetrahydrofuran, and energies for base pairing and base stacking. For proteins, we focused on maps of glycyl and alanyl dipeptides, hydrogen-bonding interactions involving the various protein polar groups, and energy refinement calculations on insulin. Unlike the models for hydrogen bonding involving nitrogen and oxygen electron donors, an adequate description of sulfur hydrogen bonding required explicit inclusion of lone pairs.},
author = {Weiner, Scott J and Kollman, Peter A and Case, David A and Singh, U Chandra and Ghio, Caterina and Alagona, Guliano and Profeta, Salvatore and Weiner, Paul and Alagona, Giuliano and Weinerl, Paul},
doi = {10.1021/ja00315a051},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Weiner et al. - 1984 - A new force field for molecular mechanical simulation of nucleic acids and proteins A New Force Field for Molecul.pdf:pdf},
isbn = {0002-7863},
issn = {0002-7863},
journal = {Journal of the American Chemical Society},
month = {feb},
number = {3},
pages = {765--784},
title = {{A new force field for molecular mechanical simulation of nucleic acids and proteins A New Force Field for Molecular Mechanical Simulation of Nucleic Acids and Proteins}},
url = {http://pubs.acs.org/doi/abs/10.1021/ja00315a051},
volume = {106},
year = {1984}
}
@article{Li2013,
abstract = {The paper suggests a new method that combines the Kennard and Stone algorithm (Kenstone, KS), hierarchical clustering (HC), and ant colony optimization (ACO)-based extreme learning machine (ELM) (KS-HC/ACO-ELM) with the density functional theory (DFT) B3LYP/6-31G(d) method to improve the accuracy of DFT calculations for the Y-NO homolysis bond dissociation energies (BDE). In this method, Kenstone divides the whole data set into two parts, the training set and the test set; HC and ACO are used to perform the cluster analysis on molecular descriptors; correlation analysis is applied for selecting the most correlated molecular descriptors in the classes, and ELM is the nonlinear model for establishing the relationship between DFT calculations and homolysis BDE experimental values. The results show that the standard deviation of homolysis BDE in the molecular test set is reduced from 4.03 kcal mol −1 calculated by the DFT B3LYP/6-31G(d) method to 0.30, 0.28, 0.29, and 0.32 kcal mol −1 by the KS-ELM, KS-HC-ELM, and KS-ACO-ELM methods and the artificial neural network (ANN) combined with KS-HC, respectively. This method predicts accurate values with much higher efficiency when compared to the larger basis set DFT calculation and may also achieve similarly accurate calculation results for larger molecules.},
author = {Li, Hong Zhi and Li, Lin and Zhong, Zi Yan and Han, Yi and Hu, Lihong and Lu, Ying Hua},
doi = {10.1155/2013/860357},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2013 - An Accurate and Efficient Method to Predict Y-NO Bond Homolysis Bond Dissociation Energies.pdf:pdf},
issn = {1024-123X},
journal = {Mathematical Problems in Engineering},
pages = {1--10},
title = {{An Accurate and Efficient Method to Predict Y-NO Bond Homolysis Bond Dissociation Energies}},
url = {http://www.hindawi.com/journals/mpe/2013/860357/},
volume = {2013},
year = {2013}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
month = {jul},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@misc{stable-baselines,
author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
booktitle = {GitHub repository},
howpublished = {$\backslash$url{\{}https://github.com/hill-a/stable-baselines{\}}},
publisher = {GitHub},
title = {{Stable Baselines}},
year = {2018}
}
@article{VanBeest1990,
abstract = {We address the problem of finding interatomic force fields for silicas from ab initio calculations on small clusters. It is shown that the force field cannot be determined from cluster data alone; incorporation of bulk-system information into the force field remains essential. Bearing this in mind, we derive a force field based on both microscopic (ab initio) and macroscopic (experimental) data. This force field combines accuracy with transferability to other polymorphs. The possibility of parametrizing other elements is also demonstrated. CN  - 0950},
author = {{Van Beest}, B. W H and Kramer, G. J. and {Van Santen}, R. A.},
doi = {10.1103/PhysRevLett.64.1955},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Van Beest, Kramer, Van Santen - 1990 - Force fields for silicas and aluminophosphates based on ab initio calculations.pdf:pdf},
isbn = {0031-9007},
issn = {00319007},
journal = {Physical Review Letters},
number = {16},
pages = {1955--1958},
pmid = {10041537},
title = {{Force fields for silicas and aluminophosphates based on ab initio calculations}},
volume = {64},
year = {1990}
}
@article{Finnila1994,
author = {Finnila, A B and Gomez, M A and Sebenik, C and Stenson, C and Doll, J D},
doi = {10.1016/0009-2614(94)00117-0},
issn = {00092614},
journal = {Chemical Physics Letters},
month = {mar},
number = {5-6},
pages = {343--348},
title = {{Quantum annealing: A new method for minimizing multidimensional functions}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0009261494001170},
volume = {219},
year = {1994}
}
@article{Goodfellow,
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/abs/1406.2661v1},
author = {Goodfellow, Ian J and Pouget-abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-farley, David},
eprint = {/arxiv.org/abs/1406.2661v1},
pages = {1--9},
primaryClass = {https:},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/abs/1406.2661v1}
}
@article{Equations1965,
abstract = {From a theory of Hohenberg and Kohn, approximation methods for treating an inhomogeneous system of interacting electrons are developed. These methods are exact for systems of slowly varying or high density. For the ground state, they lead to self-consistent equations analogous to the Hartree and Hartree-Fock equations, respectively. In these equations the exchange and correlation portions of the chemical potential of a uniform electron gas appear as additional effective potentials. (The exchange portion of our effective potential differs from that due to Slater by a factor of -';.) Electronic systems at finite temperatures and in magnetic lelds are also treated by similar methods. An appendix deals with a further correction for systems with short-wavelength density oscillations.},
archivePrefix = {arXiv},
arxivId = {10.1103/PhysRev.140.A1133},
author = {Kohn, W. and Sham, L. J.},
doi = {10.1103/PhysRev.140.A1133},
eprint = {PhysRev.140.A1133},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kohn, Sham - 1965 - Self-consistent equations including exchange and correlation effects(2).pdf:pdf},
isbn = {9783540373674},
issn = {0031899X},
journal = {Physical Review},
number = {4A},
pmid = {20432011},
primaryClass = {10.1103},
title = {{Self-consistent equations including exchange and correlation effects}},
volume = {140},
year = {1965}
}
@incollection{6302929,
abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
author = {Rumelhart, D E and McClelland, J L},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
isbn = {9780262291408},
pages = {318--362},
publisher = {MIT Press},
title = {{Learning Internal Representations by Error Propagation}},
url = {https://ieeexplore.ieee.org/document/6302929},
year = {1987}
}
@article{Leleu2019,
author = {Leleu, Timoth{\'{e}}e and Yamamoto, Yoshihisa and McMahon, Peter L and Aihara, Kazuyuki},
doi = {10.1103/PhysRevLett.122.040607},
journal = {Phys. Rev. Lett.},
month = {feb},
number = {4},
pages = {40607},
publisher = {American Physical Society},
title = {{Destabilization of Local Minima in Analog Spin Systems by Correction of Amplitude Heterogeneity}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.122.040607},
volume = {122},
year = {2019}
}
@article{Baldi2016,
abstract = {At the extreme energies of the Large Hadron Collider, massive particles can be produced at such high velocities that their hadronic decays are collimated and the resulting jets overlap. Deducing whether the substructure of an observed jet is due to a low-mass single particle or due to multiple decay objects of a massive particle is an important problem in the analysis of collider data. Traditional approaches have relied on expert features designed to detect energy deposition patterns in the calorimeter, but the complexity of the data make this task an excellent candidate for the application of machine learning tools. The data collected by the detector can be treated as a two-dimensional image, lending itself to the natural application of image classification techniques. In this work, we apply deep neural networks with a mixture of locally-connected and fully-connected nodes. Our experiments demonstrate that without the aid of expert features, such networks match or modestly outperform the current state-of-the-art approach for discriminating between jets from single hadronic particles and overlapping jets from pairs of collimated hadronic particles, and that such performance gains persist in the presence of pileup interactions.},
archivePrefix = {arXiv},
arxivId = {1603.09349},
author = {Baldi, Pierre and Bauer, Kevin and Eng, Clara and Sadowski, Peter and Whiteson, Daniel},
doi = {10.1103/PhysRevD.93.094034},
eprint = {1603.09349},
issn = {15502368},
journal = {Physical Review D - Particles, Fields, Gravitation and Cosmology},
number = {9},
pages = {1--12},
title = {{Jet substructure classification in high-energy physics with deep neural networks}},
volume = {93},
year = {2016}
}
@article{Ac2016,
author = {Acı, Mehmet and Avcı, Mutlu},
doi = {10.1007/s00339-016-0153-1},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Acı, Avcı - 2016 - Artificial neural network approach for atomic coordinate prediction of carbon nanotubes.pdf:pdf},
issn = {14320630},
journal = {Applied Physics A: Materials Science and Processing},
number = {7},
title = {{Artificial neural network approach for atomic coordinate prediction of carbon nanotubes}},
volume = {122},
year = {2016}
}
@article{Okada2019,
abstract = {The Potts model is a generalization of the Ising model with {\$}Q{\textgreater}2{\$} components. In the fully connected ferromagnetic Potts model, a first-order phase transition is induced by varying thermal fluctuations. Therefore, the computational time required to obtain the ground states by simulated annealing exponentially increases with the system size. This study analytically confirms that the transverse magnetic-field quantum annealing induces a first-order phase transition. This result implies that quantum annealing does not exponentially accelerate the ground-state search of the ferromagnetic Potts model. To avoid the first-order phase transition, we propose an iterative optimization method using a half-hot constraint that is applicable to both quantum and simulated annealing. In the limit of {\$}Q \backslashto \backslashinfty{\$}, a saddle point equation under the half-hot constraint is identical to the equation describing the behavior of the fully connected ferromagnetic Ising model, thus confirming a second-order phase transition. Furthermore, we verify the same relation between the fully connected Potts glass model and the Sherrington--Kirkpatrick model under assumptions of static approximation and replica symmetric solution. The proposed method is expected to obtain low-energy states of the Potts models with high efficiency using Ising-type computers such as the D-Wave quantum annealer and the Fujitsu Digital Annealer.},
archivePrefix = {arXiv},
arxivId = {1904.01522},
author = {Okada, Shuntaro and Ohzeki, Masayuki and Tanaka, Kazuyuki},
eprint = {1904.01522},
month = {apr},
title = {{Efficient quantum and simulated annealing of Potts models using a half-hot constraint}},
url = {http://arxiv.org/abs/1904.01522},
year = {2019}
}
@techreport{Krizhevskya,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {::},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/}
}
@article{adamoptimizer,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {::},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961 http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@techreport{Hintona,
abstract = {We show how to use "complementary priors" to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discrimi-native learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
file = {::},
title = {{A fast learning algorithm for deep belief nets}}
}
@article{Mirza2014,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
month = {nov},
pages = {1--7},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
year = {2014}
}
@techreport{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly-sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
pages = {1223--1231},
title = {{Large Scale Distributed Deep Networks}},
volume = {25},
year = {2012}
}
@article{Ballard2016,
abstract = {Methods developed to explore and characterise potential energy landscapes are applied to the corresponding landscapes obtained from optimisation of a cost function in machine learning. We consider neural network predictions for the outcome of local geometry optimisation in a triatomic cluster, where four distinct local minima exist. The accuracy of the predictions is compared for fits using data from single and multiple points in the series of atomic configurations resulting from local geometry optimisation and for alternative neural networks. The machine learning solution landscapes are visualised using disconnectivity graphs, and signatures in the effective heat capacity are analysed in terms of distributions of local minima and their properties.},
author = {Ballard, Andrew J. and Stevenson, Jacob D. and Das, Ritankar and Wales, David J.},
doi = {10.1063/1.4944672},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ballard et al. - 2016 - Energy landscapes for a machine learning application to series data.pdf:pdf},
isbn = {1089-7690 (Electronic)$\backslash$r0021-9606 (Linking)},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
number = {12},
pages = {124119},
pmid = {27036439},
title = {{Energy landscapes for a machine learning application to series data}},
url = {http://scitation.aip.org/content/aip/journal/jcp/144/12/10.1063/1.4944672},
volume = {144},
year = {2016}
}
@article{Mills2020,
abstract = {We demonstrate the use of a regressive upscaling generative adversarial network (RUGAN) as an effective way to sample state space for hexagonal porous graphene sheets. The RUGAN can, after being trained on a set of small-scale examples, generate new, energetically relevant microstates (atomic configurations). The RUGAN can generate configurations across a continuum of total energy values and produce configurations at requested energy values. The microstates produced respect periodic boundary conditions, and importantly, the fully convolutional nature of the generator allows the generation of arbitrarily large microstates, after being trained on only a small-scale data set.},
author = {Mills, Kyle and Casert, Corneel and Tamblyn, Isaac},
doi = {10.1021/acs.jpcc.0c06673},
file = {:Users/kmills/Desktop/phd{\_}references/acs.jpcc.0c06673.pdf:pdf},
issn = {1932-7447},
journal = {The Journal of Physical Chemistry C},
month = {oct},
number = {42},
pages = {23158--23163},
title = {{Adversarial Generation of Mesoscale Surfaces from Small-Scale Chemical Motifs}},
url = {https://pubs.acs.org/doi/10.1021/acs.jpcc.0c06673},
volume = {124},
year = {2020}
}
@incollection{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus-Robert},
doi = {10.1007/978-3-642-35289-8_3},
pages = {9--48},
publisher = {Springer, Berlin, Heidelberg},
title = {{Efficient BackProp}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}3},
year = {2012}
}
@article{Kohn1995,
abstract = {The standard Kohn-Sham formulation of density functional theory (DFT) is limited, for practical reasons, to systems of less than about 50-100 atoms. The computational effort scales as Nat{\^{}}$\alpha$, where Nat is the number of atoms and 2 {\textless} $\alpha$ {\textless} 3. (By comparison, conventional configuration interaction methods are limited to 5-10 atom systems.) This article deals with the prospect of practical methods that scale linearly in Nat and may thus allow calculations for systems of 10{\^{}}3-10{\^{}}4 atoms. The physical reason (“near-sightedness”) for linear scaling is presented. Implementations of linear scaling DFT by the use of generalized Wannier functions or the one-particle density matrix are discussed.},
author = {Kohn, W.},
doi = {10.1002/qua.560560407},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
month = {nov},
number = {4},
pages = {229--232},
title = {{Density functional theory for systems of very many atoms}},
url = {http://doi.wiley.com/10.1002/qua.560560407},
volume = {56},
year = {1995}
}
@article{Behler2007,
abstract = {The accurate description of chemical processes often requires the use of computationally demanding methods like density-functional theory (DFT), making long simulations of large systems unfeasible. In this Letter we introduce a new kind of neural-network representation of DFT potential-energy surfaces, which provides the energy and forces as a function of all atomic positions in systems of arbitrary size and is several orders of magnitude faster than DFT. The high accuracy of the method is demonstrated for bulk silicon and compared with empirical potentials and DFT. The method is general and can be applied to all types of periodic and nonperiodic systems.},
author = {Behler, J??rg and Parrinello, Michele},
doi = {10.1103/PhysRevLett.98.146401},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Behler, Parrinello - 2007 - Generalized neural-network representation of high-dimensional potential-energy surfaces.pdf:pdf},
isbn = {0031-9007},
issn = {00319007},
journal = {Physical Review Letters},
number = {14},
pages = {1--4},
pmid = {17501293},
title = {{Generalized neural-network representation of high-dimensional potential-energy surfaces}},
volume = {98},
year = {2007}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Ratcliff2016b,
abstract = {During the past decades, quantum mechanical methods have undergone an amaz- ing transition from pioneering investigations of experts into a wide range of practi- cal applications, made by a vast community of researchers. First principles calculations of systems containing up to a fewhundred atoms have become a stand- ard in many branches of science. The sizes of the systems which can be simulated have increased even further during recent years, and quantum-mechanical calcula- tions of systems up to many thousands of atoms are nowadays possible. This opens up new appealing possibilities, in particular for interdisciplinary work, bridging together communities of different needs and sensibilities. In this review we will present the current status of this topic, and will also give an outlook on the vast multitude of applications, challenges, and opportunities stimulated by electronic structure calculations, making this field an important working tool and bringing together researchers ofmany different domains.},
archivePrefix = {arXiv},
arxivId = {1609.00252},
author = {Ratcliff, Laura E. and Mohr, Stephan and Huhs, Georg and Deutsch, Thierry and Masella, Michel and Genovese, Luigi},
doi = {10.1002/wcms.1290},
eprint = {1609.00252},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ratcliff et al. - 2016 - Challenges in Large Scale Quantum Mechanical Calculations.pdf:pdf},
issn = {17590876},
journal = {WIREs Computational Molecular Science},
month = {jan},
number = {1},
pages = {e1290},
title = {{Challenges in Large Scale Quantum Mechanical Calculations}},
url = {http://doi.wiley.com/10.1002/wcms.1290},
volume = {7},
year = {2016}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:pdf},
isbn = {1212.5701},
journal = {arXiv},
month = {dec},
pages = {6},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@techreport{Hafner,
abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
archivePrefix = {arXiv},
arxivId = {1811.04551v5},
author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
eprint = {1811.04551v5},
file = {::},
title = {{Learning Latent Dynamics for Planning from Pixels}}
}
@article{Harris2010a,
abstract = {A superconducting chip containing a regular array of flux qubits, tunable interqubit inductive couplers, an XY-addressable readout system, on-chip programmable magnetic memory, and a sparse network of analog control lines has been studied. The architecture of the chip and the infrastructure used to control it were designed to facilitate the implementation of an adiabatic quantum optimization algorithm. The performance of an eight-qubit unit cell on this chip has been characterized by measuring its success in solving a large set of random Ising spin-glass problem instances as a function of temperature. The experimental data are consistent with the predictions of a quantum mechanical model of an eight-qubit system coupled to a thermal environment. These results highlight many of the key practical challenges that we have overcome and those that lie ahead in the quest to realize a functional large-scale adiabatic quantum information processor. {\textcopyright} 2010 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1004.1628},
author = {Harris, R. and Johnson, M. W. and Lanting, T. and Berkley, A. J. and Johansson, J. and Bunyk, P. and Tolkacheva, E. and Ladizinsky, E. and Ladizinsky, N. and Oh, T. and Cioata, F. and Perminov, I. and Spear, P. and Enderud, C. and Rich, C. and Uchaikin, S. and Thom, M. C. and Chapple, E. M. and Wang, J. and Wilson, B. and Amin, M. H.S. and Dickson, N. and Karimi, K. and MacReady, B. and Truncik, C. J.S. and Rose, G.},
doi = {10.1103/PhysRevB.82.024511},
eprint = {1004.1628},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {2},
pages = {1--15},
title = {{Experimental investigation of an eight-qubit unit cell in a superconducting optimization processor}},
volume = {82},
year = {2010}
}
@article{Kotsiantis2007,
abstract = {Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored. Povzetek:},
author = {Kotsiantis, S.B.},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kotsiantis - 2007 - Supervised Machine Learning A Review of Classification Techniques.pdf:pdf},
journal = {Informatica},
keywords = {classifiers,data mining techniques,intelligent data analysis,learning algorithms},
pages = {249--268},
title = {{Supervised Machine Learning: A Review of Classification Techniques}},
volume = {31},
year = {2007}
}
@article{Zhang2019,
abstract = {Real Time Strategy (RTS) games require macro strategies as well as micro strategies to obtain satisfactory performance since it has large state space, action space, and hidden information. This paper presents a novel hierarchical reinforcement learning model for mastering Multiplayer Online Battle Arena (MOBA) games, a sub-genre of RTS games. The novelty of this work are: (1) proposing a hierarchical framework, where agents execute macro strategies by imitation learning and carry out micromanipulations through reinforcement learning, (2) developing a simple self-learning method to get better sample efficiency for training, and (3) designing a dense reward function for multi-agent cooperation in the absence of game engine or Application Programming Interface (API). Finally, various experiments have been performed to validate the superior performance of the proposed method over other state-of-the-art reinforcement learning algorithms. Agent successfully learns to combat and defeat bronze-level built-in AI with 100{\%} win rate, and experiments show that our method can create a competitive multi-agent for a kind of mobile MOBA game {\{}{\$}\backslash{\$}it King of Glory{\}} in 5v5 mode.},
archivePrefix = {arXiv},
arxivId = {1901.08004},
author = {Zhang, Zhijian and Li, Haozheng and Zhang, Luo and Zheng, Tianyin and Zhang, Ting and Hao, Xiong and Chen, Xiaoxin and Chen, Min and Xiao, Fangxu and Zhou, Wei},
eprint = {1901.08004},
title = {{Hierarchical Reinforcement Learning for Multi-agent MOBA Game}},
url = {http://arxiv.org/abs/1901.08004},
year = {2019}
}
@article{Suzuki2016,
abstract = {Recently, machine learning has emerged as an alternative, powerful approach for predicting quantum-mechanical properties of molecules and solids. Here, using kernel ridge regression and atomic fingerprints representing local environments of atoms, we trained a machine-learning model on a crystalline silicon system in order to directly predict the atomic forces at a wide range of temperatures. Our idea is to construct a machine-learning model using a quantum-mechanical data set taken from canonical-ensemble simulations at a higher temperature, or an upper bound of the temperature range. With our model, the force prediction errors were about 2{\%} or smaller with respect to the corresponding force ranges, in the temperature region between 300 and 1650 K. We also verified the applicability to a larger system, ensuring the transferability with respect to system size.},
archivePrefix = {arXiv},
arxivId = {1608.07374},
author = {Suzuki, Teppei and Tamura, Ryo and Miyazaki, Tsuyoshi},
doi = {10.1002/qua.25307},
eprint = {1608.07374},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Suzuki, Tamura, Miyazaki - 2017 - Machine learning for atomic forces in a crystalline solid Transferability to various temperatures.pdf:pdf},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
keywords = {force fields,kernel ridge regression,machine learning,materials simulation},
number = {1},
pages = {33--39},
title = {{Machine learning for atomic forces in a crystalline solid: Transferability to various temperatures}},
url = {http://arxiv.org/abs/1608.07374},
volume = {117},
year = {2017}
}
@article{Salimans2016a,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
isbn = {0924-6495},
issn = {09246495},
pages = {1--10},
pmid = {23259955},
title = {{Improved Techniques for Training GANs}},
url = {http://arxiv.org/abs/1606.03498},
year = {2016}
}
@article{Mirzaei2010,
abstract = {We report here the results for an ab initio approach to obtain the parameters needed for molecular simulations using a polarizable force field. These parameters consist of the atomic charges, polarizabilities, and radii. The former two are readily obtained using methods reported previously (van Duijnen and Swart, J Phys Chem A 1998, 102, 2399; Swart et al. J Comput Chem 2001, 22, 79), whereas here we report a new approach for obtaining atomic second-order radii (SOR), which is based on second-order atomic moments in scaled Voronoi cells. These parameters are obtained from quantum- chemistry calculations on the monomers, and used without further adaptation directly for intermolecular interactions. The approach works very well as shown here for four dimers, where high-level coupled cluster with singles and doubles, and perturbative triples (CCSD(T)) and density functional theory (DFT) Swart-Sola `-Bickelhaupt functional including Grimme's dispersion correction (SSB-D) reference data are available for comparison. The energy surfaces for the three methods are very similar, which is also the case for the interaction between a water molecule with either a chloride anion or a sodium cation. These latter systems had previously been used to criticize Thole's damped point-dipole method, but here we show that with the correct use of the method, it is perfectly able to describe the intermolecular interactions. This is most obvious for the induced dipole moment as function of the chloride–oxygen distance, where the direct (discrete) reaction field results are virtually indistinguishable from those obtained at CCSD(T)/aug-cc-pVTZ.},
author = {Caetano, C. and Reis, J. L. and Amorim, J. and Lemes, M. Ruv and Pino, A. Dal},
doi = {10.1002/qua.22572},
isbn = {1097461X},
issn = {00207608},
journal = {International Journal of Quantum Chemistry},
keywords = {Boron phosphide,Chemical shielding,Density functional theory,Nanotube},
month = {oct},
number = {12},
pages = {2732--2740},
pmid = {20148191},
title = {{Using neural networks to solve nonlinear differential equations in atomic and molecular physics}},
url = {http://www3.interscience.wiley.com/journal/123444074/abstract{\%}5Cnpapers://6ba9bf75-7dcc-4a9b-b1a7-22d2ef0616d5/Paper/p15538 http://doi.wiley.com/10.1002/qua.22572},
volume = {111},
year = {2011}
}
@article{tensorflow,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
file = {::},
journal = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
month = {may},
pages = {265--283},
publisher = {USENIX Association},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
@article{Johnson2011,
abstract = {Many interesting but practically intractable problems can be reduced to that of finding the ground state of a system of interacting spins; however, finding such a ground state remains computationally difficult. It is believed that the ground state of some naturally occurring spin systems can be effectively attained through a process called quantum annealing. If it could be harnessed, quantum annealing might improve on known methods for solving certain types of problem. However, physical investigation of quantum annealing has been largely confined to microscopic spins in condensed-matter systems. Here we use quantum annealing to find the ground state of an artificial Ising spin system comprising an array of eight superconducting flux quantum bits with programmable spin-spin couplings. We observe a clear signature of quantum annealing, distinguishable from classical thermal annealing through the temperature dependence of the time at which the system dynamics freezes. Our implementation can be configured in situ to realize a wide variety of different spin networks, each of which can be monitored as it moves towards a low-energy configuration. This programmable artificial spin network bridges the gap between the theoretical study of ideal isolated spin networks and the experimental investigation of bulk magnetic samples. Moreover, with an increased number of spins, such a system may provide a practical physical means to implement a quantum algorithm, possibly allowing more-effective approaches to solving certain classes of hard combinatorial optimization problems. {\textcopyright} 2011 Macmillan Publishers Limited. All rights reserved.},
author = {Johnson, M. W. and Amin, M. H.S. and Gildert, S. and Lanting, T. and Hamze, F. and Dickson, N. and Harris, R. and Berkley, A. J. and Johansson, J. and Bunyk, P. and Chapple, E. M. and Enderud, C. and Hilton, J. P. and Karimi, K. and Ladizinsky, E. and Ladizinsky, N. and Oh, T. and Perminov, I. and Rich, C. and Thom, M. C. and Tolkacheva, E. and Truncik, C. J.S. and Uchaikin, S. and Wang, J. and Wilson, B. and Rose, G.},
doi = {10.1038/nature10012},
issn = {00280836},
journal = {Nature},
number = {7346},
pages = {194--198},
title = {{Quantum annealing with manufactured spins}},
volume = {473},
year = {2011}
}
@article{Dieleman2016,
abstract = {Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.},
archivePrefix = {arXiv},
arxivId = {1602.02660},
author = {Dieleman, Sander and {De Fauw}, Jeffrey and Kavukcuoglu, Koray},
eprint = {1602.02660},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Dieleman, De Fauw, Kavukcuoglu - 2016 - Exploiting Cyclic Symmetry in Convolutional Neural Networks.pdf:pdf},
isbn = {9781510829008},
issn = {1938-7228},
title = {{Exploiting Cyclic Symmetry in Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1602.02660},
year = {2016}
}
@article{Amin2018,
abstract = {Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, we propose a new machine-learning approach based on quantum Boltzmann distribution of a quantum Hamiltonian. Because of the noncommutative nature of quantum mechanics, the training process of the quantum Boltzmann machine (QBM) can become nontrivial. We circumvent the problem by introducing bounds on the quantum probabilities. This allows us to train the QBM efficiently by sampling. We show examples of QBM training with and without the bound, using exact diagonalization, and compare the results with classical Boltzmann training. We also discuss the possibility of using quantum annealing processors for QBM training and application.},
author = {Amin, Mohammad H and Andriyash, Evgeny and Rolfe, Jason and Kulchytskyy, Bohdan and Melko, Roger},
doi = {10.1103/PhysRevX.8.021050},
file = {::},
journal = {Physical Review X},
keywords = {doi:10.1103/PhysRevX.8.021050 url:https://doi.org/},
title = {{Quantum Boltzmann Machine}},
volume = {8},
year = {2018}
}
@techreport{fullyconvolutional,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
file = {::},
title = {{Fully Convolutional Networks for Semantic Segmentation}}
}
@article{Ising1925,
author = {Ising, Ernst},
doi = {10.1007/BF02980577},
issn = {00443328},
journal = {Zeitschrift f{\"{u}}r Physik},
number = {1},
pages = {253--258},
title = {{Beitrag zur Theorie des Ferromagnetismus}},
volume = {31},
year = {1925}
}
@article{Venturelli2015,
abstract = {A quantum annealing solver for the renowned job-shop scheduling problem (JSP) is presented in detail. After formulating the problem as a time-indexed quadratic unconstrained binary optimization problem, several pre-processing and graph embedding strategies are employed to compile optimally parametrized families of the JSP for scheduling instances of up to six jobs and six machines on the D-Wave Systems Vesuvius processor. Problem simplifications and partitioning algorithms, including variable pruning and running strategies that consider tailored binary searches, are discussed and the results from the processor are compared against state-of-the-art global-optimum solvers.},
archivePrefix = {arXiv},
arxivId = {1506.08479},
author = {Venturelli, Davide and Marchand, Dominic J. J. and Rojo, Galo},
eprint = {1506.08479},
month = {jun},
pages = {1--15},
title = {{Quantum Annealing Implementation of Job-Shop Scheduling}},
url = {http://arxiv.org/abs/1506.08479},
year = {2015}
}
@article{Ryczko2017,
abstract = {We study dimer molecules in two and three dimensions using both a model Lennard-Jones potential as well as Density Functional Theory (DFT) calculations. We first show that deep convolutional neural networks (DCNNs) can be used to predict the distances and energies of a dimer molecule in both two and three dimensional space using the Lennard-Jones potential. We then use a similar approach to learn hexagonal surfaces including graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures.},
archivePrefix = {arXiv},
arxivId = {1706.09496},
author = {Ryczko, Kevin and Mills, Kyle and Luchak, Iryna and Homenick, Christa and Tamblyn, Isaac},
eprint = {1706.09496},
month = {jun},
pages = {1--18},
title = {{Convolutional neural networks for atomistic systems}},
url = {http://arxiv.org/abs/1706.09496},
year = {2017}
}
@article{Riera2016,
author = {Riera, Marc and G{\"{o}}tz, Andreas W. and Paesani, Francesco},
doi = {10.1039/C6CP02553F},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Riera, G{\"{o}}tz, Paesani - 2016 - The i-TTM model for ab initio-based ion–water interaction potentials. II. Alkali metal ion–water pote.pdf:pdf},
issn = {1463-9076},
journal = {Physical Chemistry Chemical Physics},
pages = {30334--30343},
publisher = {Royal Society of Chemistry},
title = {{The i-TTM model for ab initio-based ion–water interaction potentials. II. Alkali metal ion–water potential energy functions}},
url = {http://xlink.rsc.org/?DOI=C6CP02553F},
volume = {18},
year = {2016}
}
@article{Takigawa2016,
abstract = {The d-band center for metals has been widely used in order to understand activity trends in metal-surface-catalyzed reactions in terms of the linear Br{\o}nsted–Evans–Polanyi relation and Hammer–N{\o}rskov d-band model. In this paper, the d-band centers for eleven metals (Fe, Co, Ni, Cu, Ru, Rh, Pd, Ag, Ir, Pt, Au) and their pairwise bimetals for two different structures (1{\%} metal doped- or overlayer-covered metal surfaces) are statistically predicted using machine learning methods from readily available values as descriptors for the target metals (such as the density and the enthalpy of fusion of each metal). The predictive accuracy of four regression methods with different numbers of descriptors and different test-set/training-set ratios are quantitatively evaluated using statistical cross validations. It is shown that the d-band centers are reasonably well predicted by the gradient boosting regression (GBR) method with only six descriptors, even when we predict 75{\%} of the data from only 25{\%} given for training (average root mean square error (RMSE) {\textless} 0.5 eV). This demonstrates a potential use of machine learning methods for predicting the activity trends of metal surfaces with a negligible CPU time compared to first-principles methods.},
author = {Takigawa, Ichigaku and Shimizu, Ken-ichi and Tsuda, Koji and Takakusagi, Satoru},
doi = {10.1039/C6RA04345C},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Takigawa et al. - 2016 - Machine-learning prediction of the d-band center for metals and bimetals.pdf:pdf},
issn = {2046-2069},
journal = {RSC Adv.},
number = {58},
pages = {52587--52595},
publisher = {Royal Society of Chemistry},
title = {{Machine-learning prediction of the d-band center for metals and bimetals}},
url = {http://xlink.rsc.org/?DOI=C6RA04345C},
volume = {6},
year = {2016}
}
@article{Lopez-Bezanilla2014,
abstract = {We present a Machine Learning approach to solve electronic quantum transport equations of one-dimensional nanostructures. The transmission coefficients of disordered systems were computed to provide training and test datasets to the machine. The system's representation encodes energetic as well as geometrical information to characterize similarities between disordered configurations, while the Euclidean norm is used as a measure of similarity. Errors for out-of-sample predictions systematically decrease with training set size, enabling the accurate and fast prediction of new transmission coefficients. The remarkable performance of our model to capture the complexity of interference phenomena lends further support to its viability in dealing with transport problems of undulatory nature.},
archivePrefix = {arXiv},
arxivId = {1401.8277},
author = {Lopez-Bezanilla, Alejandro and von Lilienfeld, O. Anatole},
doi = {10.1103/PhysRevB.89.235411},
eprint = {1401.8277},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Lopez-Bezanilla, von Lilienfeld - 2014 - Modeling electronic quantum transport with machine learning.pdf:pdf},
issn = {1098-0121},
journal = {Physical Review B},
month = {jun},
number = {23},
pages = {235411},
title = {{Modeling electronic quantum transport with machine learning}},
url = {http://arxiv.org/abs/1401.8277{\%}0Ahttp://dx.doi.org/10.1103/PhysRevB.89.235411 http://arxiv.org/abs/1401.8277 http://dx.doi.org/10.1103/PhysRevB.89.235411 https://link.aps.org/doi/10.1103/PhysRevB.89.235411},
volume = {89},
year = {2014}
}
@article{Tsukamoto2017,
abstract = {In today's world, there are many situations in which difficult decisions must be made under such constraints as a limited resource and a limited amount of time. These situations include disaster response planning, economic policy decision-making, and investment portfolio optimization. In such situations, it is often necessary to solve a "combinatorial optimization problem," which involves evaluating different combinations of various factors and selecting the optimum combination. Since the number of combinations increases explosively as the number of factors increases, it becomes difficult to find the best answer in a realistic amount of time using a von Neumann type processor. To give a solution for such problems, we have developed two schemes to speed up the 1024-bit Ising model and implemented them in a field-programmable gate array (FPGA). Testing demonstrated that a system using this architecture can solve a 32-city traveling salesman problem 12,000 times faster than the same algorithm running on a 3.5-GHz Intel Xeon E5-1620 v3 processor.},
author = {Tsukamoto, Sanroku and Takatsu, Motomu and Matsubara, Satoshi and Tamura, Hirotaka},
issn = {00162523},
journal = {Fujitsu Scientific and Technical Journal},
number = {5},
pages = {8--13},
title = {{An accelerator architecture for combinatorial optimization problems}},
volume = {53},
year = {2017}
}
@article{Zhu2017a,
abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.},
archivePrefix = {arXiv},
arxivId = {1709.01907},
author = {Zhu, Lingxue and Laptev, Nikolay},
doi = {10.1109/ICDMW.2017.19},
eprint = {1709.01907},
file = {::},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Anomaly detection,Bayesian neural networks,Predictive uncertainty,Time series},
month = {sep},
pages = {103--110},
publisher = {IEEE Computer Society},
title = {{Deep and Confident Prediction for Time Series at Uber}},
url = {http://arxiv.org/abs/1709.01907 http://dx.doi.org/10.1109/ICDMW.2017.19},
volume = {2017-Novem},
year = {2017}
}
@article{Ward2015,
abstract = {A very active area of materials research is to devise methods that use machine learning to automatically extract predictive models from existing materials data. While prior examples have demonstrated successful models for some applications, many more applications exist where machine learning can make a strong impact. To enable faster development of machine-learning-based models for such applications, we have created a framework capable of being applied to a broad range of materials data. Our method works by using a chemically diverse list of attributes, which we demonstrate are suitable for describing a wide variety of properties, and a novel method for partitioning the data set into groups of similar materials in order to boost the predictive accuracy. In this manuscript, we demonstrate how this new method can be used to predict diverse properties of crystalline and amorphous materials, such as band gap energy and glass-forming ability.},
archivePrefix = {arXiv},
arxivId = {1606.09551},
author = {Ward, Logan and Agrawal, Ankit and Choudhary, Alok and Wolverton, Christopher},
doi = {10.1038/npjcompumats.2016.28},
eprint = {1606.09551},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ward et al. - 2015 - A General-Purpose Machine Learning Framework for Predicting Properties of Inorganic Materials.pdf:pdf},
issn = {2057-3960},
journal = {Nature Communications},
month = {aug},
number = {July},
pages = {1--7},
title = {{A General-Purpose Machine Learning Framework for Predicting Properties of Inorganic Materials}},
url = {http://www.nature.com/articles/npjcompumats201628},
volume = {2},
year = {2015}
}
@techreport{Snoek,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization , in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparame-ters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
file = {::},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}}
}
@article{Campbell2002,
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: a single-chip chess search engine,a massively parallel system with multiple levels of parallelism,a strong emphasis on search extensions,a complex evaluation function, andeffective use of a Grandmaster game database.This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
author = {Campbell, Murray1 and {Hoane Jr}, A. Joseph2 and Hsu, Feng-hsiung3},
doi = {10.1016/S0004-3702(01)00129-1},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Campbell, Hoane Jr, Hsu - 2002 - Deep Blue.pdf:pdf},
isbn = {0004-3702},
issn = {0004-3702},
journal = {Artificial Intelligence},
keywords = {computer chess,evaluation,game tree search,parallel search,search extensions,selective search},
month = {jan},
number = {1},
pages = {57--83},
title = {{Deep Blue}},
url = {http://search2.scholarsportal.info.myaccess.library.utoronto.ca/ids70/view{\_}record.php?id=112{\&}recnum=111{\&}SID=760717fafbf1a1c5a4539da053640d93{\&}mark{\_}id=search{\%}3A112{\%}3A17{\%}2C100{\%}2C150},
volume = {134},
year = {2002}
}
@inproceedings{Simard2003,
abstract = {Neural Networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple "do-it-yourself" implementation of convolution neural networks does not require complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
author = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
booktitle = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},
doi = {10.1109/ICDAR.2003.1227801},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Simard, Steinkraus, Platt - 2003 - Best practices for convolutional neural networks applied to visual document analysis.pdf:pdf},
isbn = {0-7695-1960-1},
keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,Support vector machines,Text analysis},
pages = {958--963},
publisher = {IEEE Comput. Soc},
title = {{Best practices for convolutional neural networks applied to visual document analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1227801},
volume = {1},
year = {2003}
}
@book{VanLaarhoven1987,
address = {Dordrecht},
author = {van Laarhoven, Peter J. M. and Aarts, Emile H. L.},
doi = {10.1007/978-94-015-7744-1},
isbn = {978-90-481-8438-5},
publisher = {Springer Netherlands},
title = {{Simulated Annealing: Theory and Applications}},
url = {http://link.springer.com/10.1007/978-94-015-7744-1},
year = {1987}
}
@article{Malshe2008,
abstract = {A generalized method that permits the parameters of an arbitrary empirical potential to be efficiently and accurately fitted to a database is presented. The method permits the values of a subset of the potential parameters to be considered as general functions of the internal coordinates that define the instantaneous configuration of the system. The parameters in this subset are computed by a generalized neural network (NN) with one or more hidden layers and an input vector with at least 3 n − 6 elements, where n is the number of atoms in the system. The Levenberg–Marquardt algorithm is employed to efficiently affect the optimization of the weights and biases of the NN as well as all other potential parameters being treated as constants rather than as functions of the input coordinates. In order to effect this minimization, the usual Jacobian employed in NN operations is modified to include the Jacobian of the computed errors with respect to the parameters of the potential function. The total Jacobian employed in each epoch of minimization is the concatenation of two Jacobians, one containing derivatives of the errors with respect to the weights and biases of the network, and the other with respect to the constant parameters of the potential function. The method provides three principal advantages. First, it obviates the problem of selecting the form of the functional dependence of the parameters upon the system's coordinates by employing a NN. If this network contains a sufficient number of neurons, it will automatically find something close to the best functional form. This is the case since Hornik et al., [Neural Networks2, 359 (1989)] have shown that two-layer NNs with sigmoid transfer functions in the first hidden layer and linear functions in the output layer are universal approximators for analytic functions. Second, the entire fitting procedure is automated so that excellent fits are obtained rapidly with little human effort. Third, the method provides a procedure to avoid local minima in the multidimensional parameter hyperspace. As an illustrative example, the general method has been applied to the specific case of fitting the ab initio energies of Si 5 clusters that are observed in a molecular dynamics (MD) simulation of the machining of a silicon workpiece. The energies of the Si 5 configurations obtained in the MD calculations are computed using the B3LYP procedure with a 6 - 31 G * * basis set. The final ab initiodatabase, which comprises the density functional theory energies of 10 202 Si 5 clusters, is fitted to an empirical Tersoff potential containing nine adjustable parameters, two of which are allowed to be the functions of the Si 5 configuration. The fitting error averaged over all 10 202 points is 0.0148 eV ( 1.43 kJ mol − 1 ) . This result is comparable to the accuracy achieved by more general fitting methods that do not rely on an assumed functional form for the potential surface.},
author = {Malshe, M. and Narulkar, R. and Raff, L. M. and Hagan, M. and Bukkapatnam, S. and Komanduri, R.},
doi = {10.1063/1.2957490},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Malshe et al. - 2008 - Parametrization of analytic interatomic potential functions using neural networks.pdf:pdf},
isbn = {0021-9606},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {4},
title = {{Parametrization of analytic interatomic potential functions using neural networks}},
volume = {129},
year = {2008}
}
@article{Behler2016,
abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.},
author = {Behler, J??rg},
doi = {10.1063/1.4966192},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pages = {170901},
pmid = {27825224},
title = {{Perspective: Machine learning potentials for atomistic simulations}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966192},
volume = {145},
year = {2016}
}
@misc{Junger,
author = {J{\"{u}}nger, Michael},
title = {{Spin Glass Server (https://informatik.uni-koeln.de/spinglass/)}},
url = {https://informatik.uni-koeln.de/spinglass/}
}
@techreport{Raina2009a,
abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsuper-vised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton {\&} Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsu-pervised learning methods. We develop general principles for massively parallelizing un-supervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
file = {::},
title = {{Large-scale Deep Unsupervised Learning using Graphics Processors}},
year = {2009}
}
@techreport{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {::},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/}
}
@article{Wei2016,
abstract = {Reaction prediction remains one of the major challenges for organic chemistry and is a prerequisite for efficient synthetic planning. It is desirable to develop algorithms that, like humans, "learn" from being exposed to examples of the application of the rules of organic chemistry. We explore the use of neural networks for predicting reaction types, using a new reaction fingerprinting method. We combine this predictor with SMARTS transformations to build a system which, given a set of reagents and reactants, predicts the likely products. We test this method on problems from a popular organic chemistry textbook.},
archivePrefix = {arXiv},
arxivId = {1608.06296},
author = {Wei, Jennifer N. and Duvenaud, David and Aspuru-Guzik, Al{\'{a}}n},
doi = {10.1021/acscentsci.6b00219},
eprint = {1608.06296},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Wei, Duvenaud, Aspuru-Guzik - 2016 - Neural Networks for the Prediction of Organic Chemistry Reactions.pdf:pdf},
issn = {2374-7943},
journal = {ACS Central Science},
month = {oct},
number = {10},
pages = {725--732},
pmid = {27800555},
title = {{Neural Networks for the Prediction of Organic Chemistry Reactions}},
url = {http://arxiv.org/abs/1608.06296{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27800555{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5084081 http://pubs.acs.org/doi/10.1021/acscentsci.6b00219},
volume = {2},
year = {2016}
}
@article{Inagaki2016,
abstract = {The analysis and optimization of complex systems can be reduced to mathematical problems collectively known as combinatorial optimization. Many such problems can be mapped onto ground-state search problems of the Ising model, and various artificial spin systems are now emerging as promising approaches. However, physical Ising machines have suffered from limited numbers of spin-spin couplings because of implementations based on localized spins, resulting in severe scalability problems. We report a 2000-spin network with all-to-all spin-spin couplings. Using a measurement and feedback scheme, we coupled time-multiplexed degenerate optical parametric oscillators to implement maximum cut problems on arbitrary graph topologies with up to 2000 nodes. Our coherent Ising machine outperformed simulated annealing in terms of accuracy and computation time for a 2000-node complete graph.},
author = {Inagaki, Takahiro and Haribara, Yoshitaka and Igarashi, Koji and Sonobe, Tomohiro and Tamate, Shuhei and Honjo, Toshimori and Marandi, Alireza and McMahon, Peter L and Umeki, Takeshi and Enbutsu, Koji and Tadanaga, Osamu and Takenouchi, Hirokazu and Aihara, Kazuyuki and Kawarabayashi, Ken Ichi and Inoue, Kyo and Utsunomiya, Shoko and Takesue, Hiroki},
doi = {10.1126/science.aah4243},
issn = {10959203},
journal = {Science},
number = {6312},
pages = {603--606},
title = {{A coherent Ising machine for 2000-node optimization problems}},
volume = {354},
year = {2016}
}
@book{Hebb1949,
author = {Hebb, Donald O.},
isbn = {978-0805843002},
publisher = {Wiley},
title = {{The Organization of Behavior}},
year = {1949}
}
@article{Rosenblatt1958,
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {::},
issn = {1939-1471},
journal = {Psychological Review},
number = {6},
pages = {386--408},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
volume = {65},
year = {1958}
}
@techreport{adagrad,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function , which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Singer, Yoram},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan}},
volume = {12},
year = {2011}
}
@article{Li2012,
abstract = {A DFT-SOFM-RBFNN method is proposed to improve the accuracy of DFT calculations on Y-NO (Y = C, N, O, S) homolysis bond dissociation energies (BDE) by combining density functional theory (DFT) and artificial intelligence/machine learning methods, which consist of self-organizing feature mapping neural networks (SOFMNN) and radial basis function neural networks (RBFNN). A descriptor refinement step including SOFMNN clustering analysis and correlation analysis is implemented. The SOFMNN clustering analysis is applied to classify descriptors, and the representative descriptors in the groups are selected as neural network inputs according to their closeness to the experimental values through correlation analysis. Redundant descriptors and intuitively biased choices of descriptors can be avoided by this newly introduced step. Using RBFNN calculation with the selected descriptors, chemical accuracy (≤1 kcal{\textperiodcentered}mol(-1)) is achieved for all 92 calculated organic Y-NO homolysis BDE calculated by DFT-B3LYP, and the mean absolute deviations (MADs) of the B3LYP/6-31G(d) and B3LYP/STO-3G methods are reduced from 4.45 and 10.53 kcal{\textperiodcentered}mol(-1) to 0.15 and 0.18 kcal{\textperiodcentered}mol(-1), respectively. The improved results for the minimal basis set STO-3G reach the same accuracy as those of 6-31G(d), and thus B3LYP calculation with the minimal basis set is recommended to be used for minimizing the computational cost and to expand the applications to large molecular systems. Further extrapolation tests are performed with six molecules (two containing Si-NO bonds and two containing fluorine), and the accuracy of the tests was within 1 kcal{\textperiodcentered}mol(-1). This study shows that DFT-SOFM-RBFNN is an efficient and highly accurate method for Y-NO homolysis BDE. The method may be used as a tool to design new NO carrier molecules.},
author = {Li, Hong Zhi and Hu, Li Hong and Tao, Wei and Gao, Ting and Li, Hui and Lu, Ying Hua and Su, Zhong Min},
doi = {10.3390/ijms13078051},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2012 - A Promising tool to achieve chemical accuracy for density functional theory calculations on Y-NO homolysis bond disso.pdf:pdf},
isbn = {1661-6596},
issn = {16616596},
journal = {International Journal of Molecular Sciences},
keywords = {Density functional theory,Homolysis bond dissociation energies,Radial basis function neural network,Self-organizing feature mapping neural network,Y-NO bond},
number = {7},
pages = {8051--8070},
pmid = {22942689},
title = {{A Promising tool to achieve chemical accuracy for density functional theory calculations on Y-NO homolysis bond dissociation energies}},
volume = {13},
year = {2012}
}
@article{Schutt2017,
author = {Sch{\"{u}}tt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and M{\"{u}}ller, Klaus R. and Tkatchenko, Alexandre},
doi = {10.1038/ncomms13890},
issn = {2041-1723},
journal = {Nature Communications},
month = {jan},
pages = {13890},
title = {{Quantum-chemical insights from deep tensor neural networks}},
url = {http://www.nature.com/doifinder/10.1038/ncomms13890},
volume = {8},
year = {2017}
}
@article{Masters2018,
abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size {\$}m{\$}. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between {\$}m = 2{\$} and {\$}m = 32{\$}, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
archivePrefix = {arXiv},
arxivId = {1804.07612},
author = {Masters, Dominic and Luschi, Carlo},
eprint = {1804.07612},
file = {::},
journal = {arXiv},
month = {apr},
publisher = {arXiv},
title = {{Revisiting Small Batch Training for Deep Neural Networks}},
url = {http://arxiv.org/abs/1804.07612},
year = {2018}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network (??net) package. The construction and application of ANN potentials using ??net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite (??-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
publisher = {Elsevier B.V.},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
url = {http://dx.doi.org/10.1016/j.commatsci.2015.11.047},
volume = {114},
year = {2016}
}
@inproceedings{Werbos:81sensitivity,
author = {Werbos, P J},
booktitle = {Proceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC},
keywords = {juergen},
pages = {762--770},
title = {{Applications of Advances in Nonlinear Sensitivity Analysis}},
year = {1981}
}
@techreport{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747v2},
author = {Ruder, Sebastian},
eprint = {1609.04747v2},
file = {::},
title = {{An overview of gradient descent optimization algorithms *}},
url = {http://caffe.berkeleyvision.org/tutorial/solver.html},
year = {2016}
}
@article{Saad2012,
abstract = {Machine learning is a broad discipline that comprises a variety of techniques for extg. meaningful information and patterns from data. It draws on knowledge and "know-how" from various scientific areas such as statistics, graph theory, linear algebra, databases, mathematics, and computer science. Recently, materials scientists have begun to explore data mining ideas for discovery in materials. In this paper we explore the power of these methods for studying binary compds. that are well characterized and are often used as a test bed. By mining properties of the constituent atoms, three materials research relevant tasks, namely, sepn. of a no. of compds. into subsets in terms of their crystal structure, grouping of an unknown compd. into the most characteristically similar peers (in one instance, 100{\%} accuracy is achieved), and specific property prediction (the m.p.), are explored. [on SciFinder(R)]},
author = {Saad, Yousef and Gao, Da and Ngo, Thanh and Bobbitt, Scotty and Chelikowsky, James R. and Andreoni, Wanda},
doi = {10.1103/PhysRevB.85.104104},
isbn = {1098-0121},
issn = {1098-0121},
journal = {Physical Review B},
month = {mar},
number = {10},
pages = {104104},
title = {{Data mining for materials: Computational experiments with {\textless}math display="inline"{\textgreater} {\textless}mrow{\textgreater} {\textless}mi{\textgreater}A{\textless}/mi{\textgreater} {\textless}mi{\textgreater}B{\textless}/mi{\textgreater} {\textless}/mrow{\textgreater} {\textless}/math{\textgreater} compounds}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.85.104104},
volume = {85},
year = {2012}
}
@article{Broecker2016,
abstract = {State-of-the-art machine learning techniques promise to become a powerful tool in statistical mechanics via their capacity to distinguish different phases of matter in an automated way. Here we demonstrate that convolutional neural networks (CNN) can be optimized for quantum many-fermion systems such that they correctly identify and locate quantum phase transitions in such systems. Using auxiliary-field quantum Monte Carlo (QMC) simulations to sample the many-fermion system, we show that the Green's function (but not the auxiliary field) holds sufficient information to allow for the distinction of different fermionic phases via a CNN. We demonstrate that this QMC + machine learning approach works even for systems exhibiting a severe fermion sign problem where conventional approaches to extract information from the Green's function, e.g.{\~{}}in the form of equal-time correlation functions, fail. We expect that this capacity of hierarchical machine learning techniques to circumvent the fermion sign problem will drive novel insights into some of the most fundamental problems in statistical physics.},
archivePrefix = {arXiv},
arxivId = {1608.07848},
author = {Broecker, Peter and Carrasquilla, Juan and Melko, Roger G. and Trebst, Simon},
eprint = {1608.07848},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Broecker et al. - 2016 - Machine learning quantum phases of matter beyond the fermion sign problem.pdf:pdf},
journal = {arXiv},
pages = {8},
title = {{Machine learning quantum phases of matter beyond the fermion sign problem}},
url = {http://arxiv.org/abs/1608.07848},
year = {2016}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Brockherde2016,
abstract = {Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of density functional theory to solve electronic structure problems in a wide variety of scientific fields, ranging from materials science to biochemistry to astrophysics. Machine learning holds the promise of learning the kinetic energy functional via examples, by-passing the need to solve the Kohn-Sham equations. This should yield substantial savings in computer time, allowing either larger systems or longer time-scales to be tackled, but attempts to machine-learn this functional have been limited by the need to find its derivative. The present work overcomes this difficulty by directly learning the density-potential and energy-density maps for test systems and various molecules. Both improved accuracy and lower computational cost with this method are demonstrated by reproducing DFT energies for a range of molecular geometries generated during molecular dynamics simulations. Moreover, the methodology could be applied directly to quantum chemical calculations, allowing construction of density functionals of quantum-chemical accuracy.},
archivePrefix = {arXiv},
arxivId = {1609.02815},
author = {Brockherde, Felix and Vogt, Leslie and Li, Li and Tuckerman, Mark E. and Burke, Kieron and M{\"{u}}ller, Klaus-Robert},
eprint = {1609.02815},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Brockherde et al. - 2016 - By-passing the Kohn-Sham equations with machine learning.pdf:pdf},
journal = {arXiv},
month = {sep},
pages = {1--8},
title = {{By-passing the Kohn-Sham equations with machine learning}},
url = {http://arxiv.org/abs/1609.02815},
year = {2016}
}
@article{Krzywinski2013,
abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2613},
isbn = {1548-7091{\$}\backslash{\$}n1548-7105},
issn = {1548-7091},
journal = {Nature Methods},
number = {9},
pages = {809--810},
pmid = {24161969},
publisher = {Nature Publishing Group},
title = {{Points of significance: Importance of being uncertain}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2613},
volume = {10},
year = {2013}
}
@article{Allinger1989,
abstract = {A new molecular mechanics force field (called MM3) for the treatment of aliphatic hydrocarbons has been developed and is presented here. This force field will enable one to calculate the structures and energies, including heats of formation, conformational energies, and rotational barriers, for hydrocarbons more accurately than was possible with earlier force fields. In addition to simple molecules, a great many highly strained molecules have been studied, and the results are almost always of experimental accuracy. {\textcopyright} 1989 American Chemical Society.},
author = {Hackett, Mark J. and McQuillan, James A. and El-Assaad, Fatima and Aitken, Jade B. and Levina, Aviva and Cohen, David D. and Siegele, Rainer and Carter, Elizabeth A. and Grau, Georges E. and Hunt, Nicholas H. and Lay, Peter A.},
doi = {10.1039/c0an00269k},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Hackett et al. - 1989 - Chemical alterations to murine brain tissue induced by formalin fixation implications for biospectroscopic imagi.pdf:pdf},
isbn = {00027863 (ISSN)},
issn = {1533-4880},
journal = {Journal of the American Chemical Society},
keywords = {aliphatic hydrocarbon,drug structure,energy,methodology,nonhuman,short survey,theoretical study,theory,thermodynamics},
number = {23},
pages = {8551--8566},
pmid = {21776693},
title = {{Chemical alterations to murine brain tissue induced by formalin fixation: implications for biospectroscopic imaging and mapping studies of disease pathogenesis}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0024821263{\&}partnerID=40{\&}md5=7a3fca9b78a4242ea7de126833653326},
volume = {111},
year = {1989}
}
@techreport{Bansal,
abstract = {Reinforcement Learning is divided in two main paradigms: model-free and model-based. Each of these two paradigms has strengths and limitations, and has been successfully applied to real world domains that are appropriate to its corresponding strengths. In this paper, we present a new approach aimed at bridging the gap between these two paradigms that is at the same time data-efficient and cost-savvy. We do so by learning a probabilistic dynamics model and leveraging it as a prior for the intertwined model-free optimization. As a result, our approach can exploit the generality and structure of the dynamics model, but is also capable of ignoring its inevitable inaccuracies, by directly incorporating the evidence provided by the direct observation of the cost. Preliminary results demonstrate that our approach outperforms purely model-based and model-free approaches, as well as the approach of simply switching from a model-based to a model-free setting.},
archivePrefix = {arXiv},
arxivId = {1709.03153v2},
author = {Bansal, Somil and Calandra, Roberto and Chua, Kurtland and Levine, Sergey and Tomlin, Claire},
eprint = {1709.03153v2},
file = {::},
title = {{MBMF: Model-Based Priors for Model-Free Reinforcement Learning}}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Allinger1977,
abstract = {An improved force field for molecular mechanics calculations of the structures and energies of hydrocarbons is pre- sented. The problem of simultaneously obtaining a sufficiently large gauche butane interaction energy while keeping the hy- drogens small enough for good structural predictions was solved with the aid of onefold and twofold rotational barriers. The structural results are competitive with the best of currently available force fields, while the energy calculations are superior to any previously reported. For a list of 42 selected diverse types of hydrocarbons, the standard deviation between the calculat- ed and experimental heats of formation is 0.42 kcal/mol, compared with an average reported experimental error for the same group of compounds of 0.40 kcal/mol. It},
author = {Allinger, Norman L.},
doi = {10.1021/ja00467a001},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Allinger - 1977 - Conformational analysis. 130. MM2. A hydrocarbon force field utilizing V1 and V2 torsional terms.pdf:pdf},
isbn = {0002-7863},
issn = {0002-7863},
journal = {Journal of the American Chemical Society},
number = {25},
pages = {8127--8134},
title = {{Conformational analysis. 130. MM2. A hydrocarbon force field utilizing V1 and V2 torsional terms}},
url = {http://pubs.acs.org/doi/abs/10.1021/ja00467a001},
volume = {99},
year = {1977}
}
@article{Monterola2001,
abstract = {We present a practical method for estimating the upper error bound in the neural network (NN) solution of the nonlinear Schr{\"{o}}dinger equation (NLSE) under different degrees of nonlinearity. The error bound is a function of the nonnegative energy E value that is minimized when the NN is trained to solve the NLSE. The form of E is derived from the NLSE expression and the NN solution becomes identical with the true NLSE solution only when the E value is reduced exactly to zero. In practice, machines with finite floating-point range and accuracy are used for training and E is not decreased exactly to zero. Knowledge of the error bound permits the estimation of the maximum average error in the NN solution without prior knowledge of the true NLSE solution - a crucial factor in the practical applications of the NN technique. The error bound is verified for both the linear time - independent Schr{\"{o}}dinger equation for a free particle, and the NLSE. We also discuss the conditions where the error bound formulation is valid. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
author = {Monterola, Christopher and Saloma, Caesar},
doi = {10.1016/S0030-4018(03)01570-0},
issn = {00304018},
journal = {Optics Communications},
keywords = {Neural networks,Nonlinear Schr{\"{o}}dinger equation,Numerical approximation and analysis,Numerical simulation,Solution of equations},
month = {jul},
number = {1-6},
pages = {331--339},
pmid = {19421275},
title = {{Solving the nonlinear Schr{\"{o}}dinger equation with an unsupervised neural network: Estimation of error in solution}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0030401803015700 https://www.osapublishing.org/oe/abstract.cfm?uri=oe-9-2-72},
volume = {222},
year = {2003}
}
@techreport{Salakhutdinov,
abstract = {Most of the existing approaches to collab-orative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models , called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6{\%} better than the score of Netflix's own system.},
author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
file = {::},
title = {{Restricted Boltzmann Machines for Collaborative Filtering}}
}
@article{Kearnes2016,
abstract = {Molecular "fingerprints" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a novel machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph (atoms, bonds, distances, etc.), allowing the model to take greater advantage of information in the graph structure.},
archivePrefix = {arXiv},
arxivId = {1603.00856},
author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
doi = {10.1007/s10822-016-9938-8},
eprint = {1603.00856},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kearnes et al. - 2016 - Molecular graph convolutions moving beyond fingerprints.pdf:pdf},
issn = {15734951},
journal = {Journal of Computer-Aided Molecular Design},
keywords = {Artificial neural networks,Deep learning,Machine learning,Molecular descriptors,Virtual screening},
number = {8},
pages = {595--608},
pmid = {27558503},
publisher = {Springer International Publishing},
title = {{Molecular graph convolutions: moving beyond fingerprints}},
volume = {30},
year = {2016}
}
@article{Ponder2003,
abstract = {The chapter focuses on a general description of the force fields that are most commonly used at present, and it gives an indication of the directions of current research that may yield better functions in the near future. After a brief survey of current models, mostly generated during the 1990s, the focus of the chapter is on the general directions the field is taking in developing new models. The most commonly used protein force fields incorporate a relatively simple potential energy function: The emphasis is on the use of continuum methods to model the electrostatic effects of hydration and the introduction of polarizability to model the electronic response to changes in the environment. Some of the history and performance of widely used protein force fields based on an equation on simplest potential energy function or closely related equations are reviewed. The chapter outlines some promising developments that go beyond this, primarily by altering the way electrostatic interactions are treated. The use of atomic multipoles and off-center charge distributions, as well as attempts to incorporate electronic polarizability, are also discussed in the chapter.},
author = {Ponder, Jay W. and Case, David A.},
doi = {10.1016/S0065-3233(03)66002-X},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ponder, Case - 2003 - Force fields for protein simulations.pdf:pdf},
isbn = {9780120342662},
issn = {00653233},
journal = {Advances in Protein Chemistry},
pages = {27--85},
pmid = {14631816},
title = {{Force fields for protein simulations}},
volume = {66},
year = {2003}
}
@inproceedings{ciresan2011flexibles,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
author = {Ciresan, Dan Claudiu and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, J{\"{u}}rgen and Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, J{\"{u}}rgen},
booktitle = {Proceedings of the 22nd International Joint Conference on Artificial Intelligence},
doi = {10.5591},
isbn = {9781577355120},
issn = {10450823},
number = {1},
pages = {1237--1242},
title = {{Flexible, High Performance Convolutional Neural Networks for Image Classification}},
volume = {22},
year = {2011}
}
@article{Harris2010,
abstract = {A rf-superconducting quantum interference device (SQUID) flux qubit that is robust against fabrication variations in Josephson-junction critical currents and device inductance has been implemented. Measurements of the persistent current and of the tunneling energy between the two lowest-lying states, both in the coherent and incoherent regimes, are presented. These experimental results are shown to be in agreement with predictions of a quantum-mechanical Hamiltonian whose parameters were independently calibrated, thus justifying the identification of this device as a flux qubit. In addition, measurements of the flux and critical current noise spectral densities are presented that indicate that these devices with Nb wiring are comparable to the best Al wiring rf SQUIDs reported in the literature thus far, with a 1/f flux noise spectral density at 1 Hz of 1.3 -0.5 +0.7 $\mu$ $\Phi$0 /√ Hz. An explicit formula for converting the observed flux noise spectral density into a free-induction-decay time for a flux qubit biased to its optimal point and operated in the energy eigenbasis is presented. {\textcopyright} 2010 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {0909.4321},
author = {Harris, R. and Johansson, J. and Berkley, A. J. and Johnson, M. W. and Lanting, T. and Han, Siyuan and Bunyk, P. and Ladizinsky, E. and Oh, T. and Perminov, I. and Tolkacheva, E. and Uchaikin, S. and Chapple, E. M. and Enderud, C. and Rich, C. and Thom, M. and Wang, J. and Wilson, B. and Rose, G.},
doi = {10.1103/PhysRevB.81.134510},
eprint = {0909.4321},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {13},
pages = {1--19},
title = {{Experimental demonstration of a robust and scalable flux qubit}},
volume = {81},
year = {2010}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
doi = {10.1109/CVPR.2017.19},
eprint = {1609.04802},
month = {sep},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
year = {2016}
}
@inproceedings{schulman2015trust,
author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
booktitle = {International conference on machine learning},
pages = {1889--1897},
title = {{Trust region policy optimization}},
year = {2015}
}
@article{Kmills2017,
abstract = {We present a physically-motivated topology of a deep neural network that can efficiently infer extensive parameters (such as energy, entropy, or number of particles) of arbitrarily large systems, doing so with scaling.},
archivePrefix = {arXiv},
arxivId = {1708.06686},
author = {Mills, Kyle and Ryczko, Kevin and Luchak, Iryna and Domurad, Adam and Beeler, Chris and Tamblyn, Isaac},
doi = {10.1039/C8SC04578J},
eprint = {1708.06686},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills et al. - 2019 - Extensive deep neural networks for transferring small scale learning to large scale systems.pdf:pdf},
issn = {2041-6520},
journal = {Chemical Science},
month = {aug},
number = {15},
pages = {4129--4140},
title = {{Extensive deep neural networks for transferring small scale learning to large scale systems}},
url = {https://arxiv.org/pdf/1708.06686.pdf http://arxiv.org/abs/1708.06686 http://xlink.rsc.org/?DOI=C8SC04578J},
volume = {10},
year = {2019}
}
@article{Wagner2012,
abstract = {Large strongly correlated systems provide a challenge to modern electronic structure methods, because standard density functionals usually fail and traditional quantum chemical approaches are too demanding. The density-matrix renormalization group method, an extremely powerful tool for solving such systems, has recently been extended to handle long-range interactions on real-space grids, but is most efficient in one dimension where it can provide essentially arbitrary accuracy. Such 1d systems therefore provide a theoretical laboratory for studying strong correlation and developing density functional approximations to handle strong correlation, if they mimic three-dimensional reality sufficiently closely. We demonstrate that this is the case, and provide reference data for exact and standard approximate methods, for future use in this area.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.4788v2},
author = {Wagner, Lucas O. and Stoudenmire, E. M. and Burke, Kieron and White, Steven R.},
doi = {10.1039/c2cp24118h},
eprint = {arXiv:1202.4788v2},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Wagner et al. - 2012 - Reference electronic structure calculations in one dimension.pdf:pdf},
issn = {1463-9076},
journal = {Physical Chemistry Chemical Physics},
number = {24},
pages = {8581},
pmid = {22596085},
title = {{Reference electronic structure calculations in one dimension}},
url = {http://xlink.rsc.org/?DOI=c2cp24118h},
volume = {14},
year = {2012}
}
@article{Ciresan2011b,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Coutinho, Luciano Melo and Cec{\'{i}}lio, Roberto Avelino and Xavier, Alexandre C{\^{a}}ndido and Zanetti, Sidney Sara and Garcia, Giovanni de Oliveira},
doi = {10.1088/1751-8113/44/8/085201},
eprint = {1011.1669},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Coutinho et al. - 2011 - Caracterizao morfomtrica da bacia hidrogrfica do rio da Prata, Castelo, ES.pdf:pdf},
isbn = {9788578110796},
issn = {14137895},
journal = {Irriga},
keywords = {Flooding,Geographic information systems,Physical diagnosis,Runoff},
month = {feb},
number = {4},
pages = {369--381},
pmid = {25246403},
title = {{Caracteriza????o morfom??trica da bacia hidrogr??fica do rio da Prata, Castelo, ES}},
url = {http://arxiv.org/abs/1011.1669 http://dx.doi.org/10.1088/1751-8113/44/8/085201 http://stacks.iop.org/1751-8121/44/i=8/a=085201?key=crossref.abc74c979a75846b3de48a5587bf708f},
volume = {16},
year = {2011}
}
@article{Santoro2002,
abstract = {Probing the lowest energy configuration of a complex system by quantum annealing was recently found to be more effective than its classical, thermal counterpart. By comparing classical and quantum Monte Carlo annealing protocols on the two-dimensional random Ising model (a prototype spin glass), we confirm the superiority of quantum annealing relative to classical annealing. We also propose a theory of quantum annealing based on cascade of Landau-Zener tunneling events. For both classical and quantum annealing, the residual energy after annealing is inversely proportional to a power of the logarithm of the annealing time, but the quantum case has a larger power that makes it faster.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0205280},
author = {Santoro, Giuseppe E.},
doi = {10.1126/science.1068774},
eprint = {0205280},
issn = {00368075},
journal = {Science},
month = {mar},
number = {5564},
pages = {2427--2430},
primaryClass = {cond-mat},
title = {{Theory of Quantum Annealing of an Ising Spin Glass}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1068774},
volume = {295},
year = {2002}
}
@article{Lee,
abstract = {Inferring a generative model from data is a fundamental problem in machine learning. It is well-known that the Ising model is the maximum entropy model for binary variables which reproduces the sample mean and pairwise correlations. Learning the parameters of the Ising model from data is the challenge. We establish an analogy between the inverse Ising problem and the Ornstein-Zernike formalism in liquid state physics. Rather than analytically deriving the closure relation, we use a deep neural network to learn the closure from simulations of the Ising model. We show, using simulations as well as biochemical datasets, that the deep neural network model outperforms systematic field-theoretic expansions and can generalize well beyond the parameter regime of the training data. The neural network is able to learn from synthetic data, which can be generated with relative ease, to give accurate predictions on real world datasets.},
archivePrefix = {arXiv},
arxivId = {1706.08466},
author = {Lee, Alpha A},
eprint = {1706.08466},
month = {jun},
number = {2},
title = {{Inverse Ising inference by combining Ornstein-Zernike theory with deep learning}},
url = {http://arxiv.org/abs/1706.08466},
year = {2017}
}
@book{Press2007,
abstract = {Co-authored by four leading scientists from academia and industry, Numerical Recipes Third Edition starts with basic mathematics and computer science and proceeds to complete, working routines. Widely recognized as the most comprehensive, accessible and practical basis for scientific computing, this new edition incorporates more than 400 Numerical Recipes routines, many of them new or upgraded. The executable C++ code, now printed in color for easy reading, adopts an object-oriented style particularly suited to scientific applications. The whole book is presented in the informal, easy-to-read style that made earlier editions so popular. Please visit www.nr.com or www.cambridge.org/us/numericalrecipes for more details. More information concerning licenses is available at: www.nr.com/licenses New key features: 2 new chapters, 25 new sections, 25{\%} longer than Second Edition Thorough upgrades throughout the text Over 100 completely new routines and upgrades of many more. New Classification and Inference chapter, including Gaussian mixture models, HMMs, hierarchical clustering, Support Vector MachinesNew Computational Geometry chapter covers KD trees, quad- and octrees, Delaunay triangulation, and algorithms for lines, polygons, triangles, and spheres New sections include interior point methods for linear programming, Monte Carlo Markov Chains, spectral and pseudospectral methods for PDEs, and many new statistical distributions An expanded treatment of ODEs with completely new routines Plus comprehensive coverage of linear algebra, interpolation, special functions, random numbers, nonlinear sets of equations, optimization, eigensystems, Fourier methods and wavelets, statistical tests, ODEs and PDEs, integral equations, and inverse theory},
address = {New York, NY, USA},
author = {Frolkovi{\v{c}}, Peter},
booktitle = {Acta Applicandae Mathematicae},
doi = {10.1007/BF01321860},
edition = {3},
isbn = {9780521880688},
issn = {0167-8019},
number = {3},
pages = {297--299},
publisher = {Cambridge University Press},
title = {{Numerical recipes: The art of scientific computing}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=1aAOdzK3FegC{\&}pgis=1{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=1aAOdzK3FegC{\&}oi=fnd{\&}pg=PA1{\&}dq=Numerical+Recipes+3rd+Edition:+The+Art+of+Scientific+Computing{\&}ots=3iUpDfIuki{\&}sig=7OunhOxJ3NWR8JdLQtVAYfJ3QFQ{\%}5Cnhtt},
volume = {19},
year = {1990}
}
@article{Kim2013,
abstract = {We decompose the energy error of any variational density functional theory calculation into a contribution due to the approximate functional and that due to the approximate density. Typically, the functional error dominates, but in many interesting situations the density-driven error dominates. Examples range from calculations of electron affinities to preferred geometries of ions and radicals in solution. In these abnormal cases, the error in density functional theory can be greatly reduced by using a more accurate density. A small orbital gap often indicates a substantial density-driven error.},
archivePrefix = {arXiv},
arxivId = {1212.3054},
author = {Kim, Min Cheol and Sim, Eunji and Burke, Kieron},
doi = {10.1103/PhysRevLett.111.073003},
eprint = {1212.3054},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Sim, Burke - 2013 - Understanding and reducing errors in density functional calculations.pdf:pdf},
issn = {00319007},
journal = {Physical Review Letters},
number = {7},
pages = {1--5},
pmid = {23992062},
title = {{Understanding and reducing errors in density functional calculations}},
volume = {111},
year = {2013}
}
@techreport{Crawford,
abstract = {We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.},
archivePrefix = {arXiv},
arxivId = {1612.05695v3},
author = {Crawford, Daniel and Levit, Anna and Ghadermarzy, Navid and Oberoi, Jaspreet S},
eprint = {1612.05695v3},
file = {::},
title = {{Reinforcement Learning using Quantum Boltzmann Machines}}
}
@article{Silver2018,
abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1126/science.aar6404},
issn = {10959203},
journal = {Science},
number = {6419},
pages = {1140--1144},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
volume = {362},
year = {2018}
}
@article{Worrall2016,
abstract = {Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch. H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.},
archivePrefix = {arXiv},
arxivId = {1612.04642},
author = {Worrall, Daniel E. and Garbin, Stephan J. and Turmukhambetov, Daniyar and Brostow, Gabriel J.},
eprint = {1612.04642},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Worrall et al. - 2016 - Harmonic Networks Deep Translation and Rotation Equivariance.pdf:pdf},
month = {dec},
title = {{Harmonic Networks: Deep Translation and Rotation Equivariance}},
url = {http://arxiv.org/abs/1612.04642},
year = {2016}
}
@article{Ulissi2016,
abstract = {Surface phase diagrams are necessary for understanding surface chemistry in electrochemical catalysis, where a range of adsorbates and coverages exist at varying applied potentials. These diagrams are typically constructed using intuition, which risks missing complex coverages and configurations at potentials of interest. More accurate cluster expansion methods are often difficult to implement quickly for new surfaces. We adopt a machine learning approach to rectify both issues. Using a Gaussian process regression model, the free energy of all possible adsorbate coverages for surfaces is predicted for a finite number of adsorption sites. Our result demonstrates a rational, simple, and systematic approach for generating accurate free-energy diagrams with reduced computational resources. The Pourbaix diagram for the IrO2(110) surface (with nine coverages from fully hydrogenated to fully oxygenated surfaces) is reconstructed using just 20 electronic structure relaxations, compared to approximately 90 using t...},
author = {Ulissi, Zachary W. and Singh, Aayush R. and Tsai, Charlie and N{\o}rskov, Jens K.},
doi = {10.1021/acs.jpclett.6b01254},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Ulissi et al. - 2016 - Automated Discovery and Construction of Surface Phase Diagrams Using Machine Learning.pdf:pdf},
issn = {1948-7185},
journal = {The Journal of Physical Chemistry Letters},
number = {19},
pages = {3931--3935},
title = {{Automated Discovery and Construction of Surface Phase Diagrams Using Machine Learning}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jpclett.6b01254},
volume = {7},
year = {2016}
}
@inproceedings{kakade2002approximately,
author = {Kakade, Sham and Langford, John},
booktitle = {ICML},
pages = {267--274},
title = {{Approximately optimal approximate reinforcement learning}},
volume = {2},
year = {2002}
}
@misc{Silver2016a,
abstract = {Games are a great testing ground for developing smarter, more flexible algorithms that have the ability to tackle problems in ways similar to humans. Creating programs that are able to play games better than the best humans has a long history - the first classic game mastered by a computer was noughts and crosses (also known as tic-tac-toe) in 1952 as a PhD candidate's project. Then fell checkers in 1994. Chess was tackled by Deep Blue in 1997. The success isn't limited to board games, either - IBM's Watson won first place on Jeopardy in 2011, and in 2014 our own algorithms learned to play dozens of Atari games just from the raw pixel inputs.},
author = {Silver, David and Hassabis, Demis},
booktitle = {Google Research Blog},
title = {{AlphaGo: Mastering the ancient game of Go with Machine Learning}},
url = {https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html},
year = {2016}
}
@article{Kiyohara,
abstract = {Grain boundaries dramatically affect the properties of polycrystalline materials because of differences in atomic configuration. To fully understand the relationship between grain boundaries and materials properties, systematic studies of the grain boundary atomic structure are crucial. However, such studies are limited by the extensive computation necessary to determine the structure of a single grain boundary. If the structure could be predicted with more efficient computation, the understanding of the grain boundary would be accelerated significantly. Here, we predict grain boundary structures and energies using a machine-learning technique. Training data for non-linear regression of four symmetric-tilt grain boundaries of copper were used. The results of the regression analysis were used to predict 12 other grain boundary structures. The method accurately predicts both the structures and energies of grain boundaries. The method presented in this study is very general and can be utilized in understanding many complex interfaces.},
author = {Kiyohara, Shin and Miyata, Tomohiro and Mizoguchi, Teruyasu},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kiyohara, Miyata, Mizoguchi - Unknown - Prediction of grain boundary structure and energy by machine learning.pdf:pdf},
pages = {4--6},
title = {{Prediction of grain boundary structure and energy by machine learning}}
}
@article{GoogleResearch2015,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
journal = {None},
month = {mar},
number = {212},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf http://arxiv.org/abs/1603.04467},
volume = {1},
year = {2016}
}
@article{Hautier2010,
abstract = {Finding new compounds and their crystal structures is an essential step to new materials discoveries. We demonstrate how this search can be accelerated using a combination of machine learning techniques and high-throughput ab initio computations. Using a probabilistic model built on an experimental crystal structure database, novel compositions that are most likely to form a compound, and their most-probable crystal structures, are identified and tested for stability by ab initio computations. We performed such a large-scale search for new ternary oxides, discovering 209 new compounds with a limited computational budget. A list of these predicted compounds is provided, and we discuss the chemistries in which high discovery rates can be expected.$\backslash$nFinding new compounds and their crystal structures is an essential step to new materials discoveries. We demonstrate how this search can be accelerated using a combination of machine learning techniques and high-throughput ab initio computations. Using a probabilistic model built on an experimental crystal structure database, novel compositions that are most likely to form a compound, and their most-probable crystal structures, are identified and tested for stability by ab initio computations. We performed such a large-scale search for new ternary oxides, discovering 209 new compounds with a limited computational budget. A list of these predicted compounds is provided, and we discuss the chemistries in which high discovery rates can be expected.},
author = {Hautier, Geoffroy and Fischer, Christopher C. and Jain, Anubhav and Mueller, Tim and Ceder, Gerbrand},
doi = {10.1021/cm100795d},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Hautier et al. - 2010 - Finding Nature's Missing Ternary Oxide Compounds Using Machine Learning and Density Functional Theory.pdf:pdf},
isbn = {0897-4756},
issn = {0897-4756},
journal = {Chemistry of Materials},
month = {jun},
number = {12},
pages = {3762--3767},
title = {{Finding Nature's Missing Ternary Oxide Compounds Using Machine Learning and Density Functional Theory}},
url = {http://pubs.acs.org/doi/abs/10.1021/cm100795d},
volume = {22},
year = {2010}
}
@techreport{Wan2013,
abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regular-izing large fully-connected layers within neu-ral networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropCon-nect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropCon-nect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Lecun, Yann and Fergus, Rob},
file = {::},
title = {{Regularization of Neural Networks using DropConnect}},
year = {2013}
}
@article{Kucharski1992,
author = {Kucharski, Stanislaw A. and Bartlett, Rodney J.},
doi = {10.1002/qua.560440810},
isbn = {1097461X},
issn = {0020-7608},
journal = {International Journal of Quantum Chemistry},
month = {mar},
number = {S26},
pages = {107--115},
title = {{Coupled-cluster method for an incomplete model space}},
url = {http://doi.wiley.com/10.1002/qua.560440810},
volume = {44},
year = {1992}
}
@article{Kirkpatrick1983a,
abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
doi = {10.1126/science.220.4598.671},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kirkpatrick, Gelatt, Vecchi - 1983 - Optimization by Simulated Annealing.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {may},
number = {4598},
pages = {671--680},
pmid = {17813860},
title = {{Optimization by Simulated Annealing}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.220.4598.671},
volume = {220},
year = {1983}
}
@techreport{Gal2016,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison , Bayesian models offer a mathematically grounded framework to reason about model uncertainty , but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs-extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predic-tive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142v6},
author = {Gal, Yarin and Uk, Zg201@cam Ac},
eprint = {1506.02142v6},
file = {::},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Zoubin Ghahramani}},
url = {http://yarin.co.},
year = {2016}
}
@article{Pilania2013,
abstract = {The materials discovery process can be significantly expedited and simplified if we can learn effectively from available knowledge and data. In the present contribution, we show that efficient and accurate prediction of a diverse set of properties of material systems is possible by employing machine (or statistical) learning methods trained on quantum mechanical computations in combination with the notions of chemical similarity. Using a family of one-dimensional chain systems, we present a general formalism that allows us to discover decision rules that establish a mapping between easily accessible attributes of a system and its properties. It is shown that fingerprints based on either chemo-structural (compositional and configurational information) or the electronic charge density distribution can be used to make ultra-fast, yet accurate, property predictions. Harnessing such learning paradigms extends recent efforts to systematically explore and mine vast chemical spaces, and can significantly accelerate the discovery of new application-specific materials.},
author = {Pilania, Ghanshyam and Wang, Chenchen and Jiang, Xun and Rajasekaran, Sanguthevar and Ramprasad, Ramamurthy},
doi = {10.1038/srep02810},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Pilania et al. - 2013 - Accelerating materials property predictions using machine learning.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
pages = {2810},
pmid = {24077117},
title = {{Accelerating materials property predictions using machine learning}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3786293{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2013}
}
@article{Martonak2002,
abstract = {Quantum annealing was recently found experimentally in a disordered spin- (formula presented) magnet to be more effective than its classical, thermal counterpart. We use the random two-dimensional Ising model as a test example and perform on it both classical and quantum (path-integral) Monte Carlo annealing. A systematic study of the dependence of the final residual energy on the annealing Monte Carlo time quantitatively demonstrates the superiority of quantum relative to classical annealing in this system. In order to determine the parameter regime for optimal efficiency of the quantum annealing procedure we explore a range of values of Trotter slice number P and temperature T. This identifies two different regimes of freezing with respect to efficiency of the algorithm, and leads to useful guidelines for the optimal choice of quantum annealing parameters. {\textcopyright} 2002 The American Physical Society.},
author = {Martoň{\'{a}}k, Roman and Santoro, Giuseppe E. and Tosatti, Erio},
doi = {10.1103/PhysRevB.66.094203},
issn = {1550235X},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {9},
pages = {1--8},
title = {{Quantum annealing by the path-integral Monte Carlo method: The two-dimensional random Ising model}},
volume = {66},
year = {2002}
}
@article{LennardJones24,
abstract = {10.1098/rspa.1924.0082},
author = {Jones, J. E.},
doi = {10.1098/rspa.1924.0082},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Jones - 1924 - On the Determination of Molecular Fields. II. From the Equation of State of a Gas.pdf:pdf},
isbn = {09501207},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
month = {oct},
number = {738},
pages = {463--477},
pmid = {1000105633},
title = {{On the Determination of Molecular Fields. II. From the Equation of State of a Gas}},
url = {http://rspa.royalsocietypublishing.org/content/106/738/463.abstract},
volume = {106},
year = {1924}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
month = {dec},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Li2016,
abstract = {Kernel ridge regression is used to approximate the kinetic energy of non-interacting fermions in a one-dimensional box as a functional of their density. The properties of different kernels and methods of cross-validation are explored, and highly accurate energies are achieved. Accurate constrained optimal densities are found via a modified Euler-Lagrange constrained minimization of the total energy. A projected gradient descent algorithm is derived using local principal component analysis. Additionally, a sparse grid representation of the density can be used without degrading the perfor- mance of the methods. The implications for machine-learned density functional approximations are discussed.},
archivePrefix = {arXiv},
arxivId = {1404.1333v2},
author = {Li, Li and Snyder, John C. and Pelaschier, Isabelle M. and Huang, Jessica and Niranjan, Uma-Naresh and Duncan, Paul and Rupp, Matthias and M{\"{u}}ller, Klaus-Robert and Burke, Kieron},
doi = {10.1002/qua.25040},
eprint = {1404.1333v2},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2016 - Understanding machine-learned density functionals.pdf:pdf},
issn = {00207608},
journal = {International Journal of Quantum Chemistry},
keywords = {density functional theory,kinetic energy functional,machine learning,orbital free,self-consistent calculation},
month = {jun},
number = {11},
pages = {819--833},
title = {{Understanding machine-learned density functionals}},
url = {http://doi.wiley.com/10.1002/qua.25040},
volume = {116},
year = {2016}
}
@article{Mills2017,
abstract = {We have trained a deep (convolutional) neural network to predict the ground-state energy of an electron in four classes of confining two-dimensional electrostatic potentials. On randomly generated potentials, for which there is no analytic form for either the potential or the ground-state energy, the neural network model was able to predict the ground-state energy to within chemical accuracy, with a median absolute error of 1.49 mHa. We also investigate the performance of the model in predicting other quantities such as the kinetic energy and the first excited-state energy of random potentials. While we demonstrated this approach on a simple, tractable problem, the transferability and excellent performance of the resulting model suggests further applications of deep neural networks to problems of electronic structure.},
archivePrefix = {arXiv},
arxivId = {1702.01361},
author = {Mills, Kyle and Spanner, Michael and Tamblyn, Isaac},
doi = {10.1103/PhysRevA.96.042113},
eprint = {1702.01361},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills, Spanner, Tamblyn - 2017 - Deep learning and the Schr{\"{o}}dinger equation.pdf:pdf},
issn = {2469-9926},
journal = {Physical Review A},
month = {oct},
number = {4},
pages = {042113},
title = {{Deep learning and the Schr{\"{o}}dinger equation}},
url = {http://arxiv.org/abs/1702.01361 https://link.aps.org/doi/10.1103/PhysRevA.96.042113},
volume = {96},
year = {2017}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchan...},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
volume = {117},
year = {2013}
}
@article{Hornak2006,
abstract = {The Fast Fourier Transform (FFT) correlation approach to protein-protein docking can evaluate the energies of billions of docked conformations on a grid if the energy is described in the form of a correlation function. Here, this restriction is removed, and the approach is efficiently used with pairwise interactions potentials that substantially improve the docking results. The basic idea is approximating the interaction matrix by its eigenvectors corresponding to the few dominant eigenvalues, resulting in an energy expression written as the sum of a few correlation functions, and solving the problem by repeated FFT calculations. In addition to describing how the method is implemented, we present a novel class of structure based pairwise intermolecular potentials. The DARS (Decoys As the Reference State) potentials are extracted from structures of protein-protein complexes and use large sets of docked conformations as decoys to derive atom pair distributions in the reference state. The current version of the DARS potential works well for enzyme-inhibitor complexes. With the new FFT-based program, DARS provides much better docking results than the earlier approaches, in many cases generating 50$\backslash${\%} more near-native docked conformations. Although the potential is far from optimal for antibody-antigen pairs, the results are still slightly better than those given by an earlier FFT method. The docking program PIPER is freely available for non-commercial applications.},
archivePrefix = {arXiv},
arxivId = {q-bio/0605018},
author = {Hornak, Viktor and Abel, Robert and Okur, Asim and Strockbine, Bentley and Roitberg, Adrian and Simmerling, Carlos},
doi = {10.1002/prot.21123},
eprint = {0605018},
isbn = {0887-3585},
issn = {08873585},
journal = {Proteins: Structure, Function, and Bioinformatics},
keywords = {??-helix,Decoy analysis,Dihedral parameters,Molecular dynamics,Molecular mechanics,NMR order parameters,Trialanine},
month = {nov},
number = {3},
pages = {712--725},
pmid = {16981200},
primaryClass = {q-bio},
title = {{Comparison of multiple Amber force fields and development of improved protein backbone parameters}},
url = {http://doi.wiley.com/10.1002/prot.21123 http://arxiv.org/abs/q-bio/0605018},
volume = {65},
year = {2006}
}
@article{Mills2017a,
abstract = {We train a deep convolutional neural network to accurately predict the energies and magnetizations of Ising model configurations, using both the traditional nearest-neighbour Hamiltonian, as well as a long-range screened Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbour energy of the 4x4 Ising model. Using its success at this task, we motivate the study of the larger 8x8 Ising model, showing that the deep neural network can learn the nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. Finally, we teach the convolutional deep neural network to accurately predict a long-range interaction through a screened Coulomb Hamiltonian. In this case, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, 1600 times faster than a CUDA-optimized "exact" calculation.},
archivePrefix = {arXiv},
arxivId = {1706.09779},
author = {Mills, K and Tamblyn, I},
eprint = {1706.09779},
month = {jun},
title = {{Deep neural networks for direct, featureless learning through observation: the case of 2d spin models}},
url = {http://arxiv.org/abs/1706.09779},
year = {2017}
}
