Automatically generated by Mendeley Desktop 1.19.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ryczko2017,
abstract = {We study dimer molecules in two and three dimensions using both a model Lennard-Jones potential as well as Density Functional Theory (DFT) calculations. We first show that deep convolutional neural networks (DCNNs) can be used to predict the distances and energies of a dimer molecule in both two and three dimensional space using the Lennard-Jones potential. We then use a similar approach to learn hexagonal surfaces including graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures.},
archivePrefix = {arXiv},
arxivId = {1706.09496},
author = {Ryczko, Kevin and Mills, Kyle and Luchak, Iryna and Homenick, Christa and Tamblyn, Isaac},
eprint = {1706.09496},
month = {jun},
pages = {1--18},
title = {{Convolutional neural networks for atomistic systems}},
url = {http://arxiv.org/abs/1706.09496},
year = {2017}
}
@article{Krzywinski2013,
abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2613},
isbn = {1548-7091{\$}\backslash{\$}n1548-7105},
issn = {1548-7091},
journal = {Nature Methods},
number = {9},
pages = {809--810},
pmid = {24161969},
publisher = {Nature Publishing Group},
title = {{Points of significance: Importance of being uncertain}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2613},
volume = {10},
year = {2013}
}
@techreport{fullyconvolutional,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
file = {::},
title = {{Fully Convolutional Networks for Semantic Segmentation}}
}
@article{Metz2016,
abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
archivePrefix = {arXiv},
arxivId = {1611.02163},
author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
eprint = {1611.02163},
pages = {1--25},
pmid = {202927},
title = {{Unrolled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.02163},
year = {2016}
}
@article{Mirza2014,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
month = {nov},
pages = {1--7},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
year = {2014}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Start with raw spin configurations of a prototypical Ising model, we use principle component analysis to extract relevant low dimensional representations the original data, and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor as indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
file = {:home/kmills/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2016 - Discovering phase transitions with unsupervised learning(2).pdf:pdf},
issn = {2469-9950},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network (??net) package. The construction and application of ANN potentials using ??net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite (??-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
volume = {114},
year = {2016}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Start with raw spin configurations of a prototypical Ising model, we use principle component analysis to extract relevant low dimensional representations the original data, and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor as indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
file = {:home/kmills/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2016 - Discovering phase transitions with unsupervised learning(2).pdf:pdf},
issn = {2469-9950},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@article{Jetchev2016,
abstract = {Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.},
archivePrefix = {arXiv},
arxivId = {1611.08207},
author = {Jetchev, Nikolay and Bergmann, Urs and Vollgraf, Roland},
eprint = {1611.08207},
number = {ii},
title = {{Texture Synthesis with Spatial Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.08207},
year = {2016}
}
@article{Kim2017,
abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN},
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
eprint = {1703.05192},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.05192},
year = {2017}
}
@article{Chen2016,
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
number = {Nips},
title = {{InfoGAN : Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
year = {2016}
}
@techreport{adadelta,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information , different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701v1},
author = {Zeiler, Matthew D},
eprint = {1212.5701v1},
file = {::},
keywords = {Gradient Descent,Index Terms-Adaptive Learning Rates,Machine Learn-ing,Neural Networks},
title = {{ADADELTA: AN ADAPTIVE LEARNING RATE METHOD}}
}
@article{adamoptimizer,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {::},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@article{Gal2015a,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
isbn = {1506.02142},
issn = {1938-7228},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
volume = {48},
year = {2015}
}
@article{Metropolis1953,
author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
doi = {10.1063/1.1699114},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
month = {jun},
number = {6},
pages = {1087--1092},
title = {{Equation of State Calculations by Fast Computing Machines}},
url = {http://aip.scitation.org/doi/10.1063/1.1699114},
volume = {21},
year = {1953}
}
@incollection{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus-Robert},
doi = {10.1007/978-3-642-35289-8_3},
pages = {9--48},
publisher = {Springer, Berlin, Heidelberg},
title = {{Efficient BackProp}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}3},
year = {2012}
}
@article{tensorflow,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
file = {::},
journal = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
month = {may},
pages = {265--283},
publisher = {USENIX Association},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
month = {dec},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Tolstikhin2017,
abstract = {Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
archivePrefix = {arXiv},
arxivId = {1701.02386},
author = {Tolstikhin, Ilya and Gelly, Sylvain and Bousquet, Olivier and Simon-Gabriel, Carl-Johann and Sch{\"{o}}lkopf, Bernhard},
eprint = {1701.02386},
pages = {1--28},
title = {{AdaGAN: Boosting Generative Models}},
url = {http://arxiv.org/abs/1701.02386},
year = {2017}
}
@techreport{Gal2016,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison , Bayesian models offer a mathematically grounded framework to reason about model uncertainty , but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs-extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predic-tive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142v6},
author = {Gal, Yarin and Uk, Zg201@cam Ac},
eprint = {1506.02142v6},
file = {::},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Zoubin Ghahramani}},
url = {http://yarin.co.},
year = {2016}
}
@techreport{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Ca, James Bergstra@umontreal and Ca, Yoshua Bengio@umontreal},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization Yoshua Bengio}},
url = {http://scikit-learn.sourceforge.net.},
volume = {13},
year = {2012}
}
@article{Wetzel2017,
abstract = {We employ unsupervised machine learning techniques to learn latent parameters which best describe states of the two-dimensional Ising model and the three-dimensional XY model. These methods range from principal component analysis to artificial neural network based variational autoencoders. The states are sampled using a Monte-Carlo simulation above and below the critical temperature. We find that the predicted latent parameters correspond to the known order parameters. The latent representation of the states of the models in question are clustered, which makes it possible to identify phases without prior knowledge of their existence or the underlying Hamiltonian. Furthermore, we find that the reconstruction loss function can be used as a universal identifier for phase transitions.},
archivePrefix = {arXiv},
arxivId = {1703.02435},
author = {Wetzel, Sebastian Johann},
doi = {10.1038/nphys4037},
eprint = {1703.02435},
issn = {1745-2473},
pages = {1--8},
title = {{Unsupervised learning of phase transitions: from principal component analysis to variational autoencoders}},
url = {http://arxiv.org/abs/1703.02435},
year = {2017}
}
@article{Tanaka2016,
abstract = {We design a Convolutional Neural Network (CNN) which studies correlation between discretized inverse temperature and spin configuration of 2D Ising model and show that it can find a feature of the phase transition without teaching any a priori information for it. We also define a new order parameter via the CNN and show that it provides well approximated critical inverse temperature. In addition, we compare the activation functions for convolution layer and find that the Rectified Linear Unit (ReLU) is important to detect the phase transition of 2D Ising model.},
archivePrefix = {arXiv},
arxivId = {1609.09087},
author = {Tanaka, Akinori and Tomiya, Akio},
doi = {10.7566/JPSJ.86.063001},
eprint = {1609.09087},
issn = {0031-9015},
number = {Table I},
pages = {1--7},
title = {{Detection of phase transition via convolutional neural network}},
url = {http://arxiv.org/abs/1609.09087},
year = {2016}
}
@article{Salimans2016a,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
isbn = {0924-6495},
issn = {09246495},
pages = {1--10},
pmid = {23259955},
title = {{Improved Techniques for Training GANs}},
url = {http://arxiv.org/abs/1606.03498},
year = {2016}
}
@article{Portman2016,
abstract = {In this paper, we build and explore supervised learning models of ferromagnetic system behavior, using Monte-Carlo sampling of the spin configuration space generated by the 2D Ising model. Given the enormous size of the space of all possible Ising model realizations, the question arises as to how to choose a reasonable number of samples that will form physically meaningful and non-intersecting training and testing datasets. Here, we propose a sampling technique called ID-MH that uses the Metropolis-Hastings algorithm creating Markov process across energy levels within the predefined configuration subspace. We show that application of this method retains phase transitions in both training and testing datasets and serves the purpose of validation of a machine learning algorithm. For larger lattice dimensions, ID-MH is not feasible as it requires knowledge of the complete configuration space. As such, we develop a new "block-ID" sampling strategy: it decomposes the given structure into square blocks with lattice dimension no greater than 5 and uses ID-MH sampling of candidate blocks. Further comparison of the performance of commonly used machine learning methods such as random forests, decision trees, k nearest neighbors and artificial neural networks shows that the PCA-based Decision Tree regressor is the most accurate predictor of magnetizations of the Ising model. For energies, however, the accuracy of prediction is not satisfactory, highlighting the need to consider more algorithmically complex methods (e.g., deep learning).},
archivePrefix = {arXiv},
arxivId = {1611.05891},
author = {Portman, Nataliya and Tamblyn, Isaac},
doi = {10.1016/j.jcp.2017.06.045},
eprint = {1611.05891},
issn = {00219991},
journal = {Journal of Computational Physics},
month = {jul},
pages = {43},
title = {{Sampling algorithms for validation of supervised learning models for Ising-like systems}},
url = {http://arxiv.org/abs/1611.05891 http://linkinghub.elsevier.com/retrieve/pii/S0021999117304990},
year = {2017}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@techreport{rmsprop,
author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
file = {::},
title = {{Neural Networks for Machine Learning Lecture 6a Overview of mini-­-batch gradient descent}}
}
@book{deeplearningbook,
annote = {$\backslash$url{\{}http://www.deeplearningbook.org{\}}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@article{Lee,
abstract = {Inferring a generative model from data is a fundamental problem in machine learning. It is well-known that the Ising model is the maximum entropy model for binary variables which reproduces the sample mean and pairwise correlations. Learning the parameters of the Ising model from data is the challenge. We establish an analogy between the inverse Ising problem and the Ornstein-Zernike formalism in liquid state physics. Rather than analytically deriving the closure relation, we use a deep neural network to learn the closure from simulations of the Ising model. We show, using simulations as well as biochemical datasets, that the deep neural network model outperforms systematic field-theoretic expansions and can generalize well beyond the parameter regime of the training data. The neural network is able to learn from synthetic data, which can be generated with relative ease, to give accurate predictions on real world datasets.},
archivePrefix = {arXiv},
arxivId = {1706.08466},
author = {Lee, Alpha A},
eprint = {1706.08466},
month = {jun},
number = {2},
title = {{Inverse Ising inference by combining Ornstein-Zernike theory with deep learning}},
url = {http://arxiv.org/abs/1706.08466},
year = {2017}
}
@article{Zhu2017a,
abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.},
archivePrefix = {arXiv},
arxivId = {1709.01907},
author = {Zhu, Lingxue and Laptev, Nikolay},
doi = {10.1109/ICDMW.2017.19},
eprint = {1709.01907},
file = {::},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Anomaly detection,Bayesian neural networks,Predictive uncertainty,Time series},
month = {sep},
pages = {103--110},
publisher = {IEEE Computer Society},
title = {{Deep and Confident Prediction for Time Series at Uber}},
url = {http://arxiv.org/abs/1709.01907 http://dx.doi.org/10.1109/ICDMW.2017.19},
volume = {2017-Novem},
year = {2017}
}
@article{Hastings1970,
author = {Hastings, B Y W K},
pages = {97--109},
title = {{Monte Carlo sampling methods using Markov chains and their applications}},
year = {1970}
}
@techreport{Hinton,
author = {Hinton, G E and Srivastava, N and Krizhevsky, A and Sutskever, I and Salakhutdinov, R R},
file = {::},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}}
}
@article{Nguyen2015,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {427--436},
pmid = {24309266},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{LeCun90,
author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R and Hubbard, Wayne and Jackel, Lawrence},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Touretzky, D},
pages = {396--404},
publisher = {Morgan-Kaufmann},
title = {{Handwritten Digit Recognition with a Back-Propagation Network}},
url = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
volume = {2},
year = {1990}
}
@article{Luchak2017,
abstract = {We present a procedure for training and evaluating a deep neural network which can efficiently infer extensive parameters of arbitrarily large systems, doing so with O(N) complexity. We use a form of domain decomposition for training and inference, where each sub-domain (tile) is comprised of a non-overlapping focus region surrounded by an overlapping context region. The relative sizes of focus and context are physically motivated and depend on the locality length scale of the prob-lem. Extensive deep neural networks (EDNN) are a formulation of convolutional neural networks which provide a flexible and general approach, based on physical constraints, to describe multi-scale interactions. They are well suited to massively parallel inference, as no inter-thread communication is necessary during evaluation. Example uses for learning simple spin models, Laplacian (deriva-tive) operator, and approximating many-body quantum mechanical operators (within the density functional theory approach) are demonstrated.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.06686},
author = {Luchak, I and Mills, K and Ryczko, K and Domurad, A and Tamblyn, I},
eprint = {arXiv:1708.06686},
journal = {arXiv},
title = {{Extensive deep neural networks}},
url = {https://arxiv.org/pdf/1708.06686.pdf},
year = {2017}
}
@article{Brooks1983,
abstract = {CHARMM (Chemistry at HARvard Macromolecular Mechanics) is a highly flexible computer program which uses empirical energy functions to model macromolecular systems. The program can read or model build structures, energy minimize them by first- or second-derivative techniques, perform a normal mode or molecular dynamics simulation, and analyze the structural, equilibrium, and dynamic properties determined in these calculations. The operations that CHARMM can perform are described, and some implementation details are given. A set of parameters for the empirical energy function and a sample run are included.},
author = {Brooks, Bernard R and Bruccoleri, Robert E and Olafson, Barry D and States, David J and Swaminathan, S and Karplus, Martin},
doi = {10.1002/jcc.540040211},
isbn = {0192-8651},
issn = {1096987X},
journal = {Journal of Computational Chemistry},
number = {2},
pages = {187--217},
pmid = {1},
title = {{CHARMM: A program for macromolecular energy, minimization, and dynamics calculations}},
volume = {4},
year = {1983}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
doi = {10.1109/CVPR.2017.19},
eprint = {1609.04802},
month = {sep},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
year = {2016}
}
@article{Onsager1944,
abstract = {The partition function of a two-dimensional "ferromagnetic" with scalar "spins" (Ising model) is computed rigorously for the case of vanishing field. The eigenwert problem involved in the corresponding computation for a long strip crystal of finite width (n atoms), joined straight to itself around a cylinder, is solved by direct product decomposition; in the special case n=∞ an integral replaces a sum. The choice of different interaction energies (±J,±J′) in the (0 1) and (1 0) directions does not complicate the problem. The two-way infinite crystal has an order-disorder transition at a temperature T=Tc given by the condition sinh(2J/kTc) sinh(2J′/kTc)=1. The energy is a continuous function of T; but the specific heat becomes infinite as -log |T-Tc|. For strips of finite width, the maximum of the specific heat increases linearly with log n. The order-converting dual transformation invented by Kramers and Wannier effects a simple automorphism of the basis of the quaternion algebra which is natural to the problem in hand. In addition to the thermodynamic properties of the massive crystal, the free energy of a (0 1) boundary between areas of opposite order is computed; on this basis the mean ordered length of a strip crystal is (exp (2J/kT) tanh(2J′/kT))n.},
author = {Onsager, Lars},
doi = {10.1103/PhysRev.65.117},
isbn = {0031-899X{\$}\backslash{\$}r1536-6065},
issn = {0031899X},
journal = {Physical Review},
number = {3-4},
pages = {117--149},
pmid = {18556531},
title = {{Crystal statistics. I. A two-dimensional model with an order-disorder transition}},
volume = {65},
year = {1944}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashbackslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashbackslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashbackslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
eprint = {1703.10593},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@techreport{Srivastava2014a,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{pytorch,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
file = {::},
month = {dec},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
@article{Goodfellow,
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/abs/1406.2661v1},
author = {Goodfellow, Ian J and Pouget-abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-farley, David},
eprint = {/arxiv.org/abs/1406.2661v1},
pages = {1--9},
primaryClass = {https:},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/abs/1406.2661v1}
}
@article{Khorshidi2016,
abstract = {Electronic structure calculations, such as those employing Kohn–Sham density functional theory or ab initio wavefunction theories, have allowed for atomistic-level understandings of a wide variety of phenomena and properties of matter at small scales. However, the computational cost of electronic structure methods drastically increases with length and time scales, which makes these methods difficult for long time-scale molecular dynamics simulations or large-sized systems. Machine-learning techniques can provide accurate potentials that can match the quality of electronic structure calculations, provided sufficient training data. These potentials can then be used to rapidly simulate large and long time-scale phenomena at similar quality to the parent electronic structure approach. Machine-learning potentials usually take a bias-free mathematical form and can be readily developed for a wide variety of systems. Electronic structure calculations have favorable properties–namely that they are noiseless and targeted training data can be produced on-demand–that make them particularly well-suited for machine learning. This paper discusses our modular approach to atomistic machine learning through the development of the open-source Atomistic Machine-learning Package (Amp), which allows for representations of both the total and atom-centered potential energy surface, in both periodic and non-periodic systems. Potentials developed through the atom-centered approach are simultaneously applicable for systems with various sizes. Interpolation can be enhanced by introducing custom descriptors of the local environment. We demonstrate this in the current work for Gaussian-type, bispectrum, and Zernike-type descriptors. Amp has an intuitive and modular structure with an interface through the python scripting language yet has parallelizable fortran components for demanding tasks; it is designed to integrate closely with the widely used Atomic Simulation Environment (ASE), which makes it compatible with a wide variety of commercial and open-source electronic structure codes. We finally demonstrate that the neural network model inside Amp can accurately interpolate electronic structure energies as well as forces of thousands of multi-species atomic systems. Program summary Program title: Amp Catalogue identifier: AFAK{\_}v1{\_}0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AFAK{\_}v1{\_}0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: yes No. of lines in distributed program, including test data, etc.: 21239 No. of bytes in distributed program, including test data, etc.: 1412975 Distribution format: tar.gz Programming language: Python, Fortran. Computer: PC, Mac. Operating system: Linux, Mac, Windows. Has the code been vectorized or parallelized?: Yes RAM: Variable, depending on the number and size of atomic systems. Classification: 16.1, 2.1. External routines: ASE, NumPy, SciPy, f2py, matplotlib Nature of problem: Atomic interactions within many-body systems typically have complicated functional forms, difficult to represent in simple pre-decided closed-forms. Solution method: Machine learning provides flexible functional forms that can be improved as new situations are encountered. Typically, interatomic potentials yield from machine learning simultaneously apply to different system sizes. Unusual features: Amp is as modular as possible, providing a framework for the user to create atomic environment descriptor and regression model at will. Moreover, it has Atomic Simulation Environment (ASE) interface, facilitating interactive collaboration with other electronic structure calculators within ASE. Running time: Variable, depending on the number and size of atomic systems.},
author = {Khorshidi, Alireza and Peterson, Andrew A},
doi = {10.1016/j.cpc.2016.05.010},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Atomic Simulation Environment (ASE),Density functional theory,Neural networks,Potential energy surface,Zernike polynomials},
pages = {310--324},
publisher = {Elsevier B.V.},
title = {{Amp: A modular approach to machine learning in atomistic simulations}},
url = {http://dx.doi.org/10.1016/j.cpc.2016.05.010},
volume = {207},
year = {2016}
}
@techreport{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {::},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/}
}
@techreport{Hron2018,
abstract = {Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for varia-tional Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.},
archivePrefix = {arXiv},
arxivId = {1807.01969v1},
author = {Hron, Jiri and De, Alexander G and Matthews, G and Ghahramani, Zoubin},
eprint = {1807.01969v1},
file = {::},
isbn = {1807.01969v1},
title = {{Variational Bayesian dropout: pitfalls and fixes}},
year = {2018}
}
@techreport{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747v2},
author = {Ruder, Sebastian},
eprint = {1609.04747v2},
file = {::},
title = {{An overview of gradient descent optimization algorithms *}},
url = {http://caffe.berkeleyvision.org/tutorial/solver.html},
year = {2016}
}
@article{Huang2016a,
abstract = {In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.},
archivePrefix = {arXiv},
arxivId = {1612.04357},
author = {Huang, Xun and Li, Yixuan and Poursaeed, Omid and Hopcroft, John and Belongie, Serge},
eprint = {1612.04357},
month = {dec},
pages = {1--25},
pmid = {202927},
title = {{Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.04357},
year = {2016}
}
@techreport{Larochelle,
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
file = {::},
title = {{An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation}}
}
@techreport{Glorot,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
file = {::},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://www.iro.umontreal.}
}
@article{Srivastava2017,
abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
archivePrefix = {arXiv},
arxivId = {1705.07761},
author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael and Sutton, Charles},
eprint = {1705.07761},
month = {may},
title = {{VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning}},
url = {http://arxiv.org/abs/1705.07761},
year = {2017}
}
@article{Salimans2016,
author = {Salimans, Tim and Goodfellow, Ian and Cheung, Vicki and Radford, Alec and Chen, Xi},
number = {Nips},
pages = {1--9},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@article{Morningstar2017,
abstract = {It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.},
archivePrefix = {arXiv},
arxivId = {1708.04622},
author = {Morningstar, Alan and Melko, Roger G},
eprint = {1708.04622},
title = {{Deep Learning the Ising Model Near Criticality}},
url = {http://arxiv.org/abs/1708.04622},
year = {2017}
}
@article{Mills2017d,
abstract = {We demonstrate that a generative adversarial network can be trained to produce Ising model configurations in distinct regions of phase space. In training a generative adversarial network, the discriminator neural network becomes very good a discerning examples from the training set and examples from the testing set. We demonstrate that this ability can be used as an anomaly detector, producing estimations of operator values along with a confidence in the prediction.},
archivePrefix = {arXiv},
arxivId = {1710.08053},
author = {Mills, Kyle and Tamblyn, Isaac},
eprint = {1710.08053},
file = {::},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{Phase space sampling and operator confidence with generative adversarial networks}},
url = {http://arxiv.org/abs/1710.08053},
year = {2017}
}
@article{Behler2016,
abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.},
author = {Behler, J??rg},
doi = {10.1063/1.4966192},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pages = {170901},
pmid = {27825224},
title = {{Perspective: Machine learning potentials for atomistic simulations}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966192},
volume = {145},
year = {2016}
}
@article{Bergmann2017,
abstract = {This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribution by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scalable and it can generate output images of arbitrary large size.},
archivePrefix = {arXiv},
arxivId = {1705.06566},
author = {Bergmann, Urs and Jetchev, Nikolay and Vollgraf, Roland},
eprint = {1705.06566},
title = {{Learning Texture Manifolds with the Periodic Spatial GAN}},
url = {http://arxiv.org/abs/1705.06566},
year = {2017}
}
@article{GoogleResearch2015,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
journal = {None},
month = {mar},
number = {212},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf http://arxiv.org/abs/1603.04467},
volume = {1},
year = {2016}
}
@article{Carrasquilla2016,
abstract = {Neural networks can be used to identify phases and phase transitions in condensed matter systems via supervised machine learning. Readily programmable through modern software libraries, we show that a standard feed-forward neural network can be trained to detect multiple types of order parameter directly from raw state configurations sampled with Monte Carlo. In addition, they can detect highly non-trivial states such as Coulomb phases, and if modified to a convolutional neural network, topological phases with no conventional order parameter. We show that this classification occurs within the neural network without knowledge of the Hamiltonian or even the general locality of interactions. These results demonstrate the power of machine learning as a basic research tool in the field of condensed matter and statistical physics.},
archivePrefix = {arXiv},
arxivId = {1605.01735},
author = {Carrasquilla, Juan and Melko, Roger G},
doi = {10.1038/nphys4035},
eprint = {1605.01735},
file = {:home/kmills/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carrasquilla, Melko - 2017 - Machine learning phases of matter.pdf:pdf},
issn = {1745-2473},
journal = {Nature Physics},
month = {feb},
number = {5},
pages = {431--434},
title = {{Machine learning phases of matter}},
url = {http://arxiv.org/abs/1605.01735 http://www.nature.com/doifinder/10.1038/nphys4035},
volume = {13},
year = {2017}
}
@techreport{adagrad,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function , which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Singer, Yoram},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan}},
volume = {12},
year = {2011}
}
@article{Mills2017a,
abstract = {We train a deep convolutional neural network to accurately predict the energies and magnetizations of Ising model configurations, using both the traditional nearest-neighbour Hamiltonian, as well as a long-range screened Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbour energy of the 4x4 Ising model. Using its success at this task, we motivate the study of the larger 8x8 Ising model, showing that the deep neural network can learn the nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. Finally, we teach the convolutional deep neural network to accurately predict a long-range interaction through a screened Coulomb Hamiltonian. In this case, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, 1600 times faster than a CUDA-optimized "exact" calculation.},
archivePrefix = {arXiv},
arxivId = {1706.09779},
author = {Mills, K and Tamblyn, I},
eprint = {1706.09779},
month = {jun},
title = {{Deep neural networks for direct, featureless learning through observation: the case of 2d spin models}},
url = {http://arxiv.org/abs/1706.09779},
year = {2017}
}
@article{Farimani2017,
abstract = {We have developed a new data-driven paradigm for the rapid inference, modeling and simulation of the physics of transport phenomena by deep learning. Using conditional generative adversarial networks (cGAN), we train models for the direct generation of solutions to steady state heat conduction and incompressible fluid flow purely on observation without knowledge of the underlying governing equations. Rather than using iterative numerical methods to approximate the solution of the constitutive equations, cGANs learn to directly generate the solutions to these phenomena, given arbitrary boundary conditions and domain, with high test accuracy (MAE{\$}{\textless}{\$}1{\$}\backslash{\$}{\%}) and state-of-the-art computational performance. The cGAN framework can be used to learn causal models directly from experimental observations where the underlying physical model is complex or unknown.},
archivePrefix = {arXiv},
arxivId = {1709.02432},
author = {Farimani, Amir Barati and Gomes, Joseph and Pande, Vijay S},
eprint = {1709.02432},
title = {{Deep Learning the Physics of Transport Phenomena}},
url = {http://arxiv.org/abs/1709.02432},
volume = {94305},
year = {2017}
}
@techreport{Snoek,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization , in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparame-ters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
file = {::},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchan...},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
volume = {117},
year = {2013}
}
