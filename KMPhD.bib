Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kmills2017,
abstract = {We present a physically-motivated topology of a deep neural network that can efficiently infer extensive parameters (such as energy, entropy, or number of particles) of arbitrarily large systems, doing so with scaling.},
archivePrefix = {arXiv},
arxivId = {1708.06686},
author = {Mills, Kyle and Ryczko, Kevin and Luchak, Iryna and Domurad, Adam and Beeler, Chris and Tamblyn, Isaac},
doi = {10.1039/C8SC04578J},
eprint = {1708.06686},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills et al. - 2019 - Extensive deep neural networks for transferring small scale learning to large scale systems.pdf:pdf},
issn = {2041-6520},
journal = {Chemical Science},
month = {aug},
number = {15},
pages = {4129--4140},
title = {{Extensive deep neural networks for transferring small scale learning to large scale systems}},
url = {https://arxiv.org/pdf/1708.06686.pdf http://arxiv.org/abs/1708.06686 http://xlink.rsc.org/?DOI=C8SC04578J},
volume = {10},
year = {2019}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Onsager1944,
abstract = {The partition function of a two-dimensional "ferromagnetic" with scalar "spins" (Ising model) is computed rigorously for the case of vanishing field. The eigenwert problem involved in the corresponding computation for a long strip crystal of finite width (n atoms), joined straight to itself around a cylinder, is solved by direct product decomposition; in the special case n=∞ an integral replaces a sum. The choice of different interaction energies (±J,±J′) in the (0 1) and (1 0) directions does not complicate the problem. The two-way infinite crystal has an order-disorder transition at a temperature T=Tc given by the condition sinh(2J/kTc) sinh(2J′/kTc)=1. The energy is a continuous function of T; but the specific heat becomes infinite as -log |T-Tc|. For strips of finite width, the maximum of the specific heat increases linearly with log n. The order-converting dual transformation invented by Kramers and Wannier effects a simple automorphism of the basis of the quaternion algebra which is natural to the problem in hand. In addition to the thermodynamic properties of the massive crystal, the free energy of a (0 1) boundary between areas of opposite order is computed; on this basis the mean ordered length of a strip crystal is (exp (2J/kT) tanh(2J′/kT))n.},
author = {Onsager, Lars},
doi = {10.1103/PhysRev.65.117},
isbn = {0031-899X{\$}\backslash{\$}r1536-6065},
issn = {0031899X},
journal = {Physical Review},
number = {3-4},
pages = {117--149},
pmid = {18556531},
title = {{Crystal statistics. I. A two-dimensional model with an order-disorder transition}},
volume = {65},
year = {1944}
}
@article{Ciresan2010,
abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35{\%} error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.},
author = {Cireşan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/NECO_a_00052},
issn = {0899-7667},
journal = {Neural Computation},
month = {dec},
number = {12},
pages = {3207--3220},
title = {{Deep, Big, Simple Neural Nets for Handwritten Digit Recognition}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/NECO{\_}a{\_}00052},
volume = {22},
year = {2010}
}
@article{Silver2018,
abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1126/science.aar6404},
issn = {10959203},
journal = {Science},
number = {6419},
pages = {1140--1144},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
volume = {362},
year = {2018}
}
@article{Sandfort2019,
abstract = {Labeled medical imaging data is scarce and expensive to generate. To achieve generalizable deep learning models large amounts of data are needed. Standard data augmentation is a method to increase generalizability and is routinely performed. Generative adversarial networks offer a novel method for data augmentation. We evaluate the use of CycleGAN for data augmentation in CT segmentation tasks. Using a large image database we trained a CycleGAN to transform contrast CT images into non-contrast images. We then used the trained CycleGAN to augment our training using these synthetic non-contrast images. We compared the segmentation performance of a U-Net trained on the original dataset compared to a U-Net trained on the combined dataset of original data and synthetic non-contrast images. We further evaluated the U-Net segmentation performance on two separate datasets: The original contrast CT dataset on which segmentations were created and a second dataset from a different hospital containing only non-contrast CTs. We refer to these 2 separate datasets as the in-distribution and out-of-distribution datasets, respectively. We show that in several CT segmentation tasks performance is improved significantly, especially in out-of-distribution (noncontrast CT) data. For example, when training the model with standard augmentation techniques, performance of segmentation of the kidneys on out-of-distribution non-contrast images was dramatically lower than for in-distribution data (Dice score of 0.09 vs. 0.94 for out-of-distribution vs. in-distribution data, respectively, p {\textless} 0.001). When the kidney model was trained with CycleGAN augmentation techniques, the out-of-distribution (non-contrast) performance increased dramatically (from a Dice score of 0.09 to 0.66, p {\textless} 0.001). Improvements for the liver and spleen were smaller, from 0.86 to 0.89 and 0.65 to 0.69, respectively. We believe this method will be valuable to medical imaging researchers to reduce manual segmentation effort and cost in CT imaging.},
author = {Sandfort, Veit and Yan, Ke and Pickhardt, Perry J and Summers, Ronald M},
doi = {10.1038/s41598-019-52737-x},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {16884},
title = {{Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks}},
url = {https://doi.org/10.1038/s41598-019-52737-x},
volume = {9},
year = {2019}
}
@article{pytorch,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
file = {::},
month = {dec},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
@article{Lucas2014,
abstract = {We provide Ising formulations for many NP-complete and NP-hard problems, including all of Karp's 21 NP-complete problems. This collects and extends mappings to the Ising model from partitioning, covering and satisfiability. In each case, the required number of spins is at most cubic in the size of the problem. This work may be useful in designing adiabatic quantum optimization algorithms.},
author = {Lucas, Andrew},
doi = {10.3389/fphy.2014.00005},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Adiabatic quantum computation,Algorithms,Complexity theory,NP,Spin glasses},
number = {February},
pages = {1--14},
title = {{Ising formulations of many NP problems}},
volume = {2},
year = {2014}
}
@article{tensorflow,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
file = {::},
journal = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
month = {may},
pages = {265--283},
publisher = {USENIX Association},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
@article{Linnainmaa1976,
abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed. {\textcopyright} 1976 BIT Foundations.},
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
issn = {15729125},
journal = {BIT},
keywords = {Computational Mathematics and Numerical Analysis,Mathematics,Numeric Computing,general},
number = {2},
pages = {146--160},
publisher = {Kluwer Academic Publishers},
title = {{Taylor expansion of the accumulated rounding error}},
url = {https://link.springer.com/article/10.1007/BF01931367},
volume = {16},
year = {1976}
}
@incollection{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus-Robert},
doi = {10.1007/978-3-642-35289-8_3},
pages = {9--48},
publisher = {Springer, Berlin, Heidelberg},
title = {{Efficient BackProp}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}3},
year = {2012}
}
@techreport{Larochelle,
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
file = {::},
title = {{An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation}}
}
@article{Mirza2014,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
month = {nov},
pages = {1--7},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
year = {2014}
}
@techreport{Raina2009a,
abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsuper-vised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton {\&} Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsu-pervised learning methods. We develop general principles for massively parallelizing un-supervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
file = {::},
title = {{Large-scale Deep Unsupervised Learning using Graphics Processors}},
year = {2009}
}
@techreport{Zhang,
abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256×256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.},
archivePrefix = {arXiv},
arxivId = {1612.03242v2},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
eprint = {1612.03242v2},
file = {::},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {https://github.com/hanzhanggit/StackGAN.}
}
@techreport{Carreira-Perpinan,
abstract = {Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called "con-trastive divergence" (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution. Consider a probability distribution over a vector x (as-sumed discrete w.l.o.g.) and with parameters W p(x; W) = 1 Z(W) e −E(x;W) (1) where Z(W) = x e −E(x;W) is a normalisation constant and E(x; W) is an energy function. This class of random-field distributions has found many practical applications (Li, 2001; Winkler, 2002; Teh et al., 2003; He et al., 2004). Maximum-likelihood (ML) learning of the parameters W given an iid sample X = {\{}x n {\}} N n=1 can be done by gradient ascent: W ($\tau$ +1) = W ($\tau$) + $\eta$ ∂L(W; X) ∂W},
author = {Carreira-Perpi{\~{n}}{\'{a}}n, Migue{\'{i}} A and Hinton, Geoffrey E},
file = {::},
title = {{On Contrastive Divergence Learning}}
}
@article{Mills2017a,
abstract = {We train a deep convolutional neural network to accurately predict the energies and magnetizations of Ising model configurations, using both the traditional nearest-neighbour Hamiltonian, as well as a long-range screened Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbour energy of the 4x4 Ising model. Using its success at this task, we motivate the study of the larger 8x8 Ising model, showing that the deep neural network can learn the nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. Finally, we teach the convolutional deep neural network to accurately predict a long-range interaction through a screened Coulomb Hamiltonian. In this case, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, 1600 times faster than a CUDA-optimized "exact" calculation.},
archivePrefix = {arXiv},
arxivId = {1706.09779},
author = {Mills, Kyle and Tamblyn, Isaac},
eprint = {1706.09779},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills, Tamblyn - 2017 - Deep neural networks for direct, featureless learning through observation the case of 2d spin models(2).pdf:pdf},
month = {jun},
title = {{Deep neural networks for direct, featureless learning through observation: the case of 2d spin models}},
url = {http://arxiv.org/abs/1706.09779},
year = {2017}
}
@article{Khorshidi2016,
abstract = {Electronic structure calculations, such as those employing Kohn–Sham density functional theory or ab initio wavefunction theories, have allowed for atomistic-level understandings of a wide variety of phenomena and properties of matter at small scales. However, the computational cost of electronic structure methods drastically increases with length and time scales, which makes these methods difficult for long time-scale molecular dynamics simulations or large-sized systems. Machine-learning techniques can provide accurate potentials that can match the quality of electronic structure calculations, provided sufficient training data. These potentials can then be used to rapidly simulate large and long time-scale phenomena at similar quality to the parent electronic structure approach. Machine-learning potentials usually take a bias-free mathematical form and can be readily developed for a wide variety of systems. Electronic structure calculations have favorable properties–namely that they are noiseless and targeted training data can be produced on-demand–that make them particularly well-suited for machine learning. This paper discusses our modular approach to atomistic machine learning through the development of the open-source Atomistic Machine-learning Package (Amp), which allows for representations of both the total and atom-centered potential energy surface, in both periodic and non-periodic systems. Potentials developed through the atom-centered approach are simultaneously applicable for systems with various sizes. Interpolation can be enhanced by introducing custom descriptors of the local environment. We demonstrate this in the current work for Gaussian-type, bispectrum, and Zernike-type descriptors. Amp has an intuitive and modular structure with an interface through the python scripting language yet has parallelizable fortran components for demanding tasks; it is designed to integrate closely with the widely used Atomic Simulation Environment (ASE), which makes it compatible with a wide variety of commercial and open-source electronic structure codes. We finally demonstrate that the neural network model inside Amp can accurately interpolate electronic structure energies as well as forces of thousands of multi-species atomic systems. Program summary Program title: Amp Catalogue identifier: AFAK{\_}v1{\_}0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AFAK{\_}v1{\_}0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: yes No. of lines in distributed program, including test data, etc.: 21239 No. of bytes in distributed program, including test data, etc.: 1412975 Distribution format: tar.gz Programming language: Python, Fortran. Computer: PC, Mac. Operating system: Linux, Mac, Windows. Has the code been vectorized or parallelized?: Yes RAM: Variable, depending on the number and size of atomic systems. Classification: 16.1, 2.1. External routines: ASE, NumPy, SciPy, f2py, matplotlib Nature of problem: Atomic interactions within many-body systems typically have complicated functional forms, difficult to represent in simple pre-decided closed-forms. Solution method: Machine learning provides flexible functional forms that can be improved as new situations are encountered. Typically, interatomic potentials yield from machine learning simultaneously apply to different system sizes. Unusual features: Amp is as modular as possible, providing a framework for the user to create atomic environment descriptor and regression model at will. Moreover, it has Atomic Simulation Environment (ASE) interface, facilitating interactive collaboration with other electronic structure calculators within ASE. Running time: Variable, depending on the number and size of atomic systems.},
author = {Khorshidi, Alireza and Peterson, Andrew A},
doi = {10.1016/j.cpc.2016.05.010},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Atomic Simulation Environment (ASE),Density functional theory,Neural networks,Potential energy surface,Zernike polynomials},
pages = {310--324},
publisher = {Elsevier B.V.},
title = {{Amp: A modular approach to machine learning in atomistic simulations}},
url = {http://dx.doi.org/10.1016/j.cpc.2016.05.010},
volume = {207},
year = {2016}
}
@book{minsky69perceptrons,
address = {Cambridge, MA, USA},
author = {Minsky, Marvin and Papert, Seymour},
keywords = {linear-classification neural-networks seminal},
publisher = {MIT Press},
title = {{Perceptrons: An Introduction to Computational Geometry}},
year = {1969}
}
@article{Gal2015a,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
isbn = {1506.02142},
issn = {1938-7228},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
volume = {48},
year = {2015}
}
@article{Metropolis1953,
author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
doi = {10.1063/1.1699114},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
month = {jun},
number = {6},
pages = {1087--1092},
title = {{Equation of State Calculations by Fast Computing Machines}},
url = {http://aip.scitation.org/doi/10.1063/1.1699114},
volume = {21},
year = {1953}
}
@misc{stable-baselines,
author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
booktitle = {GitHub repository},
howpublished = {$\backslash$url{\{}https://github.com/hill-a/stable-baselines{\}}},
publisher = {GitHub},
title = {{Stable Baselines}},
year = {2018}
}
@article{Harris2010a,
abstract = {A superconducting chip containing a regular array of flux qubits, tunable interqubit inductive couplers, an XY-addressable readout system, on-chip programmable magnetic memory, and a sparse network of analog control lines has been studied. The architecture of the chip and the infrastructure used to control it were designed to facilitate the implementation of an adiabatic quantum optimization algorithm. The performance of an eight-qubit unit cell on this chip has been characterized by measuring its success in solving a large set of random Ising spin-glass problem instances as a function of temperature. The experimental data are consistent with the predictions of a quantum mechanical model of an eight-qubit system coupled to a thermal environment. These results highlight many of the key practical challenges that we have overcome and those that lie ahead in the quest to realize a functional large-scale adiabatic quantum information processor. {\textcopyright} 2010 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1004.1628},
author = {Harris, R. and Johnson, M. W. and Lanting, T. and Berkley, A. J. and Johansson, J. and Bunyk, P. and Tolkacheva, E. and Ladizinsky, E. and Ladizinsky, N. and Oh, T. and Cioata, F. and Perminov, I. and Spear, P. and Enderud, C. and Rich, C. and Uchaikin, S. and Thom, M. C. and Chapple, E. M. and Wang, J. and Wilson, B. and Amin, M. H.S. and Dickson, N. and Karimi, K. and MacReady, B. and Truncik, C. J.S. and Rose, G.},
doi = {10.1103/PhysRevB.82.024511},
eprint = {1004.1628},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {2},
pages = {1--15},
title = {{Experimental investigation of an eight-qubit unit cell in a superconducting optimization processor}},
volume = {82},
year = {2010}
}
@techreport{Wan2013,
abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regular-izing large fully-connected layers within neu-ral networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropCon-nect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropCon-nect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Lecun, Yann and Fergus, Rob},
file = {::},
title = {{Regularization of Neural Networks using DropConnect}},
year = {2013}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
doi = {10.1109/CVPR.2017.19},
eprint = {1609.04802},
month = {sep},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
year = {2016}
}
@misc{Junger,
author = {J{\"{u}}nger, Michael},
title = {{Spin Glass Server (https://informatik.uni-koeln.de/spinglass/)}},
url = {https://informatik.uni-koeln.de/spinglass/}
}
@article{adamoptimizer,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {::},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@techreport{Hoffer2017,
abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance-known as the "generalization gap" phenomenon. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on a random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
pages = {1731--1741},
title = {{Train longer, generalize better: closing the generalization gap in large batch training of neural networks}},
volume = {30},
year = {2017}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, Juergen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf},
month = {apr},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://www.idsia.ch/˜juergen/DeepLearning8Oct2014.texCompleteBIBTEXfile http://arxiv.org/abs/1404.7828 http://dx.doi.org/10.1016/j.neunet.2014.09.003},
year = {2014}
}
@article{Zhang2019,
abstract = {Real Time Strategy (RTS) games require macro strategies as well as micro strategies to obtain satisfactory performance since it has large state space, action space, and hidden information. This paper presents a novel hierarchical reinforcement learning model for mastering Multiplayer Online Battle Arena (MOBA) games, a sub-genre of RTS games. The novelty of this work are: (1) proposing a hierarchical framework, where agents execute macro strategies by imitation learning and carry out micromanipulations through reinforcement learning, (2) developing a simple self-learning method to get better sample efficiency for training, and (3) designing a dense reward function for multi-agent cooperation in the absence of game engine or Application Programming Interface (API). Finally, various experiments have been performed to validate the superior performance of the proposed method over other state-of-the-art reinforcement learning algorithms. Agent successfully learns to combat and defeat bronze-level built-in AI with 100{\%} win rate, and experiments show that our method can create a competitive multi-agent for a kind of mobile MOBA game {\{}{\$}\backslash{\$}it King of Glory{\}} in 5v5 mode.},
archivePrefix = {arXiv},
arxivId = {1901.08004},
author = {Zhang, Zhijian and Li, Haozheng and Zhang, Luo and Zheng, Tianyin and Zhang, Ting and Hao, Xiong and Chen, Xiaoxin and Chen, Min and Xiao, Fangxu and Zhou, Wei},
eprint = {1901.08004},
title = {{Hierarchical Reinforcement Learning for Multi-agent MOBA Game}},
url = {http://arxiv.org/abs/1901.08004},
year = {2019}
}
@article{Huang2016a,
abstract = {In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.},
archivePrefix = {arXiv},
arxivId = {1612.04357},
author = {Huang, Xun and Li, Yixuan and Poursaeed, Omid and Hopcroft, John and Belongie, Serge},
eprint = {1612.04357},
month = {dec},
pages = {1--25},
pmid = {202927},
title = {{Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.04357},
year = {2016}
}
@article{Farhi2019,
abstract = {The Quantum Approximate Optimization Algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization problems whose performance can only improve with the number of layers {\$}p{\$}. While QAOA holds promise as an algorithm that can be run on near-term quantum computers, its computational power has not been fully explored. In this work, we study the QAOA applied to the Sherrington-Kirkpatrick (SK) model, which can be understood as energy minimization of {\$}n{\$} spins with all-to-all random signed couplings. There is a recent classical algorithm by Montanari that can efficiently find an approximate solution for a typical instance of the SK model to within {\$}(1-\backslashbackslashepsilon){\$} times the ground state energy, so we can only hope to match its performance with the QAOA. Our main result is a novel technique that allows us to evaluate the typical-instance energy of the QAOA applied to the SK model. We produce a formula for the expected value of the energy, as a function of the {\$}2p{\$} QAOA parameters, in the infinite size limit that can be evaluated on a computer with {\$}O(16{\^{}}p){\$} complexity. We found optimal parameters up to {\$}p=8{\$} running on a laptop. Moreover, we show concentration: With probability tending to one as {\$}n\backslashbackslashto\backslashbackslashinfty{\{}\backslash{\$}{\}}, measurements of the QAOA will produce strings whose energies concentrate at our calculated value. As an algorithm running on a quantum computer, there is no need to search for optimal parameters on an instance-by-instance basis since we can determine them in advance. What we have here is a new framework for analyzing the QAOA, and our techniques can be of broad interest for evaluating its performance on more general problems.},
archivePrefix = {arXiv},
arxivId = {1910.08187},
author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam and Zhou, Leo},
eprint = {1910.08187},
pages = {1--31},
title = {{The Quantum Approximate Optimization Algorithm and the Sherrington-Kirkpatrick Model at Infinite Size}},
url = {http://arxiv.org/abs/1910.08187},
year = {2019}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashbackslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashbackslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashbackslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
eprint = {1703.10593},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@article{Farhi2001,
abstract = {A quantum system will stay near its instantaneous ground state if the Hamiltonian that governs its evolution varies slowly enough. This quantum adiabatic behavior is the basis of a new class of algorithms for quantum computing. We test one such algorithm by applying it to randomly generated, hard, instances of an NP-complete problem. For the small examples that we can simulate, the quantum adiabatic algorithm works well, and provides evidence that quantum computers (if large ones can be built) may be able to outperform ordinary computers on hard sets of instances of NP-complete problems.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0104129},
author = {Farhi, E and Goldstone, J and Gutmann, S and Lapan, J and Lundgren, A and Preda, D},
doi = {10.1126/science.1057726},
eprint = {0104129},
issn = {0036-8075},
journal = {Science},
number = {5516},
pages = {472--475},
primaryClass = {quant-ph},
title = {{A Quantum Adiabatic Evolution Algorithm Applied to Random Instances of an NP-Complete Problem}},
volume = {292},
year = {2001}
}
@techreport{Huang,
abstract = {Deep learning based image-to-image translation methods aim at learning the joint distribution of the two domains and finding transformations between them. Despite recent GAN (Generative Adversarial Network) based methods have shown compelling results, they are prone to fail at preserving image-objects and maintaining translation consistency , which reduces their practicality on tasks such as generating large-scale training data for different domains. To address this problem, we purpose a structure-aware image-to-image translation network, which is composed of encoders, generators, discriminators and parsing nets for the two domains, respectively, in a unified framework. The purposed network generates more visually plausible images compared to competing methods on different image-translation tasks. In addition, we quantitatively evaluate different methods by training Faster-RCNN and YOLO with datasets generated from the image-translation results and demonstrate significant improvement on the detection accuracies by using the proposed image-object preserving network.},
author = {Huang, Sheng-Wei and Lin, Che-Tsung and Chen, Shu-Ping and Wu, Yen-Yi and Hsu, Po-Hao and Lai, Shang-Hong},
file = {::},
keywords = {Generative adversarial network,domain adaptation,image-to-image translation,object detection,semantic segmentation},
title = {{AugGAN: Cross Domain Adaptation with GAN-based Data Augmentation}}
}
@article{Ray1989,
abstract = {The Sherrington-Kirkpatrick model under a transverse field is studied here employing the Suzuki-Trotter formula to map the model to an equivalent classical one. The effective Thouless-Anderson-Palmer free energy is used to study the stability of the system, and Monte Carlo computer simulations of the effective classical model are performed to obtain the phase diagram and the magnetization overlap distribution. Our results indicate a trivial overlap distribution due to quantum fluctuations. The phase diagram shows a slight initial increase in the glass transition temperature Tg as the transverse field is switched on, confirming that obtained by Yokota. {\textcopyright} 1989 The American Physical Society.},
author = {Ray, P. and Chakrabarti, B. K. and Chakrabarti, Arunava},
doi = {10.1103/PhysRevB.39.11828},
issn = {01631829},
journal = {Physical Review B},
number = {16},
pages = {11828--11832},
title = {{Sherrington-Kirkpatrick model in a transverse field: Absence of replica symmetry breaking due to quantum fluctuations}},
volume = {39},
year = {1989}
}
@article{Luchak2017,
abstract = {We present a procedure for training and evaluating a deep neural network which can efficiently infer extensive parameters of arbitrarily large systems, doing so with O(N) complexity. We use a form of domain decomposition for training and inference, where each sub-domain (tile) is comprised of a non-overlapping focus region surrounded by an overlapping context region. The relative sizes of focus and context are physically motivated and depend on the locality length scale of the prob-lem. Extensive deep neural networks (EDNN) are a formulation of convolutional neural networks which provide a flexible and general approach, based on physical constraints, to describe multi-scale interactions. They are well suited to massively parallel inference, as no inter-thread communication is necessary during evaluation. Example uses for learning simple spin models, Laplacian (deriva-tive) operator, and approximating many-body quantum mechanical operators (within the density functional theory approach) are demonstrated.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.06686},
author = {Luchak, I and Mills, K and Ryczko, K and Domurad, A and Tamblyn, I},
eprint = {arXiv:1708.06686},
journal = {arXiv},
title = {{Extensive deep neural networks}},
url = {https://arxiv.org/pdf/1708.06686.pdf},
year = {2017}
}
@techreport{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747v2},
author = {Ruder, Sebastian},
eprint = {1609.04747v2},
file = {::},
title = {{An overview of gradient descent optimization algorithms *}},
url = {http://caffe.berkeleyvision.org/tutorial/solver.html},
year = {2016}
}
@techreport{Snoek,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization , in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparame-ters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
file = {::},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}}
}
@article{Ising1925,
author = {Ising, Ernst},
doi = {10.1007/BF02980577},
issn = {00443328},
journal = {Zeitschrift f{\"{u}}r Physik},
number = {1},
pages = {253--258},
title = {{Beitrag zur Theorie des Ferromagnetismus}},
volume = {31},
year = {1925}
}
@article{Chen2016,
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
number = {Nips},
title = {{InfoGAN : Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
year = {2016}
}
@article{Ryczko2018a,
abstract = {{\textcopyright} 2018 Elsevier B.V. We introduce a new method, called CNNAS (convolutional neural networks for atomistic systems), for calculating the total energy of atomic systems which rivals the computational cost of empirical potentials while maintaining the accuracy of ab initio calculations. This method uses deep convolutional neural networks (CNNs), where the input to these networks are simple representations of the atomic structure. We use this approach to predict energies obtained using density functional theory (DFT) for 2D hexagonal lattices of various types. Using a dataset consisting of graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures, with and without defects, we trained a deep CNN that is capable of predicting DFT energies to an extremely high accuracy, with a mean absolute error (MAE) of 0.198 meV/atom (maximum absolute error of 16.1 meV/atom). To explore our new methodology, we investigate the ability of a deep neural network (DNN) in predicting a Lennard-Jones energy and separation distance for a dataset of dimer molecules in both two and three dimensions. In addition, we systematically investigate the flexibility of the deep learning models by performing interpolation and extrapolation tests.},
author = {Ryczko, K. and Mills, K. and Luchak, I. and Homenick, C. and Tamblyn, I.},
doi = {10.1016/j.commatsci.2018.03.005},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {2D materials,Convolutional neural networks,Deep learning,Density functional theory,Dimer molecules},
title = {{Convolutional neural networks for atomistic systems}},
volume = {149},
year = {2018}
}
@incollection{6302929,
abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
author = {Rumelhart, D E and McClelland, J L},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
isbn = {9780262291408},
pages = {318--362},
publisher = {MIT Press},
title = {{Learning Internal Representations by Error Propagation}},
url = {https://ieeexplore.ieee.org/document/6302929},
year = {1987}
}
@article{Leleu2019,
author = {Leleu, Timoth{\'{e}}e and Yamamoto, Yoshihisa and McMahon, Peter L and Aihara, Kazuyuki},
doi = {10.1103/PhysRevLett.122.040607},
journal = {Phys. Rev. Lett.},
month = {feb},
number = {4},
pages = {40607},
publisher = {American Physical Society},
title = {{Destabilization of Local Minima in Analog Spin Systems by Correction of Amplitude Heterogeneity}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.122.040607},
volume = {122},
year = {2019}
}
@article{Bian2014,
abstract = {This paper discusses techniques for solving discrete optimization problems using quantum annealing. Practical issues likely to affect the computation include precision limitations, finite temperature, bounded energy range, sparse connectivity, and small numbers of qubits. To address these concerns we propose a way of finding energy representations with large classical gaps between ground and first excited states, efficient algorithms for mapping non-compatible Ising models into the hardware, and the use of decomposition methods for problems that are too large to fit in hardware. We validate the approach by describing experiments with D-Wave quantum hardware for low density parity check decoding with up to 1000 variables.},
author = {Bian, Zhengbing and Chudak, Fabian and Israel, Robert and Lackey, Brad and Macready, William G. and Roy, Aidan},
doi = {10.3389/fphy.2014.00056},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Discrete optimization,Penalty functions,Quantum annealing,Sparse Ising model},
number = {September},
pages = {1--10},
title = {{Discrete optimization using quantum annealing on sparse Ising models}},
volume = {2},
year = {2014}
}
@article{Zhu2017a,
abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.},
archivePrefix = {arXiv},
arxivId = {1709.01907},
author = {Zhu, Lingxue and Laptev, Nikolay},
doi = {10.1109/ICDMW.2017.19},
eprint = {1709.01907},
file = {::},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Anomaly detection,Bayesian neural networks,Predictive uncertainty,Time series},
month = {sep},
pages = {103--110},
publisher = {IEEE Computer Society},
title = {{Deep and Confident Prediction for Time Series at Uber}},
url = {http://arxiv.org/abs/1709.01907 http://dx.doi.org/10.1109/ICDMW.2017.19},
volume = {2017-Novem},
year = {2017}
}
@article{Kirkpatrick1983a,
abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
doi = {10.1126/science.220.4598.671},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Kirkpatrick, Gelatt, Vecchi - 1983 - Optimization by Simulated Annealing.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {may},
number = {4598},
pages = {671--680},
pmid = {17813860},
title = {{Optimization by Simulated Annealing}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.220.4598.671},
volume = {220},
year = {1983}
}
@article{McGeoch2013,
abstract = {This paper describes an experimental study of a novel computing system (algorithm plus platform) that carries out quantum annealing, a type of adiabatic quantum computation, to solve optimization problems. We compare this system to three conventional software solvers, using instances from three NP-hard problem domains. We also describe experiments to learn how performance of the quantum annealing algorithm depends on input. Copyright 2013 ACM.},
author = {McGeoch, Catherine C. and Wang, Cong},
doi = {10.1145/2482767.2482797},
isbn = {9781450320535},
journal = {Proceedings of the ACM International Conference on Computing Frontiers, CF 2013},
keywords = {Adiabatic quantum computing,D-Wave,Heuristics,Quantum annealing},
title = {{Experimental evaluation of an adiabiatic quantum system for combinatorial optimization}},
year = {2013}
}
@article{Carrasquilla2016,
abstract = {Neural networks can be used to identify phases and phase transitions in condensed matter systems via supervised machine learning. Readily programmable through modern software libraries, we show that a standard feed-forward neural network can be trained to detect multiple types of order parameter directly from raw state configurations sampled with Monte Carlo. In addition, they can detect highly non-trivial states such as Coulomb phases, and if modified to a convolutional neural network, topological phases with no conventional order parameter. We show that this classification occurs within the neural network without knowledge of the Hamiltonian or even the general locality of interactions. These results demonstrate the power of machine learning as a basic research tool in the field of condensed matter and statistical physics.},
archivePrefix = {arXiv},
arxivId = {1605.01735},
author = {Carrasquilla, Juan and Melko, Roger G},
doi = {10.1038/nphys4035},
eprint = {1605.01735},
issn = {1745-2473},
journal = {Nature Physics},
month = {feb},
number = {5},
pages = {431--434},
title = {{Machine learning phases of matter}},
url = {http://arxiv.org/abs/1605.01735 http://www.nature.com/doifinder/10.1038/nphys4035},
volume = {13},
year = {2017}
}
@article{Masters2018,
abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size {\$}m{\$}. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between {\$}m = 2{\$} and {\$}m = 32{\$}, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
archivePrefix = {arXiv},
arxivId = {1804.07612},
author = {Masters, Dominic and Luschi, Carlo},
eprint = {1804.07612},
file = {::},
journal = {arXiv},
month = {apr},
publisher = {arXiv},
title = {{Revisiting Small Batch Training for Deep Neural Networks}},
url = {http://arxiv.org/abs/1804.07612},
year = {2018}
}
@article{Brooks1983,
abstract = {CHARMM (Chemistry at HARvard Macromolecular Mechanics) is a highly flexible computer program which uses empirical energy functions to model macromolecular systems. The program can read or model build structures, energy minimize them by first- or second-derivative techniques, perform a normal mode or molecular dynamics simulation, and analyze the structural, equilibrium, and dynamic properties determined in these calculations. The operations that CHARMM can perform are described, and some implementation details are given. A set of parameters for the empirical energy function and a sample run are included.},
author = {Brooks, Bernard R and Bruccoleri, Robert E and Olafson, Barry D and States, David J and Swaminathan, S and Karplus, Martin},
doi = {10.1002/jcc.540040211},
isbn = {0192-8651},
issn = {1096987X},
journal = {Journal of Computational Chemistry},
number = {2},
pages = {187--217},
pmid = {1},
title = {{CHARMM: A program for macromolecular energy, minimization, and dynamics calculations}},
volume = {4},
year = {1983}
}
@article{Albarr_n_Arriagada_2020,
abstract = {The characterization of an operator by its eigenvectors and eigenvalues allows us to know its action over any quantum state. Here, we propose a protocol to obtain an approximation of the eigenvectors of an arbitrary Hermitian quantum operator. This protocol is based on measurement and feedback processes, which characterize a reinforcement learning protocol. Our proposal is composed of two systems, a black box named environment and a quantum state named agent. The role of the environment is to change any quantum state by a unitary matrix where is a Hermitian operator, and $\tau$ is a real parameter. The agent is a quantum state which adapts to some eigenvector of by repeated interactions with the environment, feedback process, and semi-random rotations. With this proposal, we can obtain an approximation of the eigenvectors of a random qubit operator with average fidelity over 90{\%} in less than 10 iterations, and surpass 98{\%} in less than 300 iterations. Moreover, for the two-qubit cases, the four eigenvectors are obtained with fidelities above 89{\%} in 8000 iterations for a random operator, and fidelities of 99{\%} for an operator with the Bell states as eigenvectors. This protocol can be useful to implement semi-autonomous quantum devices which should be capable of extracting information and deciding with minimal resources and without human intervention.},
author = {Albarr{\'{a}}n-Arriagada, F and Retamal, J C and Solano, E and Lamata, L},
doi = {10.1088/2632-2153/ab43b4},
journal = {Machine Learning: Science and Technology},
month = {feb},
number = {1},
pages = {15002},
publisher = {{\{}IOP{\}} Publishing},
title = {{Reinforcement learning for semi-autonomous approximate quantum eigensolver}},
url = {https://doi.org/10.1088{\%}2F2632-2153{\%}2Fab43b4},
volume = {1},
year = {2020}
}
@article{Aramon2019,
abstract = {The Fujitsu Digital Annealer is designed to solve fully connected quadratic unconstrained binary optimization (QUBO) problems. It is implemented on application-specific CMOS hardware and currently solves problems of up to 1,024 variables. The Digital Annealer's algorithm is currently based on simulated annealing; however, it differs from it in its utilization of an efficient parallel-trial scheme and a dynamic escape mechanism. In addition, the Digital Annealer exploits the massive parallelization that custom application-specific CMOS hardware allows. We compare the performance of the Digital Annealer to simulated annealing and parallel tempering with isoenergetic cluster moves on two-dimensional and fully connected spin-glass problems with bimodal and Gaussian couplings. These represent the respective limits of sparse vs. dense problems, as well as high-degeneracy vs. low-degeneracy problems. Our results show that the Digital Annealer currently exhibits a time-to-solution speedup of roughly two orders of magnitude for fully connected spin-glass problems with bimodal or Gaussian couplings, over the single-core implementations of simulated annealing and parallel tempering Monte Carlo used in this study. The Digital Annealer does not appear to exhibit a speedup for sparse two-dimensional spin-glass problems, which we explain on theoretical grounds. We also benchmarked an early implementation of the Parallel Tempering Digital Annealer. Our results suggest an improved scaling over the other algorithms for fully connected problems of average difficulty with bimodal disorder. The next generation of the Digital Annealer is expected to be able to solve fully connected problems up to 8,192 variables in size. This would enable the study of fundamental physics problems and industrial applications that were previously inaccessible using standard computing hardware or special-purpose quantum annealing machines.},
archivePrefix = {arXiv},
arxivId = {1806.08815},
author = {Aramon, Maliheh and Rosenberg, Gili and Valiante, Elisabetta and Miyazawa, Toshiyuki and Tamura, Hirotaka and Katzgraber, Helmut G},
doi = {10.3389/fphy.2019.00048},
eprint = {1806.08815},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Benchmarking,Custom application-specific CMOS hardware,Digital Annealer,Monte Carlo simulation,Optimization},
number = {APR},
title = {{Physics-inspired optimization for quadratic unconstrained problems using a digital Annealer}},
volume = {7},
year = {2019}
}
@article{Hastings1970,
author = {Hastings, B Y W K},
pages = {97--109},
title = {{Monte Carlo sampling methods using Markov chains and their applications}},
year = {1970}
}
@article{Mills2017d,
abstract = {We demonstrate that a generative adversarial network can be trained to produce Ising model configurations in distinct regions of phase space. In training a generative adversarial network, the discriminator neural network becomes very good a discerning examples from the training set and examples from the testing set. We demonstrate that this ability can be used as an anomaly detector, producing estimations of operator values along with a confidence in the prediction.},
archivePrefix = {arXiv},
arxivId = {1710.08053},
author = {Mills, Kyle and Tamblyn, Isaac},
eprint = {1710.08053},
file = {::},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{Phase space sampling and operator confidence with generative adversarial networks}},
url = {http://arxiv.org/abs/1710.08053},
year = {2017}
}
@inproceedings{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
keywords = {GoogLeNet},
mendeley-tags = {GoogLeNet},
month = {jun},
pages = {1--9},
pmid = {24920543},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
url = {http://ieeexplore.ieee.org/document/7298594/},
volume = {07-12-June},
year = {2015}
}
@article{Tanaka2016,
abstract = {We design a Convolutional Neural Network (CNN) which studies correlation between discretized inverse temperature and spin configuration of 2D Ising model and show that it can find a feature of the phase transition without teaching any a priori information for it. We also define a new order parameter via the CNN and show that it provides well approximated critical inverse temperature. In addition, we compare the activation functions for convolution layer and find that the Rectified Linear Unit (ReLU) is important to detect the phase transition of 2D Ising model.},
archivePrefix = {arXiv},
arxivId = {1609.09087},
author = {Tanaka, Akinori and Tomiya, Akio},
doi = {10.7566/JPSJ.86.063001},
eprint = {1609.09087},
issn = {0031-9015},
number = {Table I},
pages = {1--7},
title = {{Detection of phase transition via convolutional neural network}},
url = {http://arxiv.org/abs/1609.09087},
year = {2016}
}
@article{Martonak2002,
abstract = {Quantum annealing was recently found experimentally in a disordered spin- (formula presented) magnet to be more effective than its classical, thermal counterpart. We use the random two-dimensional Ising model as a test example and perform on it both classical and quantum (path-integral) Monte Carlo annealing. A systematic study of the dependence of the final residual energy on the annealing Monte Carlo time quantitatively demonstrates the superiority of quantum relative to classical annealing in this system. In order to determine the parameter regime for optimal efficiency of the quantum annealing procedure we explore a range of values of Trotter slice number P and temperature T. This identifies two different regimes of freezing with respect to efficiency of the algorithm, and leads to useful guidelines for the optimal choice of quantum annealing parameters. {\textcopyright} 2002 The American Physical Society.},
author = {Martoň{\'{a}}k, Roman and Santoro, Giuseppe E. and Tosatti, Erio},
doi = {10.1103/PhysRevB.66.094203},
issn = {1550235X},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {9},
pages = {1--8},
title = {{Quantum annealing by the path-integral Monte Carlo method: The two-dimensional random Ising model}},
volume = {66},
year = {2002}
}
@article{Santoro2002,
abstract = {Probing the lowest energy configuration of a complex system by quantum annealing was recently found to be more effective than its classical, thermal counterpart. By comparing classical and quantum Monte Carlo annealing protocols on the two-dimensional random Ising model (a prototype spin glass), we confirm the superiority of quantum annealing relative to classical annealing. We also propose a theory of quantum annealing based on cascade of Landau-Zener tunneling events. For both classical and quantum annealing, the residual energy after annealing is inversely proportional to a power of the logarithm of the annealing time, but the quantum case has a larger power that makes it faster.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0205280},
author = {Santoro, Giuseppe E.},
doi = {10.1126/science.1068774},
eprint = {0205280},
issn = {00368075},
journal = {Science},
month = {mar},
number = {5564},
pages = {2427--2430},
primaryClass = {cond-mat},
title = {{Theory of Quantum Annealing of an Ising Spin Glass}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1068774},
volume = {295},
year = {2002}
}
@techreport{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {::},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/}
}
@article{Tsukamoto2017,
abstract = {In today's world, there are many situations in which difficult decisions must be made under such constraints as a limited resource and a limited amount of time. These situations include disaster response planning, economic policy decision-making, and investment portfolio optimization. In such situations, it is often necessary to solve a "combinatorial optimization problem," which involves evaluating different combinations of various factors and selecting the optimum combination. Since the number of combinations increases explosively as the number of factors increases, it becomes difficult to find the best answer in a realistic amount of time using a von Neumann type processor. To give a solution for such problems, we have developed two schemes to speed up the 1024-bit Ising model and implemented them in a field-programmable gate array (FPGA). Testing demonstrated that a system using this architecture can solve a 32-city traveling salesman problem 12,000 times faster than the same algorithm running on a 3.5-GHz Intel Xeon E5-1620 v3 processor.},
author = {Tsukamoto, Sanroku and Takatsu, Motomu and Matsubara, Satoshi and Tamura, Hirotaka},
issn = {00162523},
journal = {Fujitsu Scientific and Technical Journal},
number = {5},
pages = {8--13},
title = {{An accelerator architecture for combinatorial optimization problems}},
volume = {53},
year = {2017}
}
@article{Battaglia2005,
abstract = {The path integral Monte Carlo simulated quantum annealing algorithm is applied to the optimization of a large hard instance of the random satisfiability problem (N=10000). The dynamical behavior of the quantum and the classical annealing are compared, showing important qualitative differences in the way of exploring the complex energy landscape of the combinatorial optimization problem. At variance with the results obtained for the Ising spin glass and for the traveling salesman problem, in the present case the linear-schedule quantum annealing performance is definitely worse than classical annealing. Nevertheless, a quantum cooling protocol based on field-cycling and able to outperform standard classical simulated annealing over short time scales is introduced. {\textcopyright} 2005 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0502468},
author = {Battaglia, Demian A. and Santoro, Giuseppe E. and Tosatti, Erio},
doi = {10.1103/PhysRevE.71.066707},
eprint = {0502468},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {6},
pages = {1--10},
primaryClass = {cond-mat},
title = {{Optimization by quantum annealing: Lessons from hard satisfiability problems}},
volume = {71},
year = {2005}
}
@book{deeplearningbook,
annote = {$\backslash$url{\{}http://www.deeplearningbook.org{\}}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@techreport{Hinton,
author = {Hinton, G E and Srivastava, N and Krizhevsky, A and Sutskever, I and Salakhutdinov, R R},
file = {::},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}}
}
@book{Sutton2018,
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/TNN.1998.712192},
edition = {Second},
isbn = {9780262039246},
issn = {1045-9227},
month = {sep},
publisher = {The MIT Press},
title = {{Reinforcement Learning: An Introduction}},
url = {http://incompleteideas.net/book/the-book-2nd.html https://www.cambridge.org/core/product/identifier/S0263574799271172/type/journal{\_}article http://ieeexplore.ieee.org/document/712192/},
year = {2018}
}
@techreport{adadelta,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information , different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701v1},
author = {Zeiler, Matthew D},
eprint = {1212.5701v1},
file = {::},
keywords = {Gradient Descent,Index Terms-Adaptive Learning Rates,Machine Learn-ing,Neural Networks},
title = {{ADADELTA: AN ADAPTIVE LEARNING RATE METHOD}}
}
@article{Bounds1987,
abstract = {Research in spin-glass physics, population genetics, and neural network dynamics has provided powerful methods for finding near-global optima of functions that have many local optima. These techniques are being applied successfully to a wide variety of scientific and engineering problems. They may also give new insights into combinatorial optimization problems. {\textcopyright} 1987 Nature Publishing Group.},
author = {Bounds, David G.},
doi = {10.1038/329215a0},
issn = {00280836},
journal = {Nature},
number = {6136},
pages = {215--219},
title = {{New optimization methods from physics and biology}},
volume = {329},
year = {1987}
}
@article{Mnih2016,
abstract = {Wc propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Lehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous methods for deep reinforcement learning.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2850--2869},
title = {{Asynchronous methods for deep reinforcement learning}},
volume = {4},
year = {2016}
}
@phdthesis{Linnainmaa1970,
author = {Linnainmaa, Seppo},
file = {:Users/kmills/Desktop/phd{\_}references/linnainmaa1970thesis.pdf:pdf},
school = {University of Helsinki},
title = {{Alogritmin kumulatiivinen py{\"{o}}ristysvirhe yksitt{\"{a}}isten py{\"{o}}ristysvirheiden Taylor-kehitelm{\"{a}}n{\"{a}}}},
year = {1970}
}
@article{Vinyals2019,
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P and Jaderberg, Max and Vezhnevets, Alexander S and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
doi = {10.1038/s41586-019-1724-z},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {August},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {http://www.nature.com/articles/s41586-019-1724-z},
year = {2019}
}
@article{Goodfellow,
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/abs/1406.2661v1},
author = {Goodfellow, Ian J and Pouget-abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-farley, David},
eprint = {/arxiv.org/abs/1406.2661v1},
pages = {1--9},
primaryClass = {https:},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/abs/1406.2661v1}
}
@article{Ryczko2017,
abstract = {We study dimer molecules in two and three dimensions using both a model Lennard-Jones potential as well as Density Functional Theory (DFT) calculations. We first show that deep convolutional neural networks (DCNNs) can be used to predict the distances and energies of a dimer molecule in both two and three dimensional space using the Lennard-Jones potential. We then use a similar approach to learn hexagonal surfaces including graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures.},
archivePrefix = {arXiv},
arxivId = {1706.09496},
author = {Ryczko, Kevin and Mills, Kyle and Luchak, Iryna and Homenick, Christa and Tamblyn, Isaac},
eprint = {1706.09496},
month = {jun},
pages = {1--18},
title = {{Convolutional neural networks for atomistic systems}},
url = {http://arxiv.org/abs/1706.09496},
year = {2017}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/kmills/Desktop/phd{\_}references/1701.07875 (1).pdf:pdf},
journal = {arXiv},
month = {jan},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{Nguyen2015,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {427--436},
pmid = {24309266},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
volume = {07-12-June},
year = {2015}
}
@article{Harris2010,
abstract = {A rf-superconducting quantum interference device (SQUID) flux qubit that is robust against fabrication variations in Josephson-junction critical currents and device inductance has been implemented. Measurements of the persistent current and of the tunneling energy between the two lowest-lying states, both in the coherent and incoherent regimes, are presented. These experimental results are shown to be in agreement with predictions of a quantum-mechanical Hamiltonian whose parameters were independently calibrated, thus justifying the identification of this device as a flux qubit. In addition, measurements of the flux and critical current noise spectral densities are presented that indicate that these devices with Nb wiring are comparable to the best Al wiring rf SQUIDs reported in the literature thus far, with a 1/f flux noise spectral density at 1 Hz of 1.3 -0.5 +0.7 $\mu$ $\Phi$0 /√ Hz. An explicit formula for converting the observed flux noise spectral density into a free-induction-decay time for a flux qubit biased to its optimal point and operated in the energy eigenbasis is presented. {\textcopyright} 2010 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {0909.4321},
author = {Harris, R. and Johansson, J. and Berkley, A. J. and Johnson, M. W. and Lanting, T. and Han, Siyuan and Bunyk, P. and Ladizinsky, E. and Oh, T. and Perminov, I. and Tolkacheva, E. and Uchaikin, S. and Chapple, E. M. and Enderud, C. and Rich, C. and Thom, M. and Wang, J. and Wilson, B. and Rose, G.},
doi = {10.1103/PhysRevB.81.134510},
eprint = {0909.4321},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {13},
pages = {1--19},
title = {{Experimental demonstration of a robust and scalable flux qubit}},
volume = {81},
year = {2010}
}
@article{Behler2016,
abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.},
author = {Behler, J??rg},
doi = {10.1063/1.4966192},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pages = {170901},
pmid = {27825224},
title = {{Perspective: Machine learning potentials for atomistic simulations}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966192},
volume = {145},
year = {2016}
}
@techreport{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly-sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
pages = {1223--1231},
title = {{Large Scale Distributed Deep Networks}},
volume = {25},
year = {2012}
}
@article{Lee,
abstract = {Inferring a generative model from data is a fundamental problem in machine learning. It is well-known that the Ising model is the maximum entropy model for binary variables which reproduces the sample mean and pairwise correlations. Learning the parameters of the Ising model from data is the challenge. We establish an analogy between the inverse Ising problem and the Ornstein-Zernike formalism in liquid state physics. Rather than analytically deriving the closure relation, we use a deep neural network to learn the closure from simulations of the Ising model. We show, using simulations as well as biochemical datasets, that the deep neural network model outperforms systematic field-theoretic expansions and can generalize well beyond the parameter regime of the training data. The neural network is able to learn from synthetic data, which can be generated with relative ease, to give accurate predictions on real world datasets.},
archivePrefix = {arXiv},
arxivId = {1706.08466},
author = {Lee, Alpha A},
eprint = {1706.08466},
month = {jun},
number = {2},
title = {{Inverse Ising inference by combining Ornstein-Zernike theory with deep learning}},
url = {http://arxiv.org/abs/1706.08466},
year = {2017}
}
@techreport{fullyconvolutional,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
file = {::},
title = {{Fully Convolutional Networks for Semantic Segmentation}}
}
@article{Kirkpatrick1984,
abstract = {Introduction: For detecting mould damage in indoor areas different methods are used. In contrast to traditional microbiological techniques the particular qualification of MVOC measurements was postulated. Thereyby the ability of MVOC to permeate barriers like wallpaper, panelling etc. was considered as the main advantage of this method. Aim of the study: A single-blinded study was performed to analyse whether indoor environments with and without mould infestation can be differentiated by MVOC analysis. Methods: In 40 dwellings with and in 44 dwellings without mould damage the concentrations of 8 selected MVOC considered as indicative, climatic parameters, airborne particles and the air exchange rate were determined and the characteristics of the dwellings were recorded. All collected data were analysed by multiple regression analysis. Results: For most of the analysed MVOC there was no correlation between the respective concentrations and the mould state. Only, for 2-methyl-1-butanol and 1-octen-3-ol a statistically significant, but weak correlation with the presence of a mould infestation was found. The concentrations of the "M"WOC were mainly influenced by other factors. 2-Methylfuran and 3-methylfuran, often used as main indicators for mould damage, had a highly significant correlation with the smoking state. The concentration of these compounds were also significantly correlated with the absolute humidity and the air exchange rate. Concerning 3-methyl-1-butanol, 2-hexanone, 3-heptanone and dimethyldisulphide there were found only weak correlations with other parameters, the air humidity being the strongest influencing factor. Discussion and conclusion: Based on the results of the study the MVOC investigated cannot be considered as mould indicators, since the parameter "mould infestation of the dwelling" explains only a small part of the total variability of the MVOC in the regression model. The major part of the total variability originated from unknown factors or could be explained in part by other factors included in the study as smoking state, air exchange rate, interior equipment etc. In spite of the statistically significant associations with 2-methyl-l-butanol and 1-octen-3-ol and the mould presence in the dwelling these compounds cannot be considered as indicator compounds, as only a small portion (10{\%} in this case) of the total variability can be attributed to the mould state. Such minor correlations lead to an excessive part of incorrect classifications, meaning that sensivity and specificity of these compounds are too low. Obviously the MVOC are not specific to bacteria or moulds, but also have other chemical and biological sources. The low specific emission rates found in lab tests are furthermore restricting the value of the MVOC as indicators.},
author = {Kirkpatrick, Scott},
doi = {10.1007/BF01009452},
issn = {0022-4715},
journal = {Journal of Statistical Physics},
keywords = {Indicator substances,Indoor moulds,MVOC,Microbial volatile organic compounds,Mouldy homes},
month = {mar},
number = {5-6},
pages = {975--986},
title = {{Optimization by simulated annealing: Quantitative studies}},
url = {http://link.springer.com/10.1007/BF01009452},
volume = {34},
year = {1984}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Start with raw spin configurations of a prototypical Ising model, we use principle component analysis to extract relevant low dimensional representations the original data, and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor as indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
issn = {2469-9950},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@techreport{rmsprop,
author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
file = {::},
title = {{Neural Networks for Machine Learning Lecture 6a Overview of mini-­-batch gradient descent}}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Wang2001,
abstract = {A new, general, efficient Monte Carlo (MC) algorithm that offers substantial advantages over existing approaches is presented. For demonstration purposes, the application of the algorithm to the 2D ten state Potts model and Ising model is discussed.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0011174},
author = {Wang, Fugao and Landau, D P},
doi = {10.1103/PhysRevLett.86.2050},
eprint = {0011174},
issn = {00319007},
journal = {Physical Review Letters},
number = {10},
pages = {2050--2053},
primaryClass = {cond-mat},
title = {{Efficient, multiple-range random walk algorithm to calculate the density of states}},
volume = {86},
year = {2001}
}
@book{VanLaarhoven1987,
address = {Dordrecht},
author = {van Laarhoven, Peter J. M. and Aarts, Emile H. L.},
doi = {10.1007/978-94-015-7744-1},
isbn = {978-90-481-8438-5},
publisher = {Springer Netherlands},
title = {{Simulated Annealing: Theory and Applications}},
url = {http://link.springer.com/10.1007/978-94-015-7744-1},
year = {1987}
}
@article{Krzywinski2013,
abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2613},
isbn = {1548-7091{\$}\backslash{\$}n1548-7105},
issn = {1548-7091},
journal = {Nature Methods},
number = {9},
pages = {809--810},
pmid = {24161969},
publisher = {Nature Publishing Group},
title = {{Points of significance: Importance of being uncertain}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2613},
volume = {10},
year = {2013}
}
@article{Silver2017,
abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:pdf},
isbn = {3013372370},
issn = {0028-0836},
journal = {Nature},
keywords = {AlphaGo Zero},
mendeley-tags = {AlphaGo Zero},
number = {7676},
pages = {354--359},
pmid = {29052630},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@article{Tolstikhin2017,
abstract = {Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
archivePrefix = {arXiv},
arxivId = {1701.02386},
author = {Tolstikhin, Ilya and Gelly, Sylvain and Bousquet, Olivier and Simon-Gabriel, Carl-Johann and Sch{\"{o}}lkopf, Bernhard},
eprint = {1701.02386},
pages = {1--28},
title = {{AdaGAN: Boosting Generative Models}},
url = {http://arxiv.org/abs/1701.02386},
year = {2017}
}
@article{Kim2017,
abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN},
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
eprint = {1703.05192},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.05192},
year = {2017}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {::},
issn = {00280836},
journal = {Nature},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
number = {6088},
pages = {533--536},
publisher = {Nature Publishing Group},
title = {{Learning representations by back-propagating errors}},
url = {https://www-nature-com.uproxy.library.dc-uoit.ca/articles/323533a0},
volume = {323},
year = {1986}
}
@techreport{Gal2016,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison , Bayesian models offer a mathematically grounded framework to reason about model uncertainty , but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs-extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predic-tive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142v6},
author = {Gal, Yarin and Uk, Zg201@cam Ac},
eprint = {1506.02142v6},
file = {::},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Zoubin Ghahramani}},
url = {http://yarin.co.},
year = {2016}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {0028-0836},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Ferdinand1969,
abstract = {The critical-point anomaly of a plane square m×n Ising lattice with periodic boundary conditions (a torus) is analyzed asymptotically in the limit n→∞ with {\$}\xi{\$}=mn fixed. Among other results, it is shown that for fixed {\$}\tau{\$}=n(T−Tc)Tc, the specific heat per spin of a large lattice is given by Cmn(T)kBmn=A0lnn+B({\$}\tau{\$}, {\$}\xi{\$})+B1({\$}\tau{\$})(lnn)n+B2({\$}\tau{\$}, {\$}\xi{\$})n+O[(lnn)3n2], where explicit expressions can be given for A0 and for the functions B, B1, and B2. It follows that the specific-heat peak of the finite lattice is rounded on a scale {\$}\delta{\$}={\$}\Delta{\$}TTc∼1n, while the maximum in Cmn(T) is displaced from Tc by {\$}\epsilon{\$}=(Tc−Tmax)Tc∼1n. For {\$}\xi{\$}0{\textgreater}{\$}\xi{\$}{\textgreater}{\$}\xi{\$}0−1, where {\$}\xi{\$}0=3.13927⋯, the maximum lies above Tc; but for {\$}\xi{\$}{\textgreater}{\$}\xi{\$}0 or {\$}\xi{\$}{\textless}{\$}\xi{\$}0−1, the maximum is depressed below Tc; when {\$}\xi{\$}=∞, {\$}\xi{\$}0, or {\$}\xi{\$}0−1, the relative shift in the maximum from Tc is only of order (lnn)n2. Detailed graphs and numerical data are presented, and the results are compared with some for lattices with free edges. Some heuristic arguments are developed which indicate the possible nature of finite-size critical-point effects in more general systems.},
author = {Ferdinand, Arthur E and Fisher, Michael E},
doi = {10.1103/PhysRev.185.832},
issn = {0031-899X},
journal = {Physical Review},
month = {sep},
number = {2},
pages = {832--846},
pmid = {11268102},
title = {{Bounded and Inhomogeneous Ising Models. I. Specific-Heat Anomaly of a Finite Lattice}},
url = {https://link.aps.org/doi/10.1103/PhysRev.185.832},
volume = {185},
year = {1969}
}
@article{Mills2017a,
abstract = {We train a deep convolutional neural network to accurately predict the energies and magnetizations of Ising model configurations, using both the traditional nearest-neighbour Hamiltonian, as well as a long-range screened Coulomb Hamiltonian. We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbour energy of the 4x4 Ising model. Using its success at this task, we motivate the study of the larger 8x8 Ising model, showing that the deep neural network can learn the nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. Finally, we teach the convolutional deep neural network to accurately predict a long-range interaction through a screened Coulomb Hamiltonian. In this case, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, 1600 times faster than a CUDA-optimized "exact" calculation.},
archivePrefix = {arXiv},
arxivId = {1706.09779},
author = {Mills, K and Tamblyn, I},
eprint = {1706.09779},
month = {jun},
title = {{Deep neural networks for direct, featureless learning through observation: the case of 2d spin models}},
url = {http://arxiv.org/abs/1706.09779},
year = {2017}
}
@article{Metz2016,
abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
archivePrefix = {arXiv},
arxivId = {1611.02163},
author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
eprint = {1611.02163},
pages = {1--25},
pmid = {202927},
title = {{Unrolled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.02163},
year = {2016}
}
@article{Tiunov2019,
abstract = {The coherent Ising machine (CIM) enables efficient sampling of low-lying energy states of the Ising Hamiltonian with all-to-all connectivity by encoding the spins in the amplitudes of pulsed modes in an optical parametric oscillator (OPO). The interaction between the pulses is realized by means of measurement-based optoelectronic feedforward, which enhances the gain for lower-energy spin configurations. We present an efficient method of simulating the CIM on a classical computer that outperforms the CIM itself, as well as the noisy mean-field annealer in terms of both the quality of the samples and the computational speed. It is furthermore advantageous with respect to the CIM in that it can handle Ising Hamiltonians with arbitrary real-valued node coupling strengths. These results illuminate the nature of the faster performance exhibited by the CIM and may give rise to a new class of quantum-inspired algorithms of classical annealing that can successfully compete with existing methods.},
author = {Tiunov, Egor S and Ulanov, Alexander E and Lvovsky, A I},
doi = {10.1364/OE.27.010288},
journal = {Opt. Express},
keywords = {Computer simulation; Continuous variables; Neural},
month = {apr},
number = {7},
pages = {10288--10295},
publisher = {OSA},
title = {{Annealing by simulating the coherent Ising machine}},
url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-27-7-10288},
volume = {27},
year = {2019}
}
@article{Farhi2014,
abstract = {We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.},
archivePrefix = {arXiv},
arxivId = {1411.4028},
author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
eprint = {1411.4028},
pages = {1--16},
title = {{A Quantum Approximate Optimization Algorithm}},
url = {http://arxiv.org/abs/1411.4028},
year = {2014}
}
@article{Casert2020,
abstract = {Machine learning provides a novel avenue for the study of experimental realizations of many-body systems, and has recently been proven successful in analyzing properties of experimental data of ultracold quantum gases. We here show that deep learning succeeds in the more challenging task of modelling such an experimental data distribution. Our generative model (RUGAN) is able to produce snapshots of a doped two-dimensional Fermi-Hubbard model that are indistinguishable from previously reported experimental realizations. Importantly, it is capable of accurately generating snapshots at conditions for which it did not observe any experimental data, such as at higher doping values. On top of that, our generative model extracts relevant patterns from small-scale examples and can use these to construct new configurations at a larger size that serve as a precursor to observations at scales that are currently experimentally inaccessible. The snapshots created by our model-which come at effectively no cost-are extremely useful as they can be employed to quantitatively test new theoretical developments under conditions that have not been explored experimentally, parameterize phenomenological models, or train other, more data-intensive, machine learning methods. We provide predictions for experimental observables at unobserved conditions and benchmark these against modern theoretical frameworks. The deep learning method we develop here is broadly applicable and can be used for the efficient large-scale simulation of equilibrium and nonequilibrium physical systems.},
archivePrefix = {arXiv},
arxivId = {2002.07055},
author = {Casert, Corneel and Mills, Kyle and Vieijra, Tom and Ryckebusch, Jan and Tamblyn, Isaac},
eprint = {2002.07055},
file = {:Users/kmills/Desktop/external{\_}s/2002.07055.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--11},
title = {{Optical lattice experiments at unobserved conditions and scales through generative adversarial deep learning}},
year = {2020}
}
@article{Amin2018,
abstract = {Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, we propose a new machine-learning approach based on quantum Boltzmann distribution of a quantum Hamiltonian. Because of the noncommutative nature of quantum mechanics, the training process of the quantum Boltzmann machine (QBM) can become nontrivial. We circumvent the problem by introducing bounds on the quantum probabilities. This allows us to train the QBM efficiently by sampling. We show examples of QBM training with and without the bound, using exact diagonalization, and compare the results with classical Boltzmann training. We also discuss the possibility of using quantum annealing processors for QBM training and application.},
author = {Amin, Mohammad H and Andriyash, Evgeny and Rolfe, Jason and Kulchytskyy, Bohdan and Melko, Roger},
doi = {10.1103/PhysRevX.8.021050},
file = {::},
journal = {Physical Review X},
keywords = {doi:10.1103/PhysRevX.8.021050 url:https://doi.org/},
title = {{Quantum Boltzmann Machine}},
volume = {8},
year = {2018}
}
@article{Bunyk2014,
abstract = {We have developed a quantum annealing processor, based on an array of tunable coupled rf-SQUID flux qubits, fabricated in a superconducting integrated circuit process. Implementing this type of processor at a scale of 512 qubits and 1472 programmable interqubit couplers and operating at ∼ 20 mK has required attention to a number of considerations that one may ignore at the smaller scale of a few dozen or so devices. Here, we discuss some of these considerations, and the delicate balance necessary for the construction of a practical processor that respects the demanding physical requirements imposed by a quantum algorithm. In particular, we will review some of the design tradeoffs at play in the floor planning of the physical layout, driven by the desire to have an algorithmically useful set of interqubit couplers, and the simultaneous need to embed programmable control circuitry into the processor fabric. In this context, we have developed a new ultralow-power embedded superconducting digital-to-analog flux converter (DAC) used to program the processor with zero static power dissipation, optimized to achieve maximum flux storage density per unit area. The 512 single-stage, 3520 two-stage, and 512 three-stage flux DACs are controlled with an XYZ addressing scheme requiring 56 wires. Our estimate of on-chip dissipated energy for worst-case reprogramming of the whole processor is ∼65 fJ. Several chips based on this architecture have been fabricated and operated successfully at our facility, as well as two outside facilities (see, for example, the recent reporting by Jones).},
archivePrefix = {arXiv},
arxivId = {1401.5504},
author = {Bunyk, P. I. and Hoskinson, Emile M. and Johnson, Mark W. and Tolkacheva, Elena and Altomare, Fabio and Berkley, Andrew J. and Harris, Richard and Hilton, Jeremy P. and Lanting, Trevor and Przybysz, Anthony J. and Whittaker, Jed},
doi = {10.1109/TASC.2014.2318294},
eprint = {1401.5504},
issn = {10518223},
journal = {IEEE Transactions on Applied Superconductivity},
keywords = {Computational physics,quantum computing,superconducting integrated circuits},
number = {4},
pages = {1--10},
publisher = {IEEE},
title = {{Architectural Considerations in the Design of a Superconducting Quantum Annealing Processor}},
volume = {24},
year = {2014}
}
@article{Berner2019,
author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\c{e}}biak, Przemys{\l}aw Psyho and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'{o}}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pond{\'{e}}, Henrique and Pinto, De Oliveira and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
title = {{Dota 2 with Large Scale Deep Reinforcement Learning}},
year = {2019}
}
@article{Hen2012,
abstract = {We propose a method using a quantum annealer-an analog quantum computer based on the principles of quantum adiabatic evolution-to solve the graph isomorphism problem, in which one has to determine whether two graphs are isomorphic (i.e., can be transformed into each other simply by a relabeling of the vertices). We demonstrate the capabilities of the method by analyzing several types of graph families, focusing on graphs with particularly high symmetry called strongly regular graphs. We also show that our method is applicable, within certain limitations, to currently available quantum hardware such as D-Wave One. {\textcopyright} 2012 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1207.1712},
author = {Hen, Itay and Young, A. P.},
doi = {10.1103/PhysRevA.86.042310},
eprint = {1207.1712},
issn = {10502947},
journal = {Physical Review A - Atomic, Molecular, and Optical Physics},
number = {4},
pages = {1--8},
title = {{Solving the graph-isomorphism problem with a quantum annealer}},
volume = {86},
year = {2012}
}
@article{Keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say {\$}32{\$}-{\$}512{\$} data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
eprint = {1609.04836},
file = {::},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
url = {http://arxiv.org/abs/1609.04836},
year = {2016}
}
@article{Salimans2016a,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
isbn = {0924-6495},
issn = {09246495},
pages = {1--10},
pmid = {23259955},
title = {{Improved Techniques for Training GANs}},
url = {http://arxiv.org/abs/1606.03498},
year = {2016}
}
@article{Bowles2018,
abstract = {One of the biggest issues facing the use of machine learning in medical imaging is the lack of availability of large, labelled datasets. The annotation of medical images is not only expensive and time consuming but also highly dependent on the availability of expert observers. The limited amount of training data can inhibit the performance of supervised machine learning algorithms which often need very large quantities of data on which to train to avoid overfitting. So far, much effort has been directed at extracting as much information as possible from what data is available. Generative Adversarial Networks (GANs) offer a novel way to unlock additional information from a dataset by generating synthetic samples with the appearance of real images. This paper demonstrates the feasibility of introducing GAN derived synthetic data to the training datasets in two brain segmentation tasks, leading to improvements in Dice Similarity Coefficient (DSC) of between 1 and 5 percentage points under different conditions, with the strongest effects seen fewer than ten training image stacks are available.},
archivePrefix = {arXiv},
arxivId = {1810.10863},
author = {Bowles, Christopher and Chen, Liang and Guerrero, Ricardo and Bentley, Paul and Gunn, Roger and Hammers, Alexander and Dickie, David Alexander and Hern{\'{a}}ndez, Maria Vald{\'{e}}s and Wardlaw, Joanna and Rueckert, Daniel},
eprint = {1810.10863},
file = {::},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1810.10863},
year = {2018}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {Cell Biology,Life Sciences,Mathematical and Computational Biology,general},
month = {dec},
number = {4},
pages = {115--133},
publisher = {Kluwer Academic Publishers},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@article{Mills2017b,
author = {Mills, Kyle and Spanner, Michael and Tamblyn, Isaac},
doi = {10.1103/PhysRevA.96.042113},
issn = {2469-9926},
journal = {Physical Review A},
month = {oct},
number = {4},
pages = {042113},
title = {{Deep learning and the Schr{\"{o}}dinger equation}},
url = {https://link.aps.org/doi/10.1103/PhysRevA.96.042113},
volume = {96},
year = {2017}
}
@article{Agostinelli2019,
author = {Agostinelli, Forest and McAleer, Stephen and Shmakov, Alexander and Baldi, Pierre},
doi = {10.1038/s42256-019-0070-z},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
month = {aug},
number = {8},
pages = {356--363},
title = {{Solving the Rubik's cube with deep reinforcement learning and search}},
url = {http://www.nature.com/articles/s42256-019-0070-z},
volume = {1},
year = {2019}
}
@inproceedings{kakade2002approximately,
author = {Kakade, Sham and Langford, John},
booktitle = {ICML},
pages = {267--274},
title = {{Approximately optimal approximate reinforcement learning}},
volume = {2},
year = {2002}
}
@article{Wetzel2017,
abstract = {We employ unsupervised machine learning techniques to learn latent parameters which best describe states of the two-dimensional Ising model and the three-dimensional XY model. These methods range from principal component analysis to artificial neural network based variational autoencoders. The states are sampled using a Monte-Carlo simulation above and below the critical temperature. We find that the predicted latent parameters correspond to the known order parameters. The latent representation of the states of the models in question are clustered, which makes it possible to identify phases without prior knowledge of their existence or the underlying Hamiltonian. Furthermore, we find that the reconstruction loss function can be used as a universal identifier for phase transitions.},
archivePrefix = {arXiv},
arxivId = {1703.02435},
author = {Wetzel, Sebastian Johann},
doi = {10.1038/nphys4037},
eprint = {1703.02435},
issn = {1745-2473},
pages = {1--8},
title = {{Unsupervised learning of phase transitions: from principal component analysis to variational autoencoders}},
url = {http://arxiv.org/abs/1703.02435},
year = {2017}
}
@article{Jetchev2016,
abstract = {Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.},
archivePrefix = {arXiv},
arxivId = {1611.08207},
author = {Jetchev, Nikolay and Bergmann, Urs and Vollgraf, Roland},
eprint = {1611.08207},
number = {ii},
title = {{Texture Synthesis with Spatial Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.08207},
year = {2016}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Morningstar2017,
abstract = {It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.},
archivePrefix = {arXiv},
arxivId = {1708.04622},
author = {Morningstar, Alan and Melko, Roger G},
eprint = {1708.04622},
title = {{Deep Learning the Ising Model Near Criticality}},
url = {http://arxiv.org/abs/1708.04622},
year = {2017}
}
@article{Inagaki2016,
abstract = {The analysis and optimization of complex systems can be reduced to mathematical problems collectively known as combinatorial optimization. Many such problems can be mapped onto ground-state search problems of the Ising model, and various artificial spin systems are now emerging as promising approaches. However, physical Ising machines have suffered from limited numbers of spin-spin couplings because of implementations based on localized spins, resulting in severe scalability problems. We report a 2000-spin network with all-to-all spin-spin couplings. Using a measurement and feedback scheme, we coupled time-multiplexed degenerate optical parametric oscillators to implement maximum cut problems on arbitrary graph topologies with up to 2000 nodes. Our coherent Ising machine outperformed simulated annealing in terms of accuracy and computation time for a 2000-node complete graph.},
author = {Inagaki, Takahiro and Haribara, Yoshitaka and Igarashi, Koji and Sonobe, Tomohiro and Tamate, Shuhei and Honjo, Toshimori and Marandi, Alireza and McMahon, Peter L and Umeki, Takeshi and Enbutsu, Koji and Tadanaga, Osamu and Takenouchi, Hirokazu and Aihara, Kazuyuki and Kawarabayashi, Ken Ichi and Inoue, Kyo and Utsunomiya, Shoko and Takesue, Hiroki},
doi = {10.1126/science.aah4243},
issn = {10959203},
journal = {Science},
number = {6312},
pages = {603--606},
title = {{A coherent Ising machine for 2000-node optimization problems}},
volume = {354},
year = {2016}
}
@article{GoogleResearch2015,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
journal = {None},
month = {mar},
number = {212},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf http://arxiv.org/abs/1603.04467},
volume = {1},
year = {2016}
}
@article{Mnih2013,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://www.nature.com/doifinder/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Kadowaki1998,
abstract = {We introduce quantum fluctuations into the simulated annealing process of optimization problems, aiming at faster convergence to the optimal state. Quantum fluctuations cause transitions between states and thus play the same role as thermal fluctuations in the conventional approach. The idea is tested by the transverse Ising model, in which the transverse field is a function of time similar to the temperature in the conventional method. The goal is to find the ground state of the diagonal part of the Hamiltonian with high accuracy as quickly as possible. We have solved the time-dependent Schr{\"{o}}dinger equation numerically for small size systems with various exchange interactions. Comparison with the results of the corresponding classical (thermal) method reveals that the quantum annealing leads to the ground state with much larger probability in almost all cases if we use the same annealing schedule. {\textcopyright} 1998 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/9804280},
author = {Kadowaki, Tadashi and Nishimori, Hidetoshi},
doi = {10.1103/PhysRevE.58.5355},
eprint = {9804280},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {5},
pages = {5355--5363},
primaryClass = {cond-mat},
title = {{Quantum annealing in the transverse Ising model}},
volume = {58},
year = {1998}
}
@article{Barahona1982,
abstract = {In a spin glass with Ising spins, the problems of computing the magnetic partition function and finding a ground state are studied. In a finite two-dimensional lattice these problems can be solved by algorithms that require a number of steps bounded by a polynomial function of the size of the lattice. In contrast to this fact, the same problems are shown to belong to the class of NP-hard problems, both in the two-dimensional case within a magnetic field, and in the three-dimensional case. NP-hardness of a problem suggests that it is very unlikely that a polynomial algorithm could exist to solve it. {\textcopyright} 1982 The Japan Society of Applied Physics.},
author = {Barahona, F.},
doi = {10.1088/0305-4470/15/10/028},
issn = {0305-4470},
journal = {Journal of Physics A: Mathematical and General},
month = {oct},
number = {10},
pages = {3241--3253},
title = {{On the computational complexity of Ising spin glass models}},
url = {http://stacks.iop.org/0305-4470/15/i=10/a=028?key=crossref.1fe57df6674a7c759374b69321415b44},
volume = {15},
year = {1982}
}
@techreport{adagrad,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function , which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Singer, Yoram},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan}},
volume = {12},
year = {2011}
}
@article{LeCun1998b,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Dickson2013,
abstract = {Efforts to develop useful quantum computers have been blocked primarily by environmental noise. Quantum annealing is a scheme of quantum computation that is predicted to be more robust against noise, because despite the thermal environment mixing the system's state in the energy basis, the system partially retains coherence in the computational basis, and hence is able to establish well-defined eigenstates. Here we examine the environment's effect on quantum annealing using 16 qubits of a superconducting quantum processor. For a problem instance with an isolated small-gap anticrossing between the lowest two energy levels, we experimentally demonstrate that, even with annealing times eight orders of magnitude longer than the predicted single-qubit decoherence time, the probabilities of performing a successful computation are similar to those expected for a fully coherent system. Moreover, for the problem studied, we show that quantum annealing can take advantage of a thermal environment to achieve a speedup factor of up to 1,000 over a closed system. {\textcopyright} 2013 Macmillan Publishers Limited. All rights reserved.},
author = {Dickson, N. G. and Johnson, M. W. and Amin, M. H. and Harris, R. and Altomare, F. and Berkley, A. J. and Bunyk, P. and Cai, J. and Chapple, E. M. and Chavez, P. and Cioata, F. and Cirip, T. and Debuen, P. and Drew-Brook, M. and Enderud, C. and Gildert, S. and Hamze, F. and Hilton, J. P. and Hoskinson, E. and Karimi, K. and Ladizinsky, E. and Ladizinsky, N. and Lanting, T. and Mahon, T. and Neufeld, R. and Oh, T. and Perminov, I. and Petroff, C. and Przybysz, A. and Rich, C. and Spear, P. and Tcaciuc, A. and Thom, M. C. and Tolkacheva, E. and Uchaikin, S. and Wang, J. and Wilson, A. B. and Merali, Z. and Rose, G.},
doi = {10.1038/ncomms2920},
issn = {20411723},
journal = {Nature Communications},
number = {May},
pages = {1--6},
title = {{Thermally assisted quantum annealing of a 16-qubit problem}},
volume = {4},
year = {2013}
}
@techreport{Salakhutdinov,
abstract = {Most of the existing approaches to collab-orative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models , called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6{\%} better than the score of Netflix's own system.},
author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
file = {::},
title = {{Restricted Boltzmann Machines for Collaborative Filtering}}
}
@article{OpenAI2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
eprint = {1910.07113},
pages = {1--51},
title = {{Solving Rubik's Cube with a Robot Hand}},
url = {http://arxiv.org/abs/1910.07113},
year = {2019}
}
@article{Portman2016,
abstract = {In this paper, we build and explore supervised learning models of ferromagnetic system behavior, using Monte-Carlo sampling of the spin configuration space generated by the 2D Ising model. Given the enormous size of the space of all possible Ising model realizations, the question arises as to how to choose a reasonable number of samples that will form physically meaningful and non-intersecting training and testing datasets. Here, we propose a sampling technique called ID-MH that uses the Metropolis-Hastings algorithm creating Markov process across energy levels within the predefined configuration subspace. We show that application of this method retains phase transitions in both training and testing datasets and serves the purpose of validation of a machine learning algorithm. For larger lattice dimensions, ID-MH is not feasible as it requires knowledge of the complete configuration space. As such, we develop a new "block-ID" sampling strategy: it decomposes the given structure into square blocks with lattice dimension no greater than 5 and uses ID-MH sampling of candidate blocks. Further comparison of the performance of commonly used machine learning methods such as random forests, decision trees, k nearest neighbors and artificial neural networks shows that the PCA-based Decision Tree regressor is the most accurate predictor of magnetizations of the Ising model. For energies, however, the accuracy of prediction is not satisfactory, highlighting the need to consider more algorithmically complex methods (e.g., deep learning).},
archivePrefix = {arXiv},
arxivId = {1611.05891},
author = {Portman, Nataliya and Tamblyn, Isaac},
doi = {10.1016/j.jcp.2017.06.045},
eprint = {1611.05891},
issn = {00219991},
journal = {Journal of Computational Physics},
month = {jul},
pages = {43},
title = {{Sampling algorithms for validation of supervised learning models for Ising-like systems}},
url = {http://arxiv.org/abs/1611.05891 http://linkinghub.elsevier.com/retrieve/pii/S0021999117304990},
year = {2017}
}
@article{Salimans2016,
author = {Salimans, Tim and Goodfellow, Ian and Cheung, Vicki and Radford, Alec and Chen, Xi},
number = {Nips},
pages = {1--9},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@article{Venturelli2015,
abstract = {A quantum annealing solver for the renowned job-shop scheduling problem (JSP) is presented in detail. After formulating the problem as a time-indexed quadratic unconstrained binary optimization problem, several pre-processing and graph embedding strategies are employed to compile optimally parametrized families of the JSP for scheduling instances of up to six jobs and six machines on the D-Wave Systems Vesuvius processor. Problem simplifications and partitioning algorithms, including variable pruning and running strategies that consider tailored binary searches, are discussed and the results from the processor are compared against state-of-the-art global-optimum solvers.},
archivePrefix = {arXiv},
arxivId = {1506.08479},
author = {Venturelli, Davide and Marchand, Dominic J. J. and Rojo, Galo},
eprint = {1506.08479},
month = {jun},
pages = {1--15},
title = {{Quantum Annealing Implementation of Job-Shop Scheduling}},
url = {http://arxiv.org/abs/1506.08479},
year = {2015}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Start with raw spin configurations of a prototypical Ising model, we use principle component analysis to extract relevant low dimensional representations the original data, and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor as indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
issn = {2469-9950},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@inproceedings{Frid-Adar2018,
abstract = {In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6{\%} sensitivity and 88.4{\%} specificity. By adding the synthetic data augmentation the results significantly increased to 85.7{\%} sensitivity and 92.4{\%} specificity.},
archivePrefix = {arXiv},
arxivId = {1801.02385},
author = {Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2018.8363576},
eprint = {1801.02385},
isbn = {9781538636367},
issn = {19458452},
keywords = {Data augmentation,Generative adversarial network,Image synthesis,Lesion classification,Liver lesions},
month = {may},
pages = {289--293},
publisher = {IEEE Computer Society},
title = {{Synthetic data augmentation using GAN for improved liver lesion classification}},
volume = {2018-April},
year = {2018}
}
@inproceedings{Werbos:81sensitivity,
author = {Werbos, P J},
booktitle = {Proceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC},
keywords = {juergen},
pages = {762--770},
title = {{Applications of Advances in Nonlinear Sensitivity Analysis}},
year = {1981}
}
@article{Finnila1994,
author = {Finnila, A B and Gomez, M A and Sebenik, C and Stenson, C and Doll, J D},
doi = {10.1016/0009-2614(94)00117-0},
issn = {00092614},
journal = {Chemical Physics Letters},
month = {mar},
number = {5-6},
pages = {343--348},
title = {{Quantum annealing: A new method for minimizing multidimensional functions}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0009261494001170},
volume = {219},
year = {1994}
}
@techreport{Salakhutdinova,
abstract = {We present a new learning algorithm for Boltz-mann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer "pre-training" phase that allows variational inference to be initialized with a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
file = {::},
title = {{Deep Boltzmann Machines}}
}
@article{Mills2017,
abstract = {We have trained a deep (convolutional) neural network to predict the ground-state energy of an electron in four classes of confining two-dimensional electrostatic potentials. On randomly generated potentials, for which there is no analytic form for either the potential or the ground-state energy, the neural network model was able to predict the ground-state energy to within chemical accuracy, with a median absolute error of 1.49 mHa. We also investigate the performance of the model in predicting other quantities such as the kinetic energy and the first excited-state energy of random potentials. While we demonstrated this approach on a simple, tractable problem, the transferability and excellent performance of the resulting model suggests further applications of deep neural networks to problems of electronic structure.},
archivePrefix = {arXiv},
arxivId = {1702.01361},
author = {Mills, Kyle and Spanner, Michael and Tamblyn, Isaac},
doi = {10.1103/PhysRevA.96.042113},
eprint = {1702.01361},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills, Spanner, Tamblyn - 2017 - Deep learning and the Schr{\"{o}}dinger equation.pdf:pdf},
issn = {2469-9926},
journal = {Physical Review A},
month = {oct},
number = {4},
pages = {042113},
title = {{Deep learning and the Schr{\"{o}}dinger equation}},
url = {http://arxiv.org/abs/1702.01361 https://link.aps.org/doi/10.1103/PhysRevA.96.042113},
volume = {96},
year = {2017}
}
@article{Liers2005,
author = {Liers, Frauke and J{\"{u}}nger, Michael and Reinelt, Gerhard and Rinaldi, Giovanni},
doi = {10.1002/3527603794.ch4},
isbn = {3527404066},
journal = {New Optimization Algorithms in Physics},
pages = {47--69},
title = {{Computing Exact Ground States of Hard Ising Spin Glass Problems by Branch-and-Cut}},
year = {2005}
}
@article{Mills2017a,
archivePrefix = {arXiv},
arxivId = {1706.09779v1},
author = {Mills, K and Tamblyn, I},
eprint = {1706.09779v1},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Mills, Tamblyn - 2017 - Deep neural networks for direct, featureless learning through observation the case of 2d spin models(2).pdf:pdf},
title = {{Deep neural networks for direct, featureless learning through observation: the case of 2d spin models}},
year = {2017}
}
@article{Bergmann2017,
abstract = {This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribution by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scalable and it can generate output images of arbitrary large size.},
archivePrefix = {arXiv},
arxivId = {1705.06566},
author = {Bergmann, Urs and Jetchev, Nikolay and Vollgraf, Roland},
eprint = {1705.06566},
title = {{Learning Texture Manifolds with the Periodic Spatial GAN}},
url = {http://arxiv.org/abs/1705.06566},
year = {2017}
}
@article{Johnson2011,
abstract = {Many interesting but practically intractable problems can be reduced to that of finding the ground state of a system of interacting spins; however, finding such a ground state remains computationally difficult. It is believed that the ground state of some naturally occurring spin systems can be effectively attained through a process called quantum annealing. If it could be harnessed, quantum annealing might improve on known methods for solving certain types of problem. However, physical investigation of quantum annealing has been largely confined to microscopic spins in condensed-matter systems. Here we use quantum annealing to find the ground state of an artificial Ising spin system comprising an array of eight superconducting flux quantum bits with programmable spin-spin couplings. We observe a clear signature of quantum annealing, distinguishable from classical thermal annealing through the temperature dependence of the time at which the system dynamics freezes. Our implementation can be configured in situ to realize a wide variety of different spin networks, each of which can be monitored as it moves towards a low-energy configuration. This programmable artificial spin network bridges the gap between the theoretical study of ideal isolated spin networks and the experimental investigation of bulk magnetic samples. Moreover, with an increased number of spins, such a system may provide a practical physical means to implement a quantum algorithm, possibly allowing more-effective approaches to solving certain classes of hard combinatorial optimization problems. {\textcopyright} 2011 Macmillan Publishers Limited. All rights reserved.},
author = {Johnson, M. W. and Amin, M. H.S. and Gildert, S. and Lanting, T. and Hamze, F. and Dickson, N. and Harris, R. and Berkley, A. J. and Johansson, J. and Bunyk, P. and Chapple, E. M. and Enderud, C. and Hilton, J. P. and Karimi, K. and Ladizinsky, E. and Ladizinsky, N. and Oh, T. and Perminov, I. and Rich, C. and Thom, M. C. and Tolkacheva, E. and Truncik, C. J.S. and Uchaikin, S. and Wang, J. and Wilson, B. and Rose, G.},
doi = {10.1038/nature10012},
issn = {00280836},
journal = {Nature},
number = {7346},
pages = {194--198},
title = {{Quantum annealing with manufactured spins}},
volume = {473},
year = {2011}
}
@inproceedings{LeCun90,
author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R and Hubbard, Wayne and Jackel, Lawrence},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Touretzky, D},
pages = {396--404},
publisher = {Morgan-Kaufmann},
title = {{Handwritten Digit Recognition with a Back-Propagation Network}},
url = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
volume = {2},
year = {1990}
}
@techreport{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Ca, James Bergstra@umontreal and Ca, Yoshua Bengio@umontreal},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization Yoshua Bengio}},
url = {http://scikit-learn.sourceforge.net.},
volume = {13},
year = {2012}
}
@article{Sherrington1975,
abstract = {Amorphous Co2Ge was synthesized by ball milling crystalline Co2Ge, a state which cannot be obtained by traditional rapid solidification. It exhibits a single transition from the paramagnetic to the spin-glass state at 41 K (Tf) as defined by a sharp asymmetric cusp in the lowest field ac susceptibility. In fields 30 Oe the cusp in both ac and dc susceptibility curves becomes a rounded maximum and Tf decreases. Below Tf the magnetization reveals thermal hysteresis. Using the Sherrington-Kirkpatrick theory the spin-glass order parameter is derived and a spin-glass phase was synthesized by mechanical milling. This phase can be described by mean-field theory. {\textcopyright}1994 The American Physical Society.},
author = {Sherrington, David and Kirkpatrick, Scott},
doi = {10.1103/PhysRevLett.35.1792},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {dec},
number = {26},
pages = {1792--1796},
title = {{Solvable Model of a Spin-Glass}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792},
volume = {35},
year = {1975}
}
@article{Ikeda2019,
abstract = {Quantum annealing is a promising heuristic method to solve combinatorial optimization problems, and efforts to quantify performance on real-world problems provide insights into how this approach may be best used in practice. We investigate the empirical performance of quantum annealing to solve the Nurse Scheduling Problem (NSP) with hard constraints using the D-Wave 2000Q quantum annealing device. NSP seeks the optimal assignment for a set of nurses to shifts under an accompanying set of constraints on schedule and personnel. After reducing NSP to a novel Ising-type Hamiltonian, we evaluate the solution quality obtained from the D-Wave 2000Q against the constraint requirements as well as the diversity of solutions. For the test problems explored here, our results indicate that quantum annealing recovers satisfying solutions for NSP and suggests the heuristic method is potentially achievable for practical use. Moreover, we observe that solution quality can be greatly improved through the use of reverse annealing, in which it is possible to refine returned results by using the annealing process a second time. We compare the performance of NSP using both forward and reverse annealing methods and describe how this approach might be used in practice.},
archivePrefix = {arXiv},
arxivId = {1904.12139},
author = {Ikeda, Kazuki and Nakamura, Yuma and Humble, Travis S.},
doi = {10.1038/s41598-019-49172-3},
eprint = {1904.12139},
isbn = {4159801949172},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--10},
title = {{Application of Quantum Annealing to Nurse Scheduling Problem}},
volume = {9},
year = {2019}
}
@techreport{Krizhevskya,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {::},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/}
}
@techreport{Hron2018,
abstract = {Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for varia-tional Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.},
archivePrefix = {arXiv},
arxivId = {1807.01969v1},
author = {Hron, Jiri and De, Alexander G and Matthews, G and Ghahramani, Zoubin},
eprint = {1807.01969v1},
file = {::},
isbn = {1807.01969v1},
title = {{Variational Bayesian dropout: pitfalls and fixes}},
year = {2018}
}
@techreport{Glorot,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
file = {::},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://www.iro.umontreal.}
}
@article{Schulman2015a,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
eprint = {1506.02438},
pages = {1--14},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {http://arxiv.org/abs/1506.02438},
year = {2015}
}
@article{Rosenblatt1958,
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {::},
issn = {1939-1471},
journal = {Psychological Review},
number = {6},
pages = {386--408},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
volume = {65},
year = {1958}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchan...},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
volume = {117},
year = {2013}
}
@article{Boixo2014,
abstract = {Quantum technology is maturing to the point where quantum devices, such as quantum communication systems, quantum random number generators and quantum simulators may be built with capabilities exceeding classical computers. A quantum annealer, in particular, solves optimization problems by evolving a known initial configuration at non-zero temperature towards the ground state of a Hamiltonian encoding a given problem. Here, we present results from tests on a 108 qubit D-Wave One device based on superconducting flux qubits. By studying correlations we find that the device performance is inconsistent with classical annealing or that it is governed by classical spin dynamics. In contrast, we find that the device correlates well with simulated quantum annealing. We find further evidence for quantum annealing in the form of small-gap avoided level crossings characterizing the hard problems. To assess the computational power of the device we compare it against optimized classical algorithms. {\textcopyright} 2014 Macmillan Publishers Limited.},
archivePrefix = {arXiv},
arxivId = {1304.4595},
author = {Boixo, Sergio and R{\o}nnow, Troels F. and Isakov, Sergei V. and Wang, Zhihui and Wecker, David and Lidar, Daniel A. and Martinis, John M. and Troyer, Matthias},
doi = {10.1038/nphys2900},
eprint = {1304.4595},
issn = {17452481},
journal = {Nature Physics},
number = {3},
pages = {218--224},
title = {{Evidence for quantum annealing with more than one hundred qubits}},
volume = {10},
year = {2014}
}
@techreport{Crawford,
abstract = {We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.},
archivePrefix = {arXiv},
arxivId = {1612.05695v3},
author = {Crawford, Daniel and Levit, Anna and Ghadermarzy, Navid and Oberoi, Jaspreet S},
eprint = {1612.05695v3},
file = {::},
title = {{Reinforcement Learning using Quantum Boltzmann Machines}}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
month = {dec},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@phdthesis{Hochreiter1991,
author = {Hochreiter, Sepp},
school = {TU Munich},
title = {{Untersuchungen zu dynamischen neuronalen Netzen}},
type = {Diploma Thesis},
year = {1991}
}
@article{stander1994temperature,
author = {Stander, Julian and Silverman, Bernard W},
journal = {Statistics and Computing},
number = {1},
pages = {21--32},
publisher = {Springer},
title = {{Temperature schedules for simulated annealing}},
volume = {4},
year = {1994}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
month = {jul},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Heim215,
abstract = {Finding a solution to a problem often amounts to an optimization problem and thus can be recast in terms of the lowest-energy state of a system. To find such ground states, mathematical methods based on annealing were developed. To reach the ground state more quickly than with the earlier classical methods, a quantum-mechanical approach was proposed; however, the evidence for quantum speed-up is contradictory. Heim et al. show that the results depend on how the problem is described and how the optimization routine is implemented. This development should be valuable for benchmarking quantum machines.Science, this issue p. 215 Quantum annealers use quantum fluctuations to escape local minima and find low-energy configurations of a physical system. Strong evidence for superiority of quantum annealing (QA) has come from comparing QA implemented through quantum Monte Carlo (QMC) simulations to classical annealing. Motivated by recent experiments, we revisit the question of when quantum speedup may be expected. Although a better scaling is seen for QA in two-dimensional Ising spin glasses, this advantage is due to time discretization artifacts and measurements that are not possible on a physical quantum annealer. Simulations in the physically relevant continuous time limit, on the other hand, do not show superiority. Our results imply that care must be taken when using QMC simulations to assess the potential for quantum speedup.},
author = {Heim, Bettina and R{\o}nnow, Troels F and Isakov, Sergei V and Troyer, Matthias},
doi = {10.1126/science.aaa4170},
issn = {0036-8075},
journal = {Science},
number = {6231},
pages = {215--217},
publisher = {American Association for the Advancement of Science},
title = {{Quantum versus classical annealing of Ising spin glasses}},
url = {https://science.sciencemag.org/content/348/6231/215},
volume = {348},
year = {2015}
}
@article{Srivastava2017,
abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
archivePrefix = {arXiv},
arxivId = {1705.07761},
author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael and Sutton, Charles},
eprint = {1705.07761},
month = {may},
title = {{VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning}},
url = {http://arxiv.org/abs/1705.07761},
year = {2017}
}
@techreport{Srivastava2014a,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{schulman2015trust,
author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
booktitle = {International conference on machine learning},
pages = {1889--1897},
title = {{Trust region policy optimization}},
year = {2015}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network (??net) package. The construction and application of ANN potentials using ??net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite (??-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
volume = {114},
year = {2016}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:Users/kmills/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961 http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Okada2019,
abstract = {The Potts model is a generalization of the Ising model with {\$}Q{\textgreater}2{\$} components. In the fully connected ferromagnetic Potts model, a first-order phase transition is induced by varying thermal fluctuations. Therefore, the computational time required to obtain the ground states by simulated annealing exponentially increases with the system size. This study analytically confirms that the transverse magnetic-field quantum annealing induces a first-order phase transition. This result implies that quantum annealing does not exponentially accelerate the ground-state search of the ferromagnetic Potts model. To avoid the first-order phase transition, we propose an iterative optimization method using a half-hot constraint that is applicable to both quantum and simulated annealing. In the limit of {\$}Q \backslashto \backslashinfty{\$}, a saddle point equation under the half-hot constraint is identical to the equation describing the behavior of the fully connected ferromagnetic Ising model, thus confirming a second-order phase transition. Furthermore, we verify the same relation between the fully connected Potts glass model and the Sherrington--Kirkpatrick model under assumptions of static approximation and replica symmetric solution. The proposed method is expected to obtain low-energy states of the Potts models with high efficiency using Ising-type computers such as the D-Wave quantum annealer and the Fujitsu Digital Annealer.},
archivePrefix = {arXiv},
arxivId = {1904.01522},
author = {Okada, Shuntaro and Ohzeki, Masayuki and Tanaka, Kazuyuki},
eprint = {1904.01522},
month = {apr},
title = {{Efficient quantum and simulated annealing of Potts models using a half-hot constraint}},
url = {http://arxiv.org/abs/1904.01522},
year = {2019}
}
@article{Farimani2017,
abstract = {We have developed a new data-driven paradigm for the rapid inference, modeling and simulation of the physics of transport phenomena by deep learning. Using conditional generative adversarial networks (cGAN), we train models for the direct generation of solutions to steady state heat conduction and incompressible fluid flow purely on observation without knowledge of the underlying governing equations. Rather than using iterative numerical methods to approximate the solution of the constitutive equations, cGANs learn to directly generate the solutions to these phenomena, given arbitrary boundary conditions and domain, with high test accuracy (MAE{\$}{\textless}{\$}1{\$}\backslash{\$}{\%}) and state-of-the-art computational performance. The cGAN framework can be used to learn causal models directly from experimental observations where the underlying physical model is complex or unknown.},
archivePrefix = {arXiv},
arxivId = {1709.02432},
author = {Farimani, Amir Barati and Gomes, Joseph and Pande, Vijay S},
eprint = {1709.02432},
title = {{Deep Learning the Physics of Transport Phenomena}},
url = {http://arxiv.org/abs/1709.02432},
volume = {94305},
year = {2017}
}
