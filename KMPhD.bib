@techreport{Gal2016,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
eprint = {1506.02142},
file = {::},
isbn = {9781510829008},
pages = {1651--1660},
title = {{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}},
url = {http://yarin.co.},
volume = {3},
year = {2016}
}
@article{Liers2005,
author = {Liers, Frauke and J{\"{u}}nger, Michael and Reinelt, Gerhard and Rinaldi, Giovanni},
doi = {10.1002/3527603794.ch4},
isbn = {3527404066},
journal = {New Optimization Algorithms in Physics},
pages = {47--69},
title = {{Computing Exact Ground States of Hard Ising Spin Glass Problems by Branch-and-Cut}},
year = {2005}
}
@article{Kiyohara,
abstract = {Interfaces markedly affect the properties of materials because of differences in their atomic configurations. Determining the atomic structure of the interface is therefore one of the most significant tasks in materials research. However, determining the interface structure usually requires extensive computation. If the interface structure could be efficiently predicted, our understanding of the mechanisms that give rise to the interface properties would be significantly facilitated, and this would pave the way for the design of material interfaces. Using a virtual screening method based on machine learning, we demonstrate a powerful technique to determine interface energies and structures. On the basis of the results obtained by a nonlinear regression using training data from 4 interfaces, structures and energies for 13 other interfaces were predicted. Our method achieved an efficiency that is more than several hundred to several tens of thousand times higher than that of the previously reported methods. Because the present method uses geometrical factors, such as bond length and atomic density, as descriptors for the regression analysis, the method presented here is robust and general and is expected to be beneficial to understanding the nature of any interface.},
archivePrefix = {arXiv},
arxivId = {1512.03502},
author = {Kiyohara, Shin and Oda, Hiromi and Miyata, Tomohiro and Mizoguchi, Teruyasu},
doi = {10.1126/sciadv.1600746},
eprint = {1512.03502},
issn = {23752548},
journal = {Science Advances},
number = {11},
pages = {4--6},
title = {{Prediction of interface structures and energies via virtual screening}},
url = {http://arxiv.org/abs/1512.03502},
volume = {2},
year = {2016}
}
@article{Ratcliff2016b,
abstract = {During the past decades, quantum mechanical methods have undergone an amazing transition from pioneering investigations of experts into a wide range of practical applications, made by a vast community of researchers. First principles calculations of systems containing up to a few hundred atoms have become a standard in many branches of science. The sizes of the systems which can be simulated have increased even further during recent years, and quantum-mechanical calculations of systems up to many thousands of atoms are nowadays possible. This opens up new appealing possibilities, in particular for interdisciplinary work, bridging together communities of different needs and sensibilities. In this review we will present the current status of this topic, and will also give an outlook on the vast multitude of applications, challenges, and opportunities stimulated by electronic structure calculations, making this field an important working tool and bringing together researchers of many different domains. WIREs Comput Mol Sci 2017, 7:e1290. doi: 10.1002/wcms.1290. For further resources related to this article, please visit the WIREs website.},
archivePrefix = {arXiv},
arxivId = {1609.00252},
author = {Ratcliff, Laura E. and Mohr, Stephan and Huhs, Georg and Deutsch, Thierry and Masella, Michel and Genovese, Luigi},
doi = {10.1002/wcms.1290},
eprint = {1609.00252},
issn = {17590884},
journal = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
month = {jan},
number = {1},
pages = {e1290},
title = {{Challenges in large scale quantum mechanical calculations}},
url = {http://doi.wiley.com/10.1002/wcms.1290},
volume = {7},
year = {2017}
}
@article{Curtarolo2003,
abstract = {Predicting and characterizing the crystal structure of materials is a key problem in materials research and development. It is typically addressed with highly accurate quantum mechanical computations on a small set of candidate structures, or with empirical rules that have been extracted from a large amount of experimental information, but have limited predictive power. In this Letter, we transfer the concept of heuristic rule extraction to a large library of ab initio calculated information, and we demonstrate that this can be developed into a tool for crystal structure prediction. {\textcopyright} 2003 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0307262},
author = {Curtarolo, Stefano and Morgan, Dane and Persson, Kristin and Rodgers, John and Ceder, Gerbrand},
doi = {10.1103/PhysRevLett.91.135503},
eprint = {0307262},
isbn = {0031-9007},
issn = {10797114},
journal = {Physical Review Letters},
month = {sep},
number = {13},
pages = {135503},
pmid = {14525315},
primaryClass = {cond-mat},
title = {{Predicting crystal structures with data mining of quantum calculations}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.91.135503},
volume = {91},
year = {2003}
}
@article{Pedregosa2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chakraborty, Apurba and Ghosh, Saptarsi and Mukhopadhyay, Partha and Dinara, Syed Mukulika and Bag, Ankush and Mahata, Mihir K and Kumar, Rahul and Das, Subhashis and Sanjay, Jana and Majumdar, Shubhankar and Biswas, Dhrubes},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
isbn = {9780874216561},
issn = {0717-6163},
journal = {MRS Proceedings},
keywords = {12,2007,3,Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics & numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cornway,Corporate Finance,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,Industrial Organization,J.,JUVENTUD,Lumb,MODIFICACIONES CORPORALES,Male,Masood,Motivation,Movement,Public,R.,Risk-Taking,S.,S.K.,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Skan,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics & numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,advantages,aesthetics,and e-banking,and on cor-,anomaly detection,as none were found,authentication,autoinjury and health,body,business model,candidate,classification,collaboration,competition,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,credit access,credit financing,credit score,credit scoring,critical success factors,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,customer satisfaction,customer scoring,data mining,decision tree,department of economics at,e-,e- banking,e-banking,e-commerce,e-payment,e-trading,electronic communication and computation,emergency,endogenous tie,epidural,esth{\'{e}}tique,est{\'{e}}tica,feature sim-,finance includes e-payment,financial fervices technology,financial services innovation,find any reports of,fintech,fintech analysis,fintech start-ups,functions,genetic programming,global fintech comparison,high resolution images,if neuraxial anes-,in practice,indonesia,information technology,ing with neuraxial anesthesia,internet bank,internet primary bank,jarunee wonglimpiyarat,jeunesse,jibc december 2007,juvenile cultures,juventud,limitations,luation of non-urgent visits,m-commerce,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,multimodal biometric,needle through a,nes corporales,network security,networks,neural networks,no,patents analysis,perforaci{\'{o}}n corporal,piel,professor of marketing,professor of marketing at,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,recommender system,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,smart cards,social network analysis,social networks,social status,spinal,strategic,strategy,support vector machine,sustainable reconstruction,sydney fintech,sydney start-ups,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,the university of pennsylvania,the wharton school of,to a busy urban,traditional banking services,unimodal biometric,university of pennsylvania,vol,was reviewed to see,youth},
month = {jan},
number = {2},
pages = {81--87},
pmid = {15003161},
title = {{Trapping effect analysis of AlGaN/InGaN/GaN Heterostructure by conductance frequency measurement}},
url = {http://www.americanbanker.com/issues/179_124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/15003161%5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991%5Cnhttp://www.scielo},
volume = {XXXIII},
year = {2014}
}
@article{Mills2020a,
abstract = {Reinforcement learning (RL) has become a proven method for optimizing a procedure for which success has been defined, but the specific actions needed to achieve it have not. Using a method we call ‘controlled online optimization learning' (COOL), we apply the so-called ‘black box' method of RL to simulated annealing (SA), demonstrating that an RL agent based on proximal policy optimization can, through experience alone, arrive at a temperature schedule that surpasses the performance of standard heuristic temperature schedules for two classes of Hamiltonians. When the system is initialized at a cool temperature, the RL agent learns to heat the system to ‘melt' it and then slowly cool it in an effort to anneal to the ground state; if the system is initialized at a high temperature, the algorithm immediately cools the system. We investigate the performance of our RL-driven SA agent in generalizing to all Hamiltonians of a specific class. When trained on random Hamiltonians of nearest-neighbour spin glasses, the RL agent is able to control the SA process for other Hamiltonians, reaching the ground state with a higher probability than a simple linear annealing schedule. Furthermore, the scaling performance (with respect to system size) of the RL approach is far more favourable, achieving a performance improvement of almost two orders of magnitude on L = 142 systems. We demonstrate the robustness of the RL approach when the system operates in a ‘destructive observation' mode, an allusion to a quantum system where measurements destroy the state of the system. The success of the RL agent could have far-reaching impacts, from classical optimization, to quantum annealing and to the simulation of physical systems.},
author = {Mills, Kyle and Ronagh, Pooya and Tamblyn, Isaac},
doi = {10.1038/s42256-020-0226-x},
issn = {25225839},
journal = {Nature Machine Intelligence},
month = {sep},
number = {9},
pages = {509--517},
title = {{Finding the ground state of spin Hamiltonians with reinforcement learning}},
url = {http://www.nature.com/articles/s42256-020-0226-x},
volume = {2},
year = {2020}
}
@article{Wei2016,
abstract = {Reaction prediction remains one of the major challenges for organic chemistry and is a prerequisite for efficient synthetic planning. It is desirable to develop algorithms that, like humans, "learn" from being exposed to examples of the application of the rules of organic chemistry. We explore the use of neural networks for predicting reaction types, using a new reaction fingerprinting method. We combine this predictor with SMARTS transformations to build a system which, given a set of reagents and reactants, predicts the likely products. We test this method on problems from a popular organic chemistry textbook.},
archivePrefix = {arXiv},
arxivId = {1608.06296},
author = {Wei, Jennifer N. and Duvenaud, David and Aspuru-Guzik, Al{\'{a}}n},
doi = {10.1021/acscentsci.6b00219},
eprint = {1608.06296},
issn = {23747951},
journal = {ACS Central Science},
month = {oct},
number = {10},
pages = {725--732},
pmid = {27800555},
title = {{Neural networks for the prediction of organic chemistry reactions}},
url = {http://arxiv.org/abs/1608.06296%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27800555%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5084081 http://pubs.acs.org/doi/10.1021/acscentsci.6b00219},
volume = {2},
year = {2016}
}
@article{Saad2012,
abstract = {Machine learning is a broad discipline that comprises a variety of techniques for extracting meaningful information and patterns from data. It draws on knowledge and "know-how" from various scientific areas such as statistics, graph theory, linear algebra, databases, mathematics, and computer science. Recently, materials scientists have begun to explore data mining ideas for discovery in materials. In this paper we explore the power of these methods for studying binary compounds that are well characterized and are often used as a test bed. By mining properties of the constituent atoms, three materials research relevant tasks, namely, separation of a number of compounds into subsets in terms of their crystal structure, grouping of an unknown compound into the most characteristically similar peers (in one instance, 100% accuracy is achieved), and specific property prediction (the melting point), are explored. {\textcopyright} 2012 American Physical Society.},
author = {Saad, Yousef and Gao, Da and Ngo, Thanh and Bobbitt, Scotty and Chelikowsky, James R. and Andreoni, Wanda},
doi = {10.1103/PhysRevB.85.104104},
isbn = {1098-0121},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
month = {mar},
number = {10},
pages = {104104},
title = {{Data mining for materials: Computational experiments with AB compounds}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.85.104104},
volume = {85},
year = {2012}
}
@article{Allinger1977,
abstract = {An improved force field for molecular mechanics calculations of the structures and energies of hydrocarbons is presented. The problem of simultaneously obtaining a sufficiently large gauche butane interaction energy while keeping the hydrogens small enough for good structural predictions was solved with the aid of onefold and twofold rotational barriers. The structural results are competitive with the best of currently available force fields, while the energy calculations are superior to any previously reported. For a list of 42 selected diverse types of hydrocarbons, the standard deviation between the calculated and experimental heats of formation is 0.42 kcal/mol, compared with an average reported experimental error for the same group of compounds of 0.40 kcal/mol. {\textcopyright} 1977, American Chemical Society. All rights reserved.},
author = {Allinger, Norman L.},
doi = {10.1021/ja00467a001},
isbn = {0002-7863},
issn = {15205126},
journal = {Journal of the American Chemical Society},
number = {25},
pages = {8127--8134},
title = {{Conformational Analysis. 130. MM2. A Hydrocarbon Force Field Utilizing V1and V2Torsional Terms1,2}},
url = {http://pubs.acs.org/doi/abs/10.1021/ja00467a001},
volume = {99},
year = {1977}
}
@article{Manzhos2006,
abstract = {We combine the high dimensional model representation (HDMR) idea of Rabitz and co-workers [J. Phys. Chem. 110, 2474 (2006)] with neural network (NN) fits to obtain an effective means of building multidimensional potentials. We verify that it is possible to determine an accurate many-dimensional potential by doing low dimensional fits. The final potential is a sum of terms each of which depends on a subset of the coordinates. This form facilitates quantum dynamics calculations. We use NNs to represent HDMR component functions that minimize error mode term by mode term. This NN procedure makes it possible to construct high-order component functions which in turn enable us to determine a good potential. It is shown that the number of available potential points determines the order of the HDMR which should be used. {\textcopyright} 2006 American Institute of Physics.},
author = {Manzhos, Sergei and Carrington, Tucker},
doi = {10.1063/1.2336223},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {8},
pages = {084109},
pmid = {16965003},
title = {{A random-sampling high dimensional model representation neural network for building potential energy surfaces}},
url = {http://scitation.aip.org/content/aip/journal/jcp/125/8/10.1063/1.2336223},
volume = {125},
year = {2006}
}
@article{Broecker2016,
abstract = {State-of-the-art machine learning techniques promise to become a powerful tool in statistical mechanics via their capacity to distinguish different phases of matter in an automated way. Here we demonstrate that convolutional neural networks (CNN) can be optimized for quantum many-fermion systems such that they correctly identify and locate quantum phase transitions in such systems. Using auxiliary-field quantum Monte Carlo (QMC) simulations to sample the many-fermion system, we show that the Green's function holds sufficient information to allow for the distinction of different fermionic phases via a CNN. We demonstrate that this QMC + machine learning approach works even for systems exhibiting a severe fermion sign problem where conventional approaches to extract information from the Green's function, e.g. in the form of equal-time correlation functions, fail.},
archivePrefix = {arXiv},
arxivId = {1608.07848},
author = {Broecker, Peter and Carrasquilla, Juan and Melko, Roger G. and Trebst, Simon},
doi = {10.1038/s41598-017-09098-0},
eprint = {1608.07848},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {8},
pmid = {28821785},
title = {{Machine learning quantum phases of matter beyond the fermion sign problem}},
url = {http://arxiv.org/abs/1608.07848},
volume = {7},
year = {2017}
}
@article{Lin2016,
abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through “cheap learning” with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various “no-flattening theorems” showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss; for example, we show that n variables cannot be multiplied using fewer than 2 n neurons in a single hidden layer.},
archivePrefix = {arXiv},
arxivId = {1608.08225},
author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
doi = {10.1007/s10955-017-1836-5},
eprint = {1608.08225},
file = {:home/kmills/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Tegmark, Rolnick - 2016 - Why does deep and cheap learning work so well.pdf:pdf},
isbn = {9781627480031},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Artificial neural networks,Deep learning,Statistical physics},
month = {aug},
number = {6},
pages = {1223--1247},
pmid = {25246403},
title = {{Why Does Deep and Cheap Learning Work So Well?}},
url = {http://arxiv.org/abs/1608.08225 http://ieeexplore.ieee.org/document/726791/},
volume = {168},
year = {2017}
}
@article{Inagaki2016,
abstract = {The analysis and optimization of complex systems can be reduced to mathematical problems collectively known as combinatorial optimization. Many such problems can be mapped onto ground-state search problems of the Ising model, and various artificial spin systems are now emerging as promising approaches. However, physical Ising machines have suffered from limited numbers of spin-spin couplings because of implementations based on localized spins, resulting in severe scalability problems. We report a 2000-spin network with all-to-all spin-spin couplings. Using a measurement and feedback scheme, we coupled time-multiplexed degenerate optical parametric oscillators to implement maximum cut problems on arbitrary graph topologies with up to 2000 nodes. Our coherent Ising machine outperformed simulated annealing in terms of accuracy and computation time for a 2000-node complete graph.},
author = {Inagaki, Takahiro and Haribara, Yoshitaka and Igarashi, Koji and Sonobe, Tomohiro and Tamate, Shuhei and Honjo, Toshimori and Marandi, Alireza and McMahon, Peter L. and Umeki, Takeshi and Enbutsu, Koji and Tadanaga, Osamu and Takenouchi, Hirokazu and Aihara, Kazuyuki and Kawarabayashi, Ken Ichi and Inoue, Kyo and Utsunomiya, Shoko and Takesue, Hiroki},
doi = {10.1126/science.aah4243},
issn = {10959203},
journal = {Science},
number = {6312},
pages = {603--606},
title = {{A coherent Ising machine for 2000-node optimization problems}},
volume = {354},
year = {2016}
}
@article{Zhu,
abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.},
archivePrefix = {arXiv},
arxivId = {1709.01907},
author = {Zhu, Lingxue and Laptev, Nikolay},
doi = {10.1109/ICDMW.2017.19},
eprint = {1709.01907},
file = {::},
isbn = {9781538614808},
issn = {23759259},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Anomaly detection,Bayesian neural networks,Predictive uncertainty,Time series},
month = {sep},
pages = {103--110},
title = {{Deep and Confident Prediction for Time Series at Uber}},
url = {http://arxiv.org/abs/1709.01907 http://dx.doi.org/10.1109/ICDMW.2017.19},
volume = {2017-Novem},
year = {2017}
}
@article{Carr2016,
abstract = {Controlling molecule-surface interactions is key for chemical applications ranging from catalysis to gas sensing. We present a framework for accelerating the search for the global minimum on potential surfaces, corresponding to stable adsorbate-surface structures. We present a technique using Bayesian inference that enables us to predict converged density functional theory potential energies with fewer self-consistent field iterations. We then discuss how this technique fits in with the Bayesian Active Site Calculator, which applies Bayesian optimization to the problem. We demonstrate the performance of our framework using a hematite (Fe2O3) surface and present the adsorption sites found by our global optimization method for various simple hydrocarbons on the rutile TiO2 (110) surface.},
author = {Carr, S. F. and Garnett, R. and Lo, C. S.},
doi = {10.1063/1.4964671},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {15},
pages = {154106},
pmid = {27782478},
title = {{Accelerating the search for global minima on potential energy surfaces using machine learning}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/15/10.1063/1.4964671},
volume = {145},
year = {2016}
}
@article{Mnih2016,
abstract = {Wc propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Lehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P. and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2850--2869},
title = {{Asynchronous methods for deep reinforcement learning}},
volume = {4},
year = {2016}
}
@article{Bowles2018,
abstract = {One of the biggest issues facing the use of machine learning in medical imaging is the lack of availability of large, labelled datasets. The annotation of medical images is not only expensive and time consuming but also highly dependent on the availability of expert observers. The limited amount of training data can inhibit the performance of supervised machine learning algorithms which often need very large quantities of data on which to train to avoid overfitting. So far, much effort has been directed at extracting as much information as possible from what data is available. Generative Adversarial Networks (GANs) offer a novel way to unlock additional information from a dataset by generating synthetic samples with the appearance of real images. This paper demonstrates the feasibility of introducing GAN derived synthetic data to the training datasets in two brain segmentation tasks, leading to improvements in Dice Similarity Coefficient (DSC) of between 1 and 5 percentage points under different conditions, with the strongest effects seen fewer than ten training image stacks are available.},
archivePrefix = {arXiv},
arxivId = {1810.10863},
author = {Bowles, Christopher and Chen, Liang and Guerrero, Ricardo and Bentley, Paul and Gunn, Roger and Hammers, Alexander and Dickie, David Alexander and Hern{\'{a}}ndez, Maria Vald{\'{e}}s and Wardlaw, Joanna and Rueckert, Daniel},
eprint = {1810.10863},
file = {::},
issn = {23318422},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{GAN augmentation: Augmenting training data using generative adversarial networks}},
url = {http://arxiv.org/abs/1810.10863},
year = {2018}
}
@article{Battaglia2005,
abstract = {The path integral Monte Carlo simulated quantum annealing algorithm is applied to the optimization of a large hard instance of the random satisfiability problem (N=10000). The dynamical behavior of the quantum and the classical annealing are compared, showing important qualitative differences in the way of exploring the complex energy landscape of the combinatorial optimization problem. At variance with the results obtained for the Ising spin glass and for the traveling salesman problem, in the present case the linear-schedule quantum annealing performance is definitely worse than classical annealing. Nevertheless, a quantum cooling protocol based on field-cycling and able to outperform standard classical simulated annealing over short time scales is introduced. {\textcopyright} 2005 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0502468},
author = {Battaglia, Demian A. and Santoro, Giuseppe E. and Tosatti, Erio},
doi = {10.1103/PhysRevE.71.066707},
eprint = {0502468},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {6},
pages = {1--10},
primaryClass = {cond-mat},
title = {{Optimization by quantum annealing: Lessons from hard satisfiability problems}},
volume = {71},
year = {2005}
}
@article{Farhi2001,
abstract = {A quantum system will stay near its instantaneous ground state if the Hamiltonian that governs its evolution varies slowly enough. This quantum adiabatic behavior is the basis of a new class of algorithms for quantum computing. We tested one such algorithm by applying it to randomly generated hard instances of an NP-complete problem. For the small examples that we could simulate, the quantum adiabatic algorithm worked well, providing evidence that quantum computers (if large ones can be built) may be able to outperform ordinary computers on hard sets of instances of NP-complete problems.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0104129},
author = {Farhi, E. and Goldstone, J. and Gutmann, S. and Lapan, J. and Lundgren, A. and Preda, D.},
doi = {10.1126/science.1057726},
eprint = {0104129},
issn = {00368075},
journal = {Science},
number = {5516},
pages = {472--476},
pmid = {11313487},
primaryClass = {quant-ph},
title = {{A quantum adiabatic evolution algorithm applied to random instances of an NP-complete problem}},
volume = {292},
year = {2001}
}
@article{VanMilligen1995,
abstract = {A new generally applicable method to solve differential equations, based on neural networks, is proposed. Straightforward to implement, finite differences and coordinate transformations are not used. The neural network provides a flexible and compact base for representing the solution, found through the global minimization of an error functional. As a proof of principle, a two-dimensional ideal magnetohydrodynamic plasma equilibrium is solved. Since no particular topology is assumed, the technique is especially promising for the three-dimensional plasma equilibrium problem. {\textcopyright} 1995 The American Physical Society.},
author = {{Van Milligen}, B. Ph and Tribaldos, V. and Jim{\'{e}}nez, J. A.},
doi = {10.1103/PhysRevLett.75.3594},
issn = {00319007},
journal = {Physical Review Letters},
month = {nov},
number = {20},
pages = {3594--3597},
pmid = {10059679},
title = {{Neural network differential equation and plasma equilibrium solver}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.75.3594},
volume = {75},
year = {1995}
}
@article{Montavon2013,
abstract = {The combination of modern scientific computing with electronic structure theory can lead to an unprecedented amount of data amenable to intelligent data analysis for the identification of meaningful, novel and predictive structure-property relationships. Such relationships enable high-throughput screening for relevant properties in an exponentially growing pool of virtual compounds that are synthetically accessible. Here, we present a machine learning model, trained on a database of ab initio calculation results for thousands of organic molecules, that simultaneously predicts multiple electronic ground- and excited-state properties. The properties include atomization energy, polarizability, frontier orbital eigenvalues, ionization potential, electron affinity and excitation energies. The machine learning model is based on a deep multi-task artificial neural network, exploiting the underlying correlations between various molecular properties. The input is identical to ab initio methods, i.e. nuclear charges and Cartesian coordinates of all atoms. For small organic molecules, the accuracy of such a 'quantum machine' is similar, and sometimes superior, to modern quantum-chemical methods - at negligible computational cost. {\textcopyright} IOP Publishing and Deutsche Physikalische Gesellschaft.},
archivePrefix = {arXiv},
arxivId = {1305.7074},
author = {Montavon, Gr{\'{e}}goire and Rupp, Matthias and Gobre, Vivekanand and Vazquez-Mayagoitia, Alvaro and Hansen, Katja and Tkatchenko, Alexandre and M{\"{u}}ller, Klaus Robert and {Anatole Von Lilienfeld}, O.},
doi = {10.1088/1367-2630/15/9/095003},
eprint = {1305.7074},
issn = {13672630},
journal = {New Journal of Physics},
month = {sep},
number = {9},
pages = {095003},
title = {{Machine learning of molecular electronic properties in chemical compound space}},
url = {http://stacks.iop.org/1367-2630/15/i=9/a=095003?key=crossref.c7515a05af17cccdbeec1c83340d4405},
volume = {15},
year = {2013}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961 http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Srivastava2014,
abstract = {We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation X of the sum of an (approximately) low rank matrix with a second matrix endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including factor analysis, multi-task regression and robust covariance estimation. We derive a general theorem that bounds the Frobenius norm error for an estimate of the pair obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results use a "spikiness" condition that is related to, but milder than, singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields nonasymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices that can be exactly or approximately low rank, and matrices that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error. The sharpness of our nonasymptotic predictions is confirmed by numerical simulations. {\textcopyright} Institute of Mathematical Statistics, 2012.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Agarwal, Alekh and Negahban, Sahand and Wainwright, Martin J.},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Composite regularizers,High-dimensional inference,Nuclear norm},
number = {2},
pages = {1171--1197},
pmid = {23285570},
title = {{Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions}},
volume = {40},
year = {2012}
}
@article{Behler2008,
abstract = {We present a combination of the metadynamics method for the investigation of pressure-induced phase transitions in solids with a neural network representation of high-dimensional density-functional theory (DFT) potential-energy surfaces. In a recent illustration of the method for the complex high-pressure phase diagram of silicon [Behler et al., Phys. Rev. Lett., 100, 185501 (2008)] we have shown that the full sequence of phases can be reconstructed by a series of subsequent simulations. In the present paper we give a detailed account of the underlying methodology and discuss the scope and limitations of the approach, which promises to be a valuable tool for the investigation of a variety of inorganic materials. The method is several orders of magnitude faster than a direct coupling of metadynamics with electronic structure calculations, while the accuracy is essentially maintained, thus providing access to extended simulations of large systems. {\textcopyright} 2008 WILEY-VCH Verlag GmbH & Co. KGaA.},
author = {Behler, J{\"{o}}rg and Martoň{\'{a}}k, Roman and Donadio, Davide and Parrinello, Michele},
doi = {10.1002/pssb.200844219},
isbn = {0370-1972},
issn = {03701972},
journal = {Physica Status Solidi (B) Basic Research},
month = {dec},
number = {12},
pages = {2618--2629},
title = {{Pressure-induced phase transitions in silicon studied by neural network-based metadynamics simulations}},
url = {http://doi.wiley.com/10.1002/pssb.200844219},
volume = {245},
year = {2008}
}
@article{Faber2017,
abstract = {We investigate the impact of choosing regressors and molecular representations for the construction of fast machine learning (ML) models of 13 electronic ground-state properties of organic molecules. The performance of each regressor/representation/property combination is assessed using learning curves which report out-of-sample errors as a function of training set size with up to {\^{a}}118k distinct molecules. Molecular structures and properties at the hybrid density functional theory (DFT) level of theory come from the QM9 database [ Ramakrishnan et al. Sci. Data 2014, 1, 140022 ] and include enthalpies and free energies of atomization, HOMO/LUMO energies and gap, dipole moment, polarizability, zero point vibrational energy, heat capacity, and the highest fundamental vibrational frequency. Various molecular representations have been studied (Coulomb matrix, bag of bonds, BAML and ECFP4, molecular graphs (MG)), as well as newly developed distribution based variants including histograms of distances (HD), angles (HDA/MARAD), and dihedrals (HDAD). Regressors include linear models (Bayesian ridge regression (BR) and linear regression with elastic net regularization (EN)), random forest (RF), kernel ridge regression (KRR), and two types of neural networks, graph convolutions (GC) and gated graph networks (GG). Out-of sample errors are strongly dependent on the choice of representation and regressor and molecular property. Electronic properties are typically best accounted for by MG and GC, while energetic properties are better described by HDAD and KRR. The specific combinations with the lowest out-of-sample errors in the {\^{a}}118k training set size limit are (free) energies and enthalpies of atomization (HDAD/KRR), HOMO/LUMO eigenvalue and gap (MG/GC), dipole moment (MG/GC), static polarizability (MG/GG), zero point vibrational energy (HDAD/KRR), heat capacity at room temperature (HDAD/KRR), and highest fundamental vibrational frequency (BAML/RF). We present numerical evidence that ML model predictions deviate from DFT (B3LYP) less than DFT (B3LYP) deviates from experiment for all properties. Furthermore, out-of-sample prediction errors with respect to hybrid DFT reference are on par with, or close to, chemical accuracy. The results suggest that ML models could be more accurate than hybrid DFT if explicitly electron correlated quantum (or experimental) data were available.},
archivePrefix = {arXiv},
arxivId = {1702.05532},
author = {Faber, Felix A. and Hutchison, Luke and Huang, Bing and Gilmer, Justin and Schoenholz, Samuel S. and Dahl, George E. and Vinyals, Oriol and Kearnes, Steven and Riley, Patrick F. and {Von Lilienfeld}, O. Anatole},
doi = {10.1021/acs.jctc.7b00577},
eprint = {1702.05532},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
month = {feb},
number = {11},
pages = {5255--5264},
pmid = {28926232},
title = {{Prediction Errors of Molecular Machine Learning Models Lower than Hybrid DFT Error}},
url = {http://arxiv.org/abs/1702.05532},
volume = {13},
year = {2017}
}
@article{Ulissi2016,
abstract = {Surface phase diagrams are necessary for understanding surface chemistry in electrochemical catalysis, where a range of adsorbates and coverages exist at varying applied potentials. These diagrams are typically constructed using intuition, which risks missing complex coverages and configurations at potentials of interest. More accurate cluster expansion methods are often difficult to implement quickly for new surfaces. We adopt a machine learning approach to rectify both issues. Using a Gaussian process regression model, the free energy of all possible adsorbate coverages for surfaces is predicted for a finite number of adsorption sites. Our result demonstrates a rational, simple, and systematic approach for generating accurate free-energy diagrams with reduced computational resources. The Pourbaix diagram for the IrO2(110) surface (with nine coverages from fully hydrogenated to fully oxygenated surfaces) is reconstructed using just 20 electronic structure relaxations, compared to approximately 90 using typical search methods. Similar efficiency is demonstrated for the MoS2 surface.},
author = {Ulissi, Zachary W. and Singh, Aayush R. and Tsai, Charlie and N{\o}rskov, Jens K.},
doi = {10.1021/acs.jpclett.6b01254},
issn = {19487185},
journal = {Journal of Physical Chemistry Letters},
number = {19},
pages = {3931--3935},
title = {{Automated Discovery and Construction of Surface Phase Diagrams Using Machine Learning}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jpclett.6b01254},
volume = {7},
year = {2016}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
file = {::},
issn = {0033295X},
journal = {Psychological Review},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATI},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
volume = {65},
year = {1958}
}
@techreport{Bansal,
abstract = {Reinforcement Learning is divided in two main paradigms: model-free and model-based. Each of these two paradigms has strengths and limitations, and has been successfully applied to real world domains that are appropriate to its corresponding strengths. In this paper, we present a new approach aimed at bridging the gap between these two paradigms that is at the same time data-efficient and cost-savvy. We do so by learning a probabilistic dynamics model and leveraging it as a prior for the intertwined model-free optimization. As a result, our approach can exploit the generality and structure of the dynamics model, but is also capable of ignoring its inevitable inaccuracies, by directly incorporating the evidence provided by the direct observation of the cost. Preliminary results demonstrate that our approach outperforms purely model-based and model-free approaches, as well as the approach of simply switching from a model-based to a model-free setting.},
archivePrefix = {arXiv},
arxivId = {1709.03153},
author = {Bansal, Somil and Calandra, Roberto and Chua, Kurtland and Levine, Sergey and Tomlin, Claire},
booktitle = {arXiv},
eprint = {1709.03153},
file = {::},
issn = {23318422},
title = {{MBMF: Model-based priors for model-free reinforcement learning}},
year = {2017}
}
@article{Harris2010,
abstract = {A rf-superconducting quantum interference device (SQUID) flux qubit that is robust against fabrication variations in Josephson-junction critical currents and device inductance has been implemented. Measurements of the persistent current and of the tunneling energy between the two lowest-lying states, both in the coherent and incoherent regimes, are presented. These experimental results are shown to be in agreement with predictions of a quantum-mechanical Hamiltonian whose parameters were independently calibrated, thus justifying the identification of this device as a flux qubit. In addition, measurements of the flux and critical current noise spectral densities are presented that indicate that these devices with Nb wiring are comparable to the best Al wiring rf SQUIDs reported in the literature thus far, with a 1/f flux noise spectral density at 1 Hz of 1.3 -0.5 +0.7 $\mu$ $\Phi$0 /√ Hz. An explicit formula for converting the observed flux noise spectral density into a free-induction-decay time for a flux qubit biased to its optimal point and operated in the energy eigenbasis is presented. {\textcopyright} 2010 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {0909.4321},
author = {Harris, R. and Johansson, J. and Berkley, A. J. and Johnson, M. W. and Lanting, T. and Han, Siyuan and Bunyk, P. and Ladizinsky, E. and Oh, T. and Perminov, I. and Tolkacheva, E. and Uchaikin, S. and Chapple, E. M. and Enderud, C. and Rich, C. and Thom, M. and Wang, J. and Wilson, B. and Rose, G.},
doi = {10.1103/PhysRevB.81.134510},
eprint = {0909.4321},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {13},
pages = {1--19},
title = {{Experimental demonstration of a robust and scalable flux qubit}},
volume = {81},
year = {2010}
}
@article{Mills2017b,
abstract = {We have trained a deep (convolutional) neural network to predict the ground-state energy of an electron in four classes of confining two-dimensional electrostatic potentials. On randomly generated potentials, for which there is no analytic form for either the potential or the ground-state energy, the model was able to predict the ground-state energy to within chemical accuracy, with a median absolute error of 1.49 mHa. We also investigated the performance of the model in predicting other quantities such as the kinetic energy and the first excited-state energy.},
archivePrefix = {arXiv},
arxivId = {1702.01361},
author = {Mills, Kyle and Spanner, Michael and Tamblyn, Isaac},
doi = {10.1103/PhysRevA.96.042113},
eprint = {1702.01361},
issn = {24699934},
journal = {Physical Review A},
month = {oct},
number = {4},
pages = {042113},
title = {{Deep learning and the Schr{\"{o}}dinger equation}},
url = {https://link.aps.org/doi/10.1103/PhysRevA.96.042113},
volume = {96},
year = {2017}
}
@article{Sandfort2019,
abstract = {Labeled medical imaging data is scarce and expensive to generate. To achieve generalizable deep learning models large amounts of data are needed. Standard data augmentation is a method to increase generalizability and is routinely performed. Generative adversarial networks offer a novel method for data augmentation. We evaluate the use of CycleGAN for data augmentation in CT segmentation tasks. Using a large image database we trained a CycleGAN to transform contrast CT images into non-contrast images. We then used the trained CycleGAN to augment our training using these synthetic non-contrast images. We compared the segmentation performance of a U-Net trained on the original dataset compared to a U-Net trained on the combined dataset of original data and synthetic non-contrast images. We further evaluated the U-Net segmentation performance on two separate datasets: The original contrast CT dataset on which segmentations were created and a second dataset from a different hospital containing only non-contrast CTs. We refer to these 2 separate datasets as the in-distribution and out-of-distribution datasets, respectively. We show that in several CT segmentation tasks performance is improved significantly, especially in out-of-distribution (noncontrast CT) data. For example, when training the model with standard augmentation techniques, performance of segmentation of the kidneys on out-of-distribution non-contrast images was dramatically lower than for in-distribution data (Dice score of 0.09 vs. 0.94 for out-of-distribution vs. in-distribution data, respectively, p < 0.001). When the kidney model was trained with CycleGAN augmentation techniques, the out-of-distribution (non-contrast) performance increased dramatically (from a Dice score of 0.09 to 0.66, p < 0.001). Improvements for the liver and spleen were smaller, from 0.86 to 0.89 and 0.65 to 0.69, respectively. We believe this method will be valuable to medical imaging researchers to reduce manual segmentation effort and cost in CT imaging.},
author = {Sandfort, Veit and Yan, Ke and Pickhardt, Perry J. and Summers, Ronald M.},
doi = {10.1038/s41598-019-52737-x},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {16884},
pmid = {31729403},
title = {{Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks}},
url = {https://doi.org/10.1038/s41598-019-52737-x},
volume = {9},
year = {2019}
}
@article{Haarnoja2018,
abstract = {Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website3.},
archivePrefix = {arXiv},
arxivId = {1812.11103},
author = {Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
doi = {10.15607/rss.2019.xv.011},
eprint = {1812.11103},
file = {::},
issn = {23318422},
journal = {arXiv},
month = {dec},
publisher = {arXiv},
title = {{Learning to walk via deep reinforcement learning}},
url = {http://arxiv.org/abs/1812.11103},
year = {2018}
}
@article{Huang2016a,
abstract = {In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.},
archivePrefix = {arXiv},
arxivId = {1612.04357},
author = {Huang, Xun and Li, Yixuan and Poursaeed, Omid and Hopcroft, John and Belongie, Serge},
doi = {10.1109/CVPR.2017.202},
eprint = {1612.04357},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
month = {dec},
pages = {1866--1875},
pmid = {202927},
title = {{Stacked generative adversarial networks}},
url = {http://arxiv.org/abs/1612.04357},
volume = {2017-Janua},
year = {2017}
}
@article{Snyder2013,
abstract = {Using a one-dimensional model, we explore the ability of machine learning to approximate the non-interacting kinetic energy density functional of diatomics. This nonlinear interpolation between Kohn-Sham reference calculations can (i) accurately dissociate a diatomic, (ii) be systematically improved with increased reference data and (iii) generate accurate self-consistent densities via a projection method that avoids directions with no data. With relatively few densities, the error due to the interpolation is smaller than typical errors in standard exchange-correlation functionals. {\textcopyright} 2013 AIP Publishing LLC.},
archivePrefix = {arXiv},
arxivId = {1306.1812},
author = {Snyder, John C. and Rupp, Matthias and Hansen, Katja and Blooston, Leo and M{\"{u}}ller, Klaus Robert and Burke, Kieron},
doi = {10.1063/1.4834075},
eprint = {1306.1812},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {22},
pmid = {24329053},
title = {{Orbital-free bond breaking via machine learning}},
volume = {139},
year = {2013}
}
@article{Ray1989,
abstract = {The Sherrington-Kirkpatrick model under a transverse field is studied here employing the Suzuki-Trotter formula to map the model to an equivalent classical one. The effective Thouless-Anderson-Palmer free energy is used to study the stability of the system, and Monte Carlo computer simulations of the effective classical model are performed to obtain the phase diagram and the magnetization overlap distribution. Our results indicate a trivial overlap distribution due to quantum fluctuations. The phase diagram shows a slight initial increase in the glass transition temperature Tg as the transverse field is switched on, confirming that obtained by Yokota. {\textcopyright} 1989 The American Physical Society.},
author = {Ray, P. and Chakrabarti, B. K. and Chakrabarti, Arunava},
doi = {10.1103/PhysRevB.39.11828},
issn = {01631829},
journal = {Physical Review B},
number = {16},
pages = {11828--11832},
title = {{Sherrington-Kirkpatrick model in a transverse field: Absence of replica symmetry breaking due to quantum fluctuations}},
volume = {39},
year = {1989}
}
@article{Schulman2015a,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD($\lambda$). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
eprint = {1506.02438},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
pages = {1--14},
title = {{High-dimensional continuous control using generalized advantage estimation}},
url = {http://arxiv.org/abs/1506.02438},
year = {2016}
}
@article{Wallach2015,
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages. {\textcopyright} 2010 The Author(s).},
archivePrefix = {arXiv},
arxivId = {1510.02855},
author = {Silla, Carlos N. and Freitas, Alex A.},
doi = {10.1007/s10618-010-0175-9},
eprint = {1510.02855},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {DAG-structured class hierarchies,Hierarchical classification,Tree-structured class hierarchies},
number = {1-2},
pages = {31--72},
pmid = {19477997},
title = {{A survey of hierarchical classification across different application domains}},
volume = {22},
year = {2011}
}
@article{Hastings1970,
abstract = {SUMMARY: A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed. {\textcopyright} 1970 Oxford University Press.},
author = {Hastings, W. K.},
doi = {10.1093/biomet/57.1.97},
issn = {00063444},
journal = {Biometrika},
number = {1},
pages = {97--109},
title = {{Monte carlo sampling methods using Markov chains and their applications}},
volume = {57},
year = {1970}
}
@article{Funahashi1989,
abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations. {\textcopyright} 1989.},
author = {Funahashi, Ken Ichi},
doi = {10.1016/0893-6080(89)90003-8},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Continuous mapping,Hidden layer,Neural network,Output function,Realization,Sigmoid function,Unit},
number = {3},
pages = {183--192},
title = {{On the approximate realization of continuous mappings by neural networks}},
volume = {2},
year = {1989}
}
@article{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1606.03657},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {2180--2188},
title = {{InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets}},
year = {2016}
}
@article{Lundgaard2016,
abstract = {We propose a general-purpose semilocal/nonlocal exchange-correlation functional approximation, named mBEEF-vdW. The exchange is a meta generalized gradient approximation, and the correlation is a semilocal and nonlocal mixture, with the Rutgers-Chalmers approximation for van der Waals (vdW) forces. The functional is fitted within the Bayesian error estimation functional (BEEF) framework [J. Wellendorff, Phys. Rev. B 85, 235149 (2012)PRBMDO1098-012110.1103/PhysRevB.85.235149; J. Wellendorff, J. Chem. Phys. 140, 144107 (2014)JCPSA60021-960610.1063/1.4870397]. We improve the previously used fitting procedures by introducing a robust MM-estimator based loss function, reducing the sensitivity to outliers in the datasets. To more reliably determine the optimal model complexity, we furthermore introduce a generalization of the bootstrap 0.632 estimator with hierarchical bootstrap sampling and geometric mean estimator over the training datasets. Using this estimator, we show that the robust loss function leads to a 10% improvement in the estimated prediction error over the previously used least-squares loss function. The mBEEF-vdW functional is benchmarked against popular density functional approximations over a wide range of datasets relevant for heterogeneous catalysis, including datasets that were not used for its training. Overall, we find that mBEEF-vdW has a higher general accuracy than competing popular functionals, and it is one of the best performing functionals on chemisorption systems, surface energies, lattice constants, and dispersion. We also show the potential-energy curve of graphene on the nickel(111) surface, where mBEEF-vdW matches the experimental binding length. mBEEF-vdW is currently available in gpaw and other density functional theory codes through Libxc, version 3.0.0.},
author = {Lundgaard, Keld T. and Wellendorff, Jess and Voss, Johannes and Jacobsen, Karsten W. and Bligaard, Thomas},
doi = {10.1103/PhysRevB.93.235162},
isbn = {2469-9950},
issn = {24699969},
journal = {Physical Review B},
number = {23},
pages = {1--16},
title = {{MBEEF-vdW: Robust fitting of error estimation density functionals}},
volume = {93},
year = {2016}
}
@article{Wetzel2017,
abstract = {Classifying phases of matter is key to our understanding of many problems in physics. For quantum-mechanical systems in particular, the task can be daunting due to the exponentially large Hilbert space. With modern computing power and access to ever-larger data sets, classification problems are now routinely solved using machine-learning techniques. Here, we propose a neural-network approach to finding phase transitions, based on the performance of a neural network after it is trained with data that are deliberately labelled incorrectly. We demonstrate the success of this method on the topological phase transition in the Kitaev chain, the thermal phase transition in the classical Ising model, and the many-body-localization transition in a disordered quantum spin chain. Our method does not depend on order parameters, knowledge of the topological content of the phases, or any other specifics of the transition at hand. It therefore paves the way to the development of a generic tool for identifying unexplored phase transitions.},
archivePrefix = {arXiv},
arxivId = {1610.02048},
author = {{Van Nieuwenburg}, Evert P.L. and Liu, Ye Hua and Huber, Sebastian D.},
doi = {10.1038/nphys4037},
eprint = {1610.02048},
issn = {17452481},
journal = {Nature Physics},
number = {5},
pages = {435--439},
title = {{Learning phase transitions by confusion}},
url = {http://arxiv.org/abs/1703.02435},
volume = {13},
year = {2017}
}
@article{Hernandez-Lobato2015,
abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
archivePrefix = {arXiv},
arxivId = {1502.05336},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Adams, Ryan P.},
eprint = {1502.05336},
file = {::},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
month = {feb},
pages = {1861--1869},
publisher = {International Machine Learning Society (IMLS)},
title = {{Probabilistic backpropagation for scalable learning of Bayesian neural networks}},
url = {http://arxiv.org/abs/1502.05336},
volume = {3},
year = {2015}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in large data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many-body systems. Starting with raw spin configurations of a prototypical Ising model, we use principal component analysis to extract relevant low-dimensional representations of the original data and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds physical concepts such as the order parameter and structure factor to be indicators of a phase transition. We discuss the future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
issn = {24699969},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@article{Ferdinand1969,
abstract = {The critical-point anomaly of a plane square m×n Ising lattice with periodic boundary conditions (a torus) is analyzed asymptotically in the limit n→ with $\xi$=mn fixed. Among other results, it is shown that for fixed $\tau$=n(T-Tc)Tc, the specific heat per spin of a large lattice is given by Cmn(T)kBmn=A0lnn+B($\tau$,$\xi$)+B1($\tau$)(lnn)n+B2($\tau$,$\xi$)n+O[(lnn)3n2], where explicit expressions can be given for A0 and for the functions B, B1, and B2. It follows that the specific-heat peak of the finite lattice is rounded on a scale $\delta$=$\Delta$TTc∼1n, while the maximum in Cmn(T) is displaced from Tc by $\epsilon$=(Tc-Tmax)Tc∼1n. For $\xi$0>$\xi$>$\xi$0-1, where $\xi$0=3.13927, the maximum lies above Tc; but for $\xi$>$\xi$0 or $\xi$<$\xi$0-1, the maximum is depressed below Tc; when $\xi$=,$\xi$0,or$\xi$0-1, the relative shift in the maximum from Tc is only of order (lnn)n2. Detailed graphs and numerical data are presented, and the results are compared with some for lattices with free edges. Some heuristic arguments are developed which indicate the possible nature of finite-size critical-point effects in more general systems. {\textcopyright} 1969 The American Physical Society.},
author = {Ferdinand, Arthur E. and Fisher, Michael E.},
doi = {10.1103/PhysRev.185.832},
issn = {0031899X},
journal = {Physical Review},
month = {sep},
number = {2},
pages = {832--846},
pmid = {11268102},
title = {{Bounded and inhomogeneous ising models. I. Specific-heat anomaly of a finite lattice}},
url = {https://link.aps.org/doi/10.1103/PhysRev.185.832},
volume = {185},
year = {1969}
}
@article{Rupp2012,
abstract = {We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schr{\"{o}}dinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves. {\textcopyright} 2012 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1109.2618},
author = {Rupp, Matthias and Tkatchenko, Alexandre and M{\"{u}}ller, Klaus Robert and {Von Lilienfeld}, O. Anatole},
doi = {10.1103/PhysRevLett.108.058301},
eprint = {1109.2618},
isbn = {1079-7114 (Electronic)\r0031-9007 (Linking)},
issn = {00319007},
journal = {Physical Review Letters},
keywords = {Learning/Statistics & Optimisation},
month = {jan},
number = {5},
pages = {058301},
pmid = {22400967},
title = {{Fast and accurate modeling of molecular atomization energies with machine learning}},
url = {http://eprints.pascal-network.org/archive/00009418/ http://link.aps.org/doi/10.1103/PhysRevLett.108.058301 https://link.aps.org/doi/10.1103/PhysRevLett.108.058301},
volume = {108},
year = {2012}
}
@article{Mills2017a,
abstract = {We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbor energy of the 4×4 Ising model. Using its success at this task, we motivate the study of the larger 8×8 Ising model, showing that the deep neural network can learn the nearest-neighbor Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. We then demonstrate the ability of the neural network to learn other spin models, teaching the convolutional deep neural network to accurately predict the long-range interaction of a screened Coulomb Hamiltonian, a sinusoidally attenuated screened Coulomb Hamiltonian, and a modified Potts model Hamiltonian. In the case of the long-range interaction, we demonstrate the ability of the neural network to recover the phase transition with equivalent accuracy to the numerically exact method. Furthermore, in the case of the long-range interaction, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, and do so 1600 times faster than a CUDA-optimized exact calculation. Additionally, we demonstrate how the neural network succeeds at these tasks by looking at the weights learned in a simplified demonstration.},
archivePrefix = {arXiv},
arxivId = {1706.09779},
author = {Mills, Kyle and Tamblyn, Isaac},
doi = {10.1103/PhysRevE.97.032119},
eprint = {1706.09779},
issn = {24700053},
journal = {Physical Review E},
month = {jun},
number = {3},
pmid = {29776084},
title = {{Deep neural networks for direct, featureless learning through observation: The case of two-dimensional spin models}},
url = {http://arxiv.org/abs/1706.09779},
volume = {97},
year = {2018}
}
@techreport{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
author = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
isbn = {9781627480031},
issn = {10495258},
pages = {1223--1231},
title = {{Large scale distributed deep networks}},
volume = {2},
year = {2012}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchange-correlation functionals, PBE and RPBE, as reference methods. We find that the binding energy errors of the NN potentials with respect to DFT are significantly lower than the typical uncertainties of DFT calculations arising from the choice of the exchange-correlation functional. Further, we examine the role of van der Waals interactions, which are not properly described by GGA functionals. Specifically, we incorporate the D3 scheme suggested by Grimme (J. Chem. Phys. 2010, 132, 154104) in our potentials and demonstrate that it can be applied to GGA-based NN potentials in the same way as to DFT calculations without modification. Our results show that the description of small water clusters provided by the RPBE functional is significantly improved if van der Waals interactions are included, while in case of the PBE functional, which is well-known to yield stronger binding than RPBE, van der Waals corrections lead to overestimated binding energies. {\textcopyright} 2013 American Chemical Society.},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
month = {aug},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
url = {http://pubs.acs.org/doi/abs/10.1021/jp401225b},
volume = {117},
year = {2013}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {::},
issn = {00280836},
journal = {Nature},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
number = {6088},
pages = {533--536},
publisher = {Nature Publishing Group},
title = {{Learning representations by back-propagating errors}},
url = {https://www-nature-com.uproxy.library.dc-uoit.ca/articles/323533a0},
volume = {323},
year = {1986}
}
@article{Balabin2009,
abstract = {Artificial neural network (ANN) approach has been applied to estimate the density functional theory (DFT) energy with large basis set using lower-level energy values and molecular descriptors. A total of 208 different molecules were used for the ANN training, cross validation, and testing by applying BLYP, B3LYP, and BMK density functionals. Hartree-Fock results were reported for comparison. Furthermore, constitutional molecular descriptor (CD) and quantum-chemical molecular descriptor (QD) were used for building the calibration model. The neural network structure optimization, leading to four to five hidden neurons, was also carried out. The usage of several low-level energy values was found to greatly reduce the prediction error. An expected error, mean absolute deviation, for ANN approximation to DFT energies was 0.6±0.2 kcal mol-1. In addition, the comparison of the different density functionals with the basis sets and the comparison of multiple linear regression results were also provided. The CDs were found to overcome limitation of the QD. Furthermore, the effective ANN model for DFT/ 6-311G (3df,3pd) and DFT/ 6-311G (2df,2pd) energy estimation was developed, and the benchmark results were provided. {\textcopyright} 2009 American Institute of Physics.},
author = {Balabin, Roman M. and Lomakina, Ekaterina I.},
doi = {10.1063/1.3206326},
isbn = {1089-7690 (Electronic)\r0021-9606 (Linking)},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {7},
pmid = {19708729},
title = {{Neural network approach to quantum-chemistry data: Accurate prediction of density functional theory energies}},
volume = {131},
year = {2009}
}
@techreport{adadelta,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {::},
keywords = {Gradient Descent,Index Terms-Adaptive Learning Rates,Machine Learn-ing,Neural Networks},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Bounds1987,
abstract = {Research in spin-glass physics, population genetics, and neural network dynamics has provided powerful methods for finding near-global optima of functions that have many local optima. These techniques are being applied successfully to a wide variety of scientific and engineering problems. They may also give new insights into combinatorial optimization problems. {\textcopyright} 1987 Nature Publishing Group.},
author = {Bounds, David G.},
doi = {10.1038/329215a0},
issn = {00280836},
journal = {Nature},
number = {6136},
pages = {215--219},
title = {{New optimization methods from physics and biology}},
volume = {329},
year = {1987}
}
@article{Hen2012,
abstract = {We propose a method using a quantum annealer-an analog quantum computer based on the principles of quantum adiabatic evolution-to solve the graph isomorphism problem, in which one has to determine whether two graphs are isomorphic (i.e., can be transformed into each other simply by a relabeling of the vertices). We demonstrate the capabilities of the method by analyzing several types of graph families, focusing on graphs with particularly high symmetry called strongly regular graphs. We also show that our method is applicable, within certain limitations, to currently available quantum hardware such as D-Wave One. {\textcopyright} 2012 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1207.1712},
author = {Hen, Itay and Young, A. P.},
doi = {10.1103/PhysRevA.86.042310},
eprint = {1207.1712},
issn = {10502947},
journal = {Physical Review A - Atomic, Molecular, and Optical Physics},
number = {4},
pages = {1--8},
title = {{Solving the graph-isomorphism problem with a quantum annealer}},
volume = {86},
year = {2012}
}
@inproceedings{LeCun90,
abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.},
author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, Richard and Hubbard, Wayne and Jackel, Lawrence},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Touretzky, D},
pages = {396--404},
publisher = {Morgan-Kaufmann},
title = {{Handwritten digit recognition with a back-propagation network}},
url = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
volume = {2},
year = {1990}
}
@article{Gal2016a,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
file = {::},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
month = {jun},
pages = {1651--1660},
title = {{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}},
url = {http://yarin.co. http://arxiv.org/abs/1506.02142},
volume = {3},
year = {2016}
}
@article{Burmeister1995,
abstract = {Go provides artificial intelligence (AI) and cognitive science researchers with an easily specified formal domain in which skills of human intelligence cannot be matched by currently known programming techniques. Go is a much more widely played game than chess (principally in Japan, Korea and China), yet it is not well known to AI and cognitive science researchers and our goal in this paper is to introduce some of the challenges of the game to the AI community in the form of a comparison with chess. Go has been called a possible "task par excellence for AI" by Berliner [1] and we conclude that Go is a domain in which the development of new programming techniques is not only possible but is in fact necessary.},
author = {Burmeister, Jay and Wiles, Janet},
doi = {10.1109/ANZIIS.1995.705737},
isbn = {0864224303},
journal = {ANZIIS 1995 - Proceedings of the 3rd Australian and New Zealand Conference on Intelligent Information Systems},
pages = {181--186},
title = {{The challenge of go as a domain for AI research: A comparison between go and chess}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=705737%5Cnpapers3://publication/uuid/B00B0293-9C0A-4C08-8BD6-34ED130B0272},
year = {1995}
}
@article{Tian2017,
abstract = {A concept for prediction of organic coatings, based on the alternating hydrostatic pressure (AHP) accelerated tests, has been presented. An AHP accelerated test with different pressure values has been employed to evaluate coating degradation. And a back-propagation artificial neural network (BP-ANN) has been established to predict the service property and the service lifetime of coatings. The pressure value (P), immersion time (t) and service property (impedance modulus |Z|) are utilized as the parameters of the network. The average accuracies of the predicted service property and immersion time by the established network are 98.6% and 84.8%, respectively. The combination of accelerated test and prediction method by BP-ANN is promising to evaluate and predict coating property used in deep sea.},
author = {Tian, Wenliang and Meng, Fandi and Liu, Li and Li, Ying and Wang, Fuhui},
doi = {10.1038/srep40827},
issn = {20452322},
journal = {Scientific Reports},
month = {jan},
number = {January},
pages = {40827},
pmid = {28094340},
publisher = {Nature Publishing Group},
title = {{Lifetime prediction for organic coating under alternating hydrostatic pressure by artificial neural network}},
url = {http://www.nature.com/articles/srep40827},
volume = {7},
year = {2017}
}
@article{Mills2017a,
abstract = {We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbor energy of the 4×4 Ising model. Using its success at this task, we motivate the study of the larger 8×8 Ising model, showing that the deep neural network can learn the nearest-neighbor Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. We then demonstrate the ability of the neural network to learn other spin models, teaching the convolutional deep neural network to accurately predict the long-range interaction of a screened Coulomb Hamiltonian, a sinusoidally attenuated screened Coulomb Hamiltonian, and a modified Potts model Hamiltonian. In the case of the long-range interaction, we demonstrate the ability of the neural network to recover the phase transition with equivalent accuracy to the numerically exact method. Furthermore, in the case of the long-range interaction, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, and do so 1600 times faster than a CUDA-optimized exact calculation. Additionally, we demonstrate how the neural network succeeds at these tasks by looking at the weights learned in a simplified demonstration.},
archivePrefix = {arXiv},
arxivId = {1706.09779},
author = {Mills, Kyle and Tamblyn, Isaac},
doi = {10.1103/PhysRevE.97.032119},
eprint = {1706.09779},
issn = {24700053},
journal = {Physical Review E},
month = {jun},
number = {3},
pmid = {29776084},
title = {{Deep neural networks for direct, featureless learning through observation: The case of two-dimensional spin models}},
url = {http://arxiv.org/abs/1706.09779},
volume = {97},
year = {2018}
}
@article{Zhanga,
abstract = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
archivePrefix = {arXiv},
arxivId = {1710.10916},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N.},
doi = {10.1109/TPAMI.2018.2856256},
eprint = {1710.10916},
file = {::},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Generative models,generative adversarial networks (GANs),multi-distribution approximation,multi-stage GANs,photo-realistic image generation,text-to-image synthesis},
month = {dec},
number = {8},
pages = {1947--1962},
pmid = {30010548},
title = {{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.03242},
volume = {41},
year = {2019}
}
@article{stander1994temperature,
abstract = {It is well known that the behaviour of the simulated annealing approach to optimization is crucially dependent on the choice of temperature schedule. In this paper, a dynamic programming approach is used to find the temperature schedule which is optimal for a simple minimization problem. The optimal schedule is compared with certain standard non-optimal choices. These generally perform well provided the first and last temperatures are suitably selected. Indeed, these temperatures can be chosen in such a way as to make the performance of the logarithmic schedule almost optimal. This optimal performance is fairly robust to the choice of the first temperature. The dynamic programming approach cannot be applied directly to problems of more realistic size, such as those arising in statistical image reconstruction. Nevertheless, some simulation experiments suggest that the general conclusions from the simple minimization problem do carry over to larger problems. Various families of schedules can be made to perform well with suitable choice of the first and last temperatures, and the logarithmic schedule combines good performance with reasonable robustness to the choice of the first temperature. {\textcopyright} 1994 Chapman & Hall.},
author = {Stander, Julian and Silverman, Bernard W.},
doi = {10.1007/BF00143921},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Dynamic programming,image reconstruction,stochastic optimization},
number = {1},
pages = {21--32},
publisher = {Springer},
title = {{Temperature schedules for simulated annealing}},
volume = {4},
year = {1994}
}
@article{GoogleResearch2015,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
journal = {None},
month = {mar},
number = {212},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
volume = {1},
year = {2016}
}
@techreport{rmsprop,
abstract = {Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012},
author = {Hinton, G E},
booktitle = {Hinton NNHinton NN},
file = {::},
pages = {29},
title = {{Neural Networks for Machine Learning Lecture 6a Overview of mini- ­ ‐ batch gradient descent Reminder : The error surface for a linear neuron}},
url = {https://www.cs.toronto.edu/$\sim$tijmen/csc321/slides/lecture_slides_lec6.pdf%0Ahttp://www.cs.toronto.edu/$\sim$tijmen/csc321/slides/lecture_slides_lec6.pdf},
year = {2012}
}
@article{Kim2013,
abstract = {We decompose the energy error of any variational density functional theory calculation into a contribution due to the approximate functional and that due to the approximate density. Typically, the functional error dominates, but in many interesting situations the density-driven error dominates. Examples range from calculations of electron affinities to preferred geometries of ions and radicals in solution. In these abnormal cases, the error in density functional theory can be greatly reduced by using a more accurate density. A small orbital gap often indicates a substantial density-driven error. {\textcopyright} 2013 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1212.3054},
author = {Kim, Min Cheol and Sim, Eunji and Burke, Kieron},
doi = {10.1103/PhysRevLett.111.073003},
eprint = {1212.3054},
issn = {00319007},
journal = {Physical Review Letters},
number = {7},
pages = {1--5},
pmid = {23992062},
title = {{Understanding and reducing errors in density functional calculations}},
volume = {111},
year = {2013}
}
@article{Behler2007,
abstract = {The accurate description of chemical processes often requires the use of computationally demanding methods like density-functional theory (DFT), making long simulations of large systems unfeasible. In this Letter we introduce a new kind of neural-network representation of DFT potential-energy surfaces, which provides the energy and forces as a function of all atomic positions in systems of arbitrary size and is several orders of magnitude faster than DFT. The high accuracy of the method is demonstrated for bulk silicon and compared with empirical potentials and DFT. The method is general and can be applied to all types of periodic and nonperiodic systems. {\textcopyright} 2007 The American Physical Society.},
author = {Behler, J{\"{o}}rg and Parrinello, Michele},
doi = {10.1103/PhysRevLett.98.146401},
isbn = {0031-9007},
issn = {00319007},
journal = {Physical Review Letters},
number = {14},
pages = {1--4},
pmid = {17501293},
title = {{Generalized neural-network representation of high-dimensional potential-energy surfaces}},
volume = {98},
year = {2007}
}
@techreport{Hron2018,
abstract = {Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for variational Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.},
archivePrefix = {arXiv},
arxivId = {1807.01969},
author = {Hron, Jiri and {De G Matthews}, Alexander G. and Ghahramani, Zoubin},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1807.01969},
file = {::},
isbn = {9781510867963},
pages = {3199--3219},
title = {{Variational Bayesian dropout: Pitfalls and fixes}},
volume = {5},
year = {2018}
}
@book{deeplearningbook,
abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.},
annote = {\url{http://www.deeplearningbook.org}},
author = {Courville, Ian Goodfellow and Yoshua Bengio and Aaron},
booktitle = {Nature},
isbn = {3463353563306},
keywords = {cnn,convolutional neural networks,deep learning,plant disease},
number = {7553},
pages = {1--73},
publisher = {MIT Press},
title = {{Deep learning 简介 一 、 什么是 Deep Learning ？}},
volume = {29},
year = {2016}
}
@techreport{Salakhutdinova,
abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer "pre-training" phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks. {\textcopyright} 2009 by the authors.},
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
booktitle = {Journal of Machine Learning Research},
file = {::},
issn = {15324435},
pages = {448--455},
title = {{Deep Boltzmann machines}},
volume = {5},
year = {2009}
}
@techreport{fullyconvolutional,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {::},
issn = {01628828},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
number = {4},
pages = {640--651},
pmid = {27244717},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
volume = {39},
year = {2017}
}
@article{Harris2010a,
abstract = {A superconducting chip containing a regular array of flux qubits, tunable interqubit inductive couplers, an XY-addressable readout system, on-chip programmable magnetic memory, and a sparse network of analog control lines has been studied. The architecture of the chip and the infrastructure used to control it were designed to facilitate the implementation of an adiabatic quantum optimization algorithm. The performance of an eight-qubit unit cell on this chip has been characterized by measuring its success in solving a large set of random Ising spin-glass problem instances as a function of temperature. The experimental data are consistent with the predictions of a quantum mechanical model of an eight-qubit system coupled to a thermal environment. These results highlight many of the key practical challenges that we have overcome and those that lie ahead in the quest to realize a functional large-scale adiabatic quantum information processor. {\textcopyright} 2010 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1004.1628},
author = {Harris, R. and Johnson, M. W. and Lanting, T. and Berkley, A. J. and Johansson, J. and Bunyk, P. and Tolkacheva, E. and Ladizinsky, E. and Ladizinsky, N. and Oh, T. and Cioata, F. and Perminov, I. and Spear, P. and Enderud, C. and Rich, C. and Uchaikin, S. and Thom, M. C. and Chapple, E. M. and Wang, J. and Wilson, B. and Amin, M. H.S. and Dickson, N. and Karimi, K. and MacReady, B. and Truncik, C. J.S. and Rose, G.},
doi = {10.1103/PhysRevB.82.024511},
eprint = {1004.1628},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {2},
pages = {1--15},
title = {{Experimental investigation of an eight-qubit unit cell in a superconducting optimization processor}},
volume = {82},
year = {2010}
}
@article{Shen2016,
abstract = {Molecular dynamics simulation with multiscale quantum mechanics/molecular mechanics (QM/MM) methods is a very powerful tool for understanding the mechanism of chemical and biological processes in solution or enzymes. However, its computational cost can be too high for many biochemical systems because of the large number of ab initio QM calculations. Semiempirical QM/MM simulations have much higher efficiency. Its accuracy can be improved with a correction to reach the ab initio QM/MM level. The computational cost on the ab initio calculation for the correction determines the efficiency. In this paper we developed a neural network method for QM/MM calculation as an extension of the neural-network representation reported by Behler and Parrinello. With this approach, the potential energy of any configuration along the reaction path for a given QM/MM system can be predicted at the ab initio QM/MM level based on the semiempirical QM/MM simulations. We further applied this method to three reactions in water to calculate the free energy changes. The free-energy profile obtained from the semiempirical QM/MM simulation is corrected to the ab initio QM/MM level with the potential energies predicted with the constructed neural network. The results are in excellent accordance with the reference data that are obtained from the ab initio QM/MM molecular dynamics simulation or corrected with direct ab initio QM/MM potential energies. Compared with the correction using direct ab initio QM/MM potential energies, our method shows a speed-up of 1 or 2 orders of magnitude. It demonstrates that the neural network method combined with the semiempirical QM/MM calculation can be an efficient and reliable strategy for chemical reaction simulations.},
author = {Shen, Lin and Wu, Jingheng and Yang, Weitao},
doi = {10.1021/acs.jctc.6b00663},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {10},
pages = {4934--4946},
pmid = {27552235},
title = {{Multiscale Quantum Mechanics/Molecular Mechanics Simulations with Neural Networks}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jctc.6b00663},
volume = {12},
year = {2016}
}
@article{Linnainmaa1976,
abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed. {\textcopyright} 1976 BIT Foundations.},
author = {Linnainmaa, Seppo},
doi = {10.1007/BF01931367},
issn = {15729125},
journal = {Bit},
keywords = {Computational Mathematics and Numerical Analysis,Mathematics,Numeric Computing,general},
number = {2},
pages = {146--160},
publisher = {Kluwer Academic Publishers},
title = {{Taylor expansion of the accumulated rounding error}},
url = {https://link.springer.com/article/10.1007/BF01931367},
volume = {16},
year = {1976}
}
@techreport{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Communications of the ACM},
doi = {10.1145/3065386},
file = {::},
issn = {15577317},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://code.google.com/p/cuda-convnet/},
volume = {60},
year = {2017}
}
@techreport{Wan2013,
abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models. Copyright 2013 by the author(s).},
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
booktitle = {30th International Conference on Machine Learning, ICML 2013},
file = {::},
number = {PART 3},
pages = {2095--2103},
title = {{Regularization of neural networks using DropConnect}},
year = {2013}
}
@book{Sutton2018,
abstract = {CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11},
author = {Sutton, R.S. and Barto, A.G.},
booktitle = {IEEE Transactions on Neural Networks},
doi = {10.1109/tnn.1998.712192},
edition = {Second},
isbn = {9780262039246},
issn = {1045-9227},
month = {sep},
number = {5},
pages = {1054--1054},
publisher = {The MIT Press},
title = {{Reinforcement Learning: An Introduction}},
url = {http://incompleteideas.net/book/the-book-2nd.html https://www.cambridge.org/core/product/identifier/S0263574799271172/type/journal_article http://ieeexplore.ieee.org/document/712192/},
volume = {9},
year = {1998}
}
@article{Monterola2001,
abstract = {We present a practical method for estimating the upper error bound in the neural network (NN) solution of the nonlinear Schr{\"{o}}dinger equation (NLSE) under different degrees of nonlinearity. The error bound is a function of the nonnegative energy E value that is minimized when the NN is trained to solve the NLSE. The form of E is derived from the NLSE expression and the NN solution becomes identical with the true NLSE solution only when the E value is reduced exactly to zero. In practice, machines with finite floating-point range and accuracy are used for training and E is not decreased exactly to zero. Knowledge of the error bound permits the estimation of the maximum average error in the NN solution without prior knowledge of the true NLSE solution - a crucial factor in the practical applications of the NN technique. The error bound is verified for both the linear time - independent Schr{\"{o}}dinger equation for a free particle, and the NLSE. We also discuss the conditions where the error bound formulation is valid. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
author = {Monterola, Christopher and Saloma, Caesar},
doi = {10.1016/S0030-4018(03)01570-0},
issn = {00304018},
journal = {Optics Communications},
keywords = {Neural networks,Nonlinear Schr{\"{o}}dinger equation,Numerical approximation and analysis,Numerical simulation,Solution of equations},
month = {jul},
number = {1-6},
pages = {331--339},
pmid = {19421275},
title = {{Solving the nonlinear Schr{\"{o}}dinger equation with an unsupervised neural network: Estimation of error in solution}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0030401803015700 https://www.osapublishing.org/oe/abstract.cfm?uri=oe-9-2-72},
volume = {222},
year = {2003}
}
@incollection{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most "classical" second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations. {\textcopyright} Springer-Verlag Berlin Heidelberg 2012.},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus Robert},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-35289-8_3},
isbn = {9783642352881},
issn = {16113349},
pages = {9--48},
publisher = {Springer, Berlin, Heidelberg},
title = {{Efficient backprop}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3},
volume = {7700 LECTU},
year = {2012}
}
@techreport{Kendall,
abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
archivePrefix = {arXiv},
arxivId = {1703.04977},
author = {Kendall, Alex and Gal, Yarin},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1703.04977},
file = {::},
isbn = {1703.04977v2},
issn = {10495258},
pages = {5575--5585},
title = {{What uncertainties do we need in Bayesian deep learning for computer vision?}},
volume = {2017-Decem},
year = {2017}
}
@article{Riera2016,
abstract = {A new set of i-TTM potential energy functions describing the interactions between alkali metal ions and water molecules is reported. Following our previous study on halide ion-water interactions [J. Phys. Chem. B, 2016, 120, 1822], the new i-TTM potentials are derived from fits to CCSD(T) reference energies and, by construction, are compatible with the MB-pol many-body potential, which has been shown to accurately predict the properties of water from the gas to the condensed phase. Within the i-TTM formalism, two-body repulsion, electrostatic, and dispersion energies are treated explicitly, while many-body effects are represented by classical induction. The accuracy of the new i-TTM potentials is assessed through extensive comparisons with results obtained from different ab initio methods, including CCSD(T), CCSD(T)-F12b, DF-MP2, and several DFT models, as well as from polarizable force fields for M+(H2O)n clusters with M+ = Li+, Na+, K+, Rb+, and Cs+, and n = 1-4.},
author = {Riera, Marc and G{\"{o}}tz, Andreas W. and Paesani, Francesco},
doi = {10.1039/c6cp02553f},
issn = {14639076},
journal = {Physical Chemistry Chemical Physics},
number = {44},
pages = {30334--30343},
publisher = {Royal Society of Chemistry},
title = {{The i-TTM model for ab initio-based ion-water interaction potentials. II. Alkali metal ion-water potential energy functions}},
url = {http://xlink.rsc.org/?DOI=C6CP02553F},
volume = {18},
year = {2016}
}
@article{Shafaei2016b,
abstract = {Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60,000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision.},
archivePrefix = {arXiv},
arxivId = {1608.01745},
author = {Shafaei, Alireza and Little, James J. and Schmidt, Mark},
doi = {10.5244/C.30.26},
eprint = {1608.01745},
journal = {British Machine Vision Conference 2016, BMVC 2016},
pages = {26.1--26.13},
title = {{Play and learn: Using video games to train computer vision models}},
url = {http://arxiv.org/abs/1608.01745},
volume = {2016-Septe},
year = {2016}
}
@article{Luchak2017,
abstract = {We present a physically-motivated topology of a deep neural network that can efficiently infer extensive parameters (such as energy, entropy, or number of particles) of arbitrarily large systems, doing so with scaling. We use a form of domain decomposition for training and inference, where each sub-domain (tile) is comprised of a non-overlapping focus region surrounded by an overlapping context region. The size of these regions is motivated by the physical interaction length scales of the problem. We demonstrate the application of EDNNs to three physical systems: the Ising model and two hexagonal/graphene-like datasets. In the latter, an EDNN was able to make total energy predictions of a 60 atoms system, with comparable accuracy to density functional theory (DFT), in 57 milliseconds. Additionally EDNNs are well suited for massively parallel evaluation, as no communication is necessary during neural network evaluation. We demonstrate that EDNNs can be used to make an energy prediction of a two-dimensional 35.2 million atom system, over 1.0 $\mu$m2 of material, at an accuracy comparable to DFT, in under 25 minutes. Such a system exists on a length scale visible with optical microscopy and larger than some living organisms.},
archivePrefix = {arXiv},
arxivId = {1708.06686},
author = {Mills, Kyle and Ryczko, Kevin and Luchak, Iryna and Domurad, Adam and Beeler, Chris and Tamblyn, Isaac},
doi = {10.1039/C8SC04578J},
eprint = {1708.06686},
issn = {20416539},
journal = {Chemical Science},
number = {15},
pages = {4129--4140},
title = {{Extensive deep neural networks for transferring small scale learning to large scale systems}},
url = {https://arxiv.org/pdf/1708.06686.pdf},
volume = {10},
year = {2019}
}
@techreport{Hoffer2017,
abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomenon. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on a random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
booktitle = {arXiv},
file = {::},
issn = {23318422},
pages = {1731--1741},
title = {{Train longer, generalize better: Closing the generalization gap in large batch training of neural networks}},
volume = {30},
year = {2017}
}
@article{Chetlur2014,
abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.},
archivePrefix = {arXiv},
arxivId = {1410.0759},
author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
eprint = {1410.0759},
journal = {arXiv},
month = {oct},
pages = {1--9},
title = {{cuDNN: Efficient Primitives for Deep Learning}},
url = {http://arxiv.org/abs/1410.0759},
year = {2014}
}
@article{Dolgirev2016,
abstract = {We present a new method for a fast, unbiased and accurate representation of interatomic interactions. It is a combination of an artificial neural network and our new approach for pair potential reconstruction. The potential reconstruction method is simple and computationally cheap and gives rich information about interactions in crystals. This method can be combined with structure prediction and molecular dynamics simulations, providing accuracy similar to ab initio methods, but at a small fraction of the cost. We present applications to real systems and discuss the insight provided by our method.},
author = {Dolgirev, Pavel E. and Kruglov, Ivan A. and Oganov, Artem R.},
doi = {10.1063/1.4961886},
isbn = {0021-9606},
issn = {21583226},
journal = {AIP Advances},
month = {aug},
number = {8},
pages = {085318},
title = {{Machine learning scheme for fast extraction of chemically interpretable interatomic potentials}},
url = {http://dx.doi.org/10.1063/1.4961886 http://aip.scitation.org/doi/10.1063/1.4961886},
volume = {6},
year = {2016}
}
@techreport{Segui2015a,
abstract = {Learning to count is a learning strategy that has been recently proposed in the literature for dealing with problems where estimating the number of object instances in a scene is the final objective. In this framework, the task of learning to detect and localize individual object instances is seen as a harder task that can be evaded by casting the problem as that of computing a regression value from hand-crafted image features. In this paper we explore the features that are learned when training a counting convolutional neural network in order to understand their underlying representation. To this end we define a counting problem for MNIST data and show that the internal representation of the network is able to classify digits in spite of the fact that no direct supervision was provided for them during training. We also present preliminary results about a deep network that is able to count the number of pedestrians in a scene.},
archivePrefix = {arXiv},
arxivId = {1505.08082},
author = {Segui, Santi and Pujol, Oriol and Vitria, Jordi},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2015.7301276},
eprint = {1505.08082},
file = {::},
isbn = {9781467367592},
issn = {21607516},
keywords = {Accuracy,Feature extraction,Neural networks,Proposals,Supervised learning,Training,Visualization},
pages = {90--96},
title = {{Learning to count with deep object features}},
volume = {2015-Octob},
year = {2015}
}
@article{VanMilligen1995,
abstract = {A new generally applicable method to solve differential equations, based on neural networks, is proposed. Straightforward to implement, finite differences and coordinate transformations are not used. The neural network provides a flexible and compact base for representing the solution, found through the global minimization of an error functional. As a proof of principle, a two-dimensional ideal magnetohydrodynamic plasma equilibrium is solved. Since no particular topology is assumed, the technique is especially promising for the three-dimensional plasma equilibrium problem. {\textcopyright} 1995 The American Physical Society.},
author = {{Van Milligen}, B. Ph and Tribaldos, V. and Jim{\'{e}}nez, J. A.},
doi = {10.1103/PhysRevLett.75.3594},
issn = {00319007},
journal = {Physical Review Letters},
number = {20},
pages = {3594--3597},
pmid = {10059679},
title = {{Neural network differential equation and plasma equilibrium solver}},
volume = {75},
year = {1995}
}
@article{Bengio:2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks. {\textcopyright} 2009 Y. Bengio.},
address = {Hanover, MA, USA},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
isbn = {2200000006},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
month = {jan},
number = {1},
pages = {1--27},
pmid = {17348934},
publisher = {Now Publishers Inc.},
title = {{Learning deep architectures for AI}},
url = {http://dx.doi.org/10.1561/2200000006 http://www.nowpublishers.com/article/Details/MAL-006},
volume = {2},
year = {2009}
}
@article{Lucas2014,
abstract = {We provide Ising formulations for many NP-complete and NP-hard problems, including all of Karp's 21 NP-complete problems. This collects and extends mappings to the Ising model from partitioning, covering, and satisfiability. In each case, the required number of spins is at most cubic in the size of the problem. This work may be useful in designing adiabatic quantum optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1302.5843},
author = {Lucas, Andrew},
doi = {10.3389/fphy.2014.00005},
eprint = {1302.5843},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Adiabatic quantum computation,Algorithms,Complexity theory,NP,Spin glasses},
number = {February},
pages = {1--14},
title = {{Ising formulations of many NP problems}},
volume = {2},
year = {2014}
}
@article{Ramakrishnan2015,
abstract = {Due to its favorable computational efficiency, time-dependent (TD) density functional theory (DFT) enables the prediction of electronic spectra in a high-throughput manner across chemical space. Its predictions, however, can be quite inaccurate. We resolve this issue with machine learning models trained on deviations of reference second-order approximate coupled-cluster (CC2) singles and doubles spectra from TDDFT counterparts, or even from DFT gap. We applied this approach to low-lying singlet-singlet vertical electronic spectra of over 20000 synthetically feasible small organic molecules with up to eight CONF atoms. The prediction errors decay monotonously as a function of training set size. For a training set of 10000 molecules, CC2 excitation energies can be reproduced to within ±0.1 eV for the remaining molecules. Analysis of our spectral database via chromophore counting suggests that even higher accuracies can be achieved. Based on the evidence collected, we discuss open challenges associated with data-driven modeling of high-lying spectra and transition intensities.},
archivePrefix = {arXiv},
arxivId = {1504.01966},
author = {Ramakrishnan, Raghunathan and Hartmann, Mia and Tapavicza, Enrico and {Von Lilienfeld}, O. Anatole},
doi = {10.1063/1.4928757},
eprint = {1504.01966},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {8},
pages = {6},
pmid = {26328822},
title = {{Electronic spectra from TDDFT and machine learning in chemical space}},
url = {http://dx.doi.org/10.1063/1.4928757},
volume = {143},
year = {2015}
}
@article{Kingma2014a,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
eprint = {1312.6114},
file = {::},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Auto-encoding variational bayes}},
url = {https://arxiv.org/abs/1312.6114v10 http://arxiv.org/abs/1312.6114},
year = {2014}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {14764687},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Mehta2014,
abstract = {Measuring the morphological parameters of galaxies is a key requirement for studying their formation and evolution. Surveys such as the Sloan Digital Sky Survey have resulted in the availability of very large collections of images, which have permitted population-wide analyses of galaxy morphology. Morphological analysis has traditionally been carried out mostly via visual inspection by trained experts, which is time consuming and does not scale to large (≳10<sup>4</sup>) numbers of images. Although attempts have been made to build automated classification systems, these have not been able to achieve the desired level of accuracy. The Galaxy Zoo project successfully applied a crowdsourcing strategy, inviting online users to classify images by answering a series of questions. Unfortunately, even this approach does not scale well enough to keep up with the increasing availability of galaxy images. We present a deep neural network model for galaxy morphology classification which exploits translational and rotational symmetry. It was developed in the context of the Galaxy Challenge, an international competition to build the best model for morphology classification based on annotated images from the Galaxy Zoo project. For images with high agreement among the Galaxy Zoo participants, our model is able to reproduce their consensus with near-perfect accuracy (>99 per cent) for most questions. Confident model predictions are highly accurate, which makes the model suitable for filtering large collections of images and forwarding challenging images to experts for manual annotation. This approach greatly reduces the experts' workload without affecting accuracy. The application of these algorithms to larger sets of training data will be critical for analysing results from future surveys such as the Large Synoptic Survey Telescope.},
archivePrefix = {arXiv},
arxivId = {1503.07077},
author = {Dieleman, Sander and Willett, Kyle W. and Dambre, Joni},
doi = {10.1093/mnras/stv632},
eprint = {1503.07077},
isbn = {1410.3831},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Catalogues,Galaxies: general,Methods: data analysis,Techniques: image processing},
month = {jun},
number = {2},
pages = {1441--1459},
pmid = {7491034},
title = {{Rotation-invariant convolutional neural networks for galaxy morphology prediction}},
url = {http://arxiv.org/abs/1301.3124 http://arxiv.org/abs/1410.3831 http://academic.oup.com/mnras/article/450/2/1441/979677/Rotationinvariant-convolutional-neural-networks},
volume = {450},
year = {2015}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Husz{\'{a}}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
doi = {10.1109/CVPR.2017.19},
eprint = {1609.04802},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
month = {sep},
pages = {105--114},
title = {{Photo-realistic single image super-resolution using a generative adversarial network}},
url = {http://arxiv.org/abs/1609.04802},
volume = {2017-Janua},
year = {2017}
}
@article{Tsukamoto2017,
abstract = {In today's world, there are many situations in which difficult decisions must be made under such constraints as a limited resource and a limited amount of time. These situations include disaster response planning, economic policy decision-making, and investment portfolio optimization. In such situations, it is often necessary to solve a "combinatorial optimization problem," which involves evaluating different combinations of various factors and selecting the optimum combination. Since the number of combinations increases explosively as the number of factors increases, it becomes difficult to find the best answer in a realistic amount of time using a von Neumann type processor. To give a solution for such problems, we have developed two schemes to speed up the 1024-bit Ising model and implemented them in a field-programmable gate array (FPGA). Testing demonstrated that a system using this architecture can solve a 32-city traveling salesman problem 12,000 times faster than the same algorithm running on a 3.5-GHz Intel Xeon E5-1620 v3 processor.},
author = {Tsukamoto, Sanroku and Takatsu, Motomu and Matsubara, Satoshi and Tamura, Hirotaka},
issn = {00162523},
journal = {Fujitsu Scientific and Technical Journal},
number = {5},
pages = {8--13},
title = {{An accelerator architecture for combinatorial optimization problems}},
volume = {53},
year = {2017}
}
@article{LennardJones24,
abstract = { The investigation of a preceding paper has shown that the temperature variation of viscosity, as determined experimentally, can be satisfactorily explained in many gases on the assumption that the repulsive and attractive parts of the molecular field are each according to an inverse power of the distance. In some cases, in argon, for example, it was further shown that the experimental facts can be explained by more than one molecular model, from which we inferred that viscosity results alone are insufficient to determine precisely the nature of molecular fields. The object of the present paper is to ascertain whether a molecular model of the same type will also explain available experimental data concerning the equation of state of a gas, and if so, whether the results so obtained, when taken in conjunction with those obtained from viscosity, will definitely fix the molecular field. Such an investigation is made possible by the elaborate analysis by Kamerlingh Onnes of the observational material. He has expressed the results in the form of an empirical equation of state of the type pv = A + B/ v + C/ v 2 + D/ v 4 + E/ v 6 + F/ v 8 , where the coefficients A ... F, called by him virial coefficients , are determined as functions of the temperature to fit the observations. Now it is possible by various methods to obtain a theoretical expression for B as a function of the temperature and a strict comparison can then be made between theory and experiment. Unfortunately the solution for B, although applicable to any molecular model of spherical symmetry, is purely formal and contains an integral which can be evaluated only in special cases. This has been done up to now for only two simple models, viz., a van der Waals molecule, and a molecule repelling according to an inverse power law (without attraction), but it is shown in this paper that it can also be evaluated in the case of the model, which was successful in explaining viscosity results. As the two other models just mentioned are particular cases of this, the appropriate formul{\ae} for B are easily deduced from the general one given here. },
author = {Jones, J. E.},
doi = {10.1098/rspa.1924.0082},
isbn = {09501207},
issn = {0950-1207},
journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
month = {oct},
number = {738},
pages = {463--477},
pmid = {1000105633},
title = {{On the determination of molecular fields. —II. From the equation of state of a gas}},
url = {http://rspa.royalsocietypublishing.org/content/106/738/463.abstract},
volume = {106},
year = {1924}
}
@article{Li2016,
abstract = {Machine learning (ML) is an increasingly popular statistical tool for analyzing either measured or calculated data sets. Here, we explore its application to a well-defined physics problem, investigating issues of how the underlying physics is handled by ML, and how self-consistent solutions can be found by limiting the domain in which ML is applied. The particular problem is how to find accurate approximate density functionals for the kinetic energy (KE) of noninteracting electrons. Kernel ridge regression is used to approximate the KE of non-interacting fermions in a one dimensional box as a functional of their density. The properties of different kernels and methods of cross-validation are explored, reproducing the physics faithfully in some cases, but not others. We also address how self-consistency can be achieved with information on only a limited electronic density domain. Accurate constrained optimal densities are found via a modified Euler-Lagrange constrained minimization of the machine-learned total energy, despite the poor quality of its functional derivative. A projected gradient descent algorithm is derived using local principal component analysis. Additionally, a sparse grid representation of the density can be used without degrading the performance of the methods. The implications for machine-learned density functional approximations are discussed.},
archivePrefix = {arXiv},
arxivId = {1404.1333},
author = {Li, Li and Snyder, John C. and Pelaschier, Isabelle M. and Huang, Jessica and Niranjan, Uma Naresh and Duncan, Paul and Rupp, Matthias and M{\"{u}}ller, Klaus Robert and Burke, Kieron},
doi = {10.1002/qua.25040},
eprint = {1404.1333},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
keywords = {density functional theory,kinetic energy functional,machine learning,orbital free,self-consistent calculation},
month = {jun},
number = {11},
pages = {819--833},
title = {{Understanding machine-learned density functionals}},
url = {http://doi.wiley.com/10.1002/qua.25040},
volume = {116},
year = {2016}
}
@article{adamoptimizer,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {::},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@article{Goodfellow,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to <sup>1</sup>/<inf>2</inf> everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/abs/1406.2661v1},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.3156/jsoft.29.5_177_2},
eprint = {/arxiv.org/abs/1406.2661v1},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {2672--2680},
primaryClass = {https:},
title = {{Generative adversarial nets}},
url = {https://arxiv.org/abs/1406.2661v1},
volume = {3},
year = {2014}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {Cell Biology,Life Sciences,Mathematical and Computational Biology,general},
month = {dec},
number = {4},
pages = {115--133},
publisher = {Kluwer Academic Publishers},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://link.springer.com/article/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@article{Mirzaei2010,
abstract = {To represent the solution of a differential equation by an artificial neural network (ANN) was an idea introduced by Lagaris. Sugawara applied this concept to solve Schr{\"{o}}dinger's equation for select systems. We have submitted their method to a new kind of application. Here, for the first time, the approach is applied to the equations derived from density functional theory (DFT). At first, we have tested the procedure for two simple systems: the double harmonic oscillator and the hydrogen atom. The ANN solutions obtained for these simple systems reproduced the analytical results easily. Next, we have moved to the Tomas-Fermi theory and the Kohn-Sham formulation of DFT. In order to show the feasibility of the ANN representation of electronic density, we have solved the Hooke model-atom and two light atoms: helium and lithium. The ANN results match well with the analytical solution to the Hooke model-atom and with the numerical solutions for helium and lithium. {\textcopyright} 2010 Wiley Periodicals, Inc.},
author = {Caetano, C. and Reis, J. L. and Amorim, J. and Lemes, M. Ruv and Pino, A. Dal},
doi = {10.1002/qua.22572},
isbn = {1097461X},
issn = {00207608},
journal = {International Journal of Quantum Chemistry},
keywords = {Schr{\"{o}}dinger's equation,Thomas-Fermi equation,density functional theory,genetic algorithm,neural networks},
month = {oct},
number = {12},
pages = {2732--2740},
pmid = {20148191},
title = {{Using neural networks to solve nonlinear differential equations in atomic and molecular physics}},
url = {http://www3.interscience.wiley.com/journal/123444074/abstract%5Cnpapers://6ba9bf75-7dcc-4a9b-b1a7-22d2ef0616d5/Paper/p15538 http://doi.wiley.com/10.1002/qua.22572},
volume = {111},
year = {2011}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network ({\ae}net) package. The construction and application of ANN potentials using {\ae}net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite ($\alpha$-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
publisher = {Elsevier B.V.},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
url = {http://dx.doi.org/10.1016/j.commatsci.2015.11.047},
volume = {114},
year = {2016}
}
@article{cole2007,
abstract = {We have developed a classical two- and three-body interaction potential to simulate the hydroxylated, natively oxidized Si surface in contact with water solutions, based on the combination and extension of the Stillinger-Weber potential and of a potential originally developed to simulate Si O2 polymorphs. The potential parameters are chosen to reproduce the structure, charge distribution, tensile surface stress, and interactions with single water molecules of a natively oxidized Si surface model previously obtained by means of accurate density functional theory simulations. We have applied the potential to the case of hydrophilic silicon wafer bonding at room temperature, revealing maximum room temperature work of adhesion values for natively oxidized and amorphous silica surfaces of 97 and 90 mJ m2, respectively, at a water adsorption coverage of approximately 1 ML. The difference arises from the stronger interaction of the natively oxidized surface with liquid water, resulting in a higher heat of immersion (203 vs 166 mJ m2), and may be explained in terms of the more pronounced water structuring close to the surface in alternating layers of larger and smaller densities with respect to the liquid bulk. The computed force-displacement bonding curves may be a useful input for cohesive zone models where both the topographic details of the surfaces and the dependence of the attractive force on the initial surface separation and wetting can be taken into account. {\textcopyright} 2007 American Institute of Physics.},
archivePrefix = {arXiv},
arxivId = {0807.3215},
author = {Cole, Daniel J. and Payne, Mike C. and Cs{\'{a}}nyi, G{\'{a}}bor and Spearing, S. Mark and Ciacchi, Lucio Colombi},
doi = {10.1063/1.2799196},
eprint = {0807.3215},
isbn = {0021-9606},
issn = {00219606},
journal = {Journal of Chemical Physics},
month = {nov},
number = {20},
pages = {204704},
pmid = {18052443},
title = {{Development of a classical force field for the oxidized Si surface: Application to hydrophilic wafer bonding}},
url = {http://aip.scitation.org/doi/10.1063/1.2799196},
volume = {127},
year = {2007}
}
@article{Huang2016,
abstract = {The predictive accuracy of Machine Learning (ML) models of molecular properties depends on the choice of the molecular representation. Inspired by the postulates of quantum mechanics, we introduce a hierarchy of representations which meet uniqueness and target similarity criteria. To systematically control target similarity, we simply rely on interatomic many body expansions, as implemented in universal force-fields, including Bonding, Angular (BA), and higher order terms. Addition of higher order contributions systematically increases similarity to the true potential energy and predictive accuracy of the resulting ML models. We report numerical evidence for the performance of BAML models trained on molecular properties pre-calculated at electron-correlated and density functional theory level of theory for thousands of small organic molecules. Properties studied include enthalpies and free energies of atomization, heat capacity, zero-point vibrational energies, dipole-moment, polarizability, HOMO/LUMO energies and gap, ionization potential, electron affinity, and electronic excitations. After training, BAML predicts energies or electronic properties of out-of-sample molecules with unprecedented accuracy and speed.},
archivePrefix = {arXiv},
arxivId = {1608.06194},
author = {Huang, Bing and {Von Lilienfeld}, O. Anatole},
doi = {10.1063/1.4964627},
eprint = {1608.06194},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {16},
title = {{Communication: Understanding molecular representations in machine learning: The role of uniqueness and target similarity}},
url = {http://arxiv.org/abs/1608.06194},
volume = {145},
year = {2016}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961 http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network ({\ae}net) package. The construction and application of ANN potentials using {\ae}net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite ($\alpha$-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
volume = {114},
year = {2016}
}
@article{Lopez-Bezanilla2014,
abstract = {We present a machine learning approach to solve electronic quantum transport equations of one-dimensional nanostructures. The transmission coefficients of disordered systems were computed to provide training and test data sets to the machine. The system's representation encodes energetic as well as geometrical information to characterize similarities between disordered configurations, while the Euclidean norm is used as a measure of similarity. Errors for out-of-sample predictions systematically decrease with training set size, enabling the accurate and fast prediction of new transmission coefficients. The remarkable performance of our model to capture the complexity of interference phenomena lends further support to its viability in dealing with transport problems of undulatory nature. {\textcopyright} 2014 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1401.8277},
author = {Lopez-Bezanilla, Alejandro and {Von Lilienfeld}, O. Anatole},
doi = {10.1103/PhysRevB.89.235411},
eprint = {1401.8277},
issn = {1550235X},
journal = {Physical Review B - Condensed Matter and Materials Physics},
month = {jun},
number = {23},
pages = {235411},
title = {{Modeling electronic quantum transport with machine learning}},
url = {http://arxiv.org/abs/1401.8277%0Ahttp://dx.doi.org/10.1103/PhysRevB.89.235411 http://arxiv.org/abs/1401.8277 http://dx.doi.org/10.1103/PhysRevB.89.235411 https://link.aps.org/doi/10.1103/PhysRevB.89.235411},
volume = {89},
year = {2014}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
issn = {23318422},
journal = {arXiv},
month = {jul},
pages = {1--12},
title = {{Proximal policy optimization algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@inproceedings{ciresan2011flexibles,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.},
author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
isbn = {9781577355120},
issn = {10450823},
number = {1},
pages = {1237--1242},
title = {{Flexible, high performance convolutional neural networks for image classification}},
volume = {22},
year = {2011}
}
@article{Santoro2002,
abstract = {Probing the lowest energy configuration of a complex system by quantum annealing was recently found to be more effective than its classical, thermal counterpart. By comparing classical and quantum Monte Carlo annealing protocols on the two-dimensional random Ising model (a prototype spin glass), we confirm the superiority of quantum annealing relative to classical annealing. We also propose a theory of quantum annealing based on cascade of Landau-Zener tunneling events. For both classical and quantum annealing, the residual energy after annealing is inversely proportional to a power of the logarithm of the annealing time, but the quantum case has a larger power that makes it faster.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0205280},
author = {Santoro, Giuseppe E. and Martoň{\'{a}}k, Roman and Tosatti, Erio and Car, Robert},
doi = {10.1126/science.1068774},
eprint = {0205280},
issn = {00368075},
journal = {Science},
month = {mar},
number = {5564},
pages = {2427--2430},
pmid = {11923532},
primaryClass = {cond-mat},
title = {{Theory of quantum annealing of an Ising spin glass}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1068774},
volume = {295},
year = {2002}
}
@article{Choubisa2020,
abstract = {Mapping materials science problems onto computational frameworks suitable for machine learning can accelerate materials discovery. Combining proposed crystal site feature embedding (CSFE) representation with convolutional and extensive deep neural networks, we achieve a low mean absolute test error of 3.7 meV/atom and 0.069 eV on density functional theory energies and band gaps of mixed halide perovskites. We explore how a small amount of cadmium doping can potentially be applied in solar cell design and sample the large chemical space by using a variational autoencoder to discover interesting perovskites with band gaps in the ultraviolet and infrared. Additionally, we use CSFE to explore chemical spaces and small doping concentrations beyond those used for training. We further show that CSFE has a mean absolute test error of 7 meV/atom and 0.13 eV for total energies and band gaps for 2D perovskites and discuss its adaptability for exploration of an even wider variety of chemical systems. Density functional theory (DFT) is of interest in modern-day materials discovery. However, DFT is computationally expensive. Here, we develop a new crystal site feature embedding (CSFE) representation that achieves low error in predicting DFT properties and enables predicting properties of chemical families and doping fractions beyond those present in the training datasets. Using CSFE with autoencoders, we present a scheme that enables sampling of large chemical spaces and offers insight into key semiconductor parameters such as band gap. We demonstrate that CSFE works on both 2D and 3D perovskites and identify promising ultraviolet and infrared candidate materials. Here, we report crystal site feature embedding (CSFE), a representation for machine learning of materials that achieves low mean absolute errors for density functional theory band gaps and formation energies. Using CSFE with CNNs and EDNNs, we explored chemical families and doping fractions beyond those present in the training dataset. CSFE allowed us to sample large chemical spaces for materials of interest using autoencoders. We demonstrate the application of the representation by finding perovskite compositions for the ultraviolet and infrared.},
author = {Choubisa, Hitarth and Askerka, Mikhail and Ryczko, Kevin and Voznyy, Oleksandr and Mills, Kyle and Tamblyn, Isaac and Sargent, Edward H.},
doi = {10.1016/j.matt.2020.04.016},
file = {::},
issn = {25902385},
journal = {Matter},
keywords = {MAP3: Understanding,auto-encoders,convolutional neural networks,density functional theory,extensive deep neural networks,halide perovskites,machine learning,materials discovery,optoelectronic materials,photovoltaics},
month = {aug},
number = {2},
pages = {433--448},
publisher = {Cell Press},
title = {{Crystal Site Feature Embedding Enables Exploration of Large Chemical Spaces}},
url = {https://doi.org/10.1016/j.matt.2020.04.016},
volume = {3},
year = {2020}
}
@article{Kadowaki1998,
abstract = {We introduce quantum fluctuations into the simulated annealing process of optimization problems, aiming at faster convergence to the optimal state. Quantum fluctuations cause transitions between states and thus play the same role as thermal fluctuations in the conventional approach. The idea is tested by the transverse Ising model, in which the transverse field is a function of time similar to the temperature in the conventional method. The goal is to find the ground state of the diagonal part of the Hamiltonian with high accuracy as quickly as possible. We have solved the time-dependent Schr{\"{o}}dinger equation numerically for small size systems with various exchange interactions. Comparison with the results of the corresponding classical (thermal) method reveals that the quantum annealing leads to the ground state with much larger probability in almost all cases if we use the same annealing schedule. {\textcopyright} 1998 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/9804280},
author = {Kadowaki, Tadashi and Nishimori, Hidetoshi},
doi = {10.1103/PhysRevE.58.5355},
eprint = {9804280},
issn = {1063651X},
journal = {Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
number = {5},
pages = {5355--5363},
primaryClass = {cond-mat},
title = {{Quantum annealing in the transverse Ising model}},
volume = {58},
year = {1998}
}
@article{Castro2000,
abstract = {In 1989 Hornik as well as Funahashi established that multilayer feedforward networks without the squashing function in the output layer are universal approximators. This result has been often used improperly because it has been applied to multilayer feedforward networks with the squashing function in the output layer. In this paper, we will prove that also this kind of neural networks are universal approximators, i.e. they are capable of approximating any Borel measurable function from one finite dimensional space into (0,1)(n) to any desired degree of accuracy, provided sufficiently many hidden units are available. (C) 2000 Elsevier Science Ltd.},
author = {Castro, J. L. and Mantas, C. J. and Ben{\'{i}}tez, J. M.},
doi = {10.1016/S0893-6080(00)00031-9},
issn = {08936080},
journal = {Neural Networks},
keywords = {Continuous functions,Feedforward networks,Squashing functions,Universal approximation},
number = {6},
pages = {561--563},
pmid = {10987509},
title = {{Neural networks with a continuous squashing function in the output are universal approximators}},
volume = {13},
year = {2000}
}
@article{Takigawa2016,
abstract = {The d-band center for metals has been widely used in order to understand activity trends in metal-surface-catalyzed reactions in terms of the linear Br{\o}nsted-Evans-Polanyi relation and Hammer-N{\o}rskov d-band model. In this paper, the d-band centers for eleven metals (Fe, Co, Ni, Cu, Ru, Rh, Pd, Ag, Ir, Pt, Au) and their pairwise bimetals for two different structures (1% metal doped- or overlayer-covered metal surfaces) are statistically predicted using machine learning methods from readily available values as descriptors for the target metals (such as the density and the enthalpy of fusion of each metal). The predictive accuracy of four regression methods with different numbers of descriptors and different test-set/training-set ratios are quantitatively evaluated using statistical cross validations. It is shown that the d-band centers are reasonably well predicted by the gradient boosting regression (GBR) method with only six descriptors, even when we predict 75% of the data from only 25% given for training (average root mean square error (RMSE) < 0.5 eV). This demonstrates a potential use of machine learning methods for predicting the activity trends of metal surfaces with a negligible CPU time compared to first-principles methods.},
author = {Takigawa, Ichigaku and Shimizu, Ken Ichi and Tsuda, Koji and Takakusagi, Satoru},
doi = {10.1039/c6ra04345c},
issn = {20462069},
journal = {RSC Advances},
number = {58},
pages = {52587--52595},
publisher = {Royal Society of Chemistry},
title = {{Machine-learning prediction of the d-band center for metals and bimetals}},
url = {http://xlink.rsc.org/?DOI=C6RA04345C},
volume = {6},
year = {2016}
}
@article{Behler2007,
abstract = {The accurate description of chemical processes often requires the use of computationally demanding methods like density-functional theory (DFT), making long simulations of large systems unfeasible. In this Letter we introduce a new kind of neural-network representation of DFT potential-energy surfaces, which provides the energy and forces as a function of all atomic positions in systems of arbitrary size and is several orders of magnitude faster than DFT. The high accuracy of the method is demonstrated for bulk silicon and compared with empirical potentials and DFT. The method is general and can be applied to all types of periodic and nonperiodic systems. {\textcopyright} 2007 The American Physical Society.},
author = {Behler, J{\"{o}}rg and Parrinello, Michele},
doi = {10.1103/PhysRevLett.98.146401},
isbn = {0031-9007},
issn = {00319007},
journal = {Physical Review Letters},
number = {14},
pages = {1--4},
pmid = {17501293},
title = {{Generalized neural-network representation of high-dimensional potential-energy surfaces}},
volume = {98},
year = {2007}
}
@article{Okada2019,
abstract = {The Potts model is a generalization of the Ising model with Q > 2 components. In the fully connected ferromagnetic Potts model, a first-order phase transition is induced by varying thermal fluctuations. Therefore, the computational time required to obtain the ground states by simulated annealing exponentially increases with the system size. This study analytically confirms that the transverse magnetic-field quantum annealing induces a first-order phase transition. This result implies that quantum annealing does not exponentially accelerate the ground-state search of the ferromagnetic Potts model. To avoid the first-order phase transition, we propose an iterative optimization method using a half-hot constraint that is applicable to both quantum and simulated annealing. In the limit of Q → ∞, a saddle point equation under the half-hot constraint is identical to the equation describing the behavior of the fully connected ferromagnetic Ising model, thus confirming a second-order phase transition. Furthermore, we verify the same relation between the fully connected Potts glass model and the Sherrington-Kirkpatrick model under assumptions of static approximation and replica symmetric solution. The proposed method is expected to obtain low-energy states of the Potts models with high efficiency using Ising-type computers such as the D-Wave quantum annealer and the Fujitsu Digital Annealer.},
archivePrefix = {arXiv},
arxivId = {1904.01522},
author = {Okada, Shuntaro and Ohzeki, Masayuki and Tanaka, Kazuyuki},
eprint = {1904.01522},
issn = {23318422},
journal = {arXiv},
month = {apr},
title = {{Efficient quantum and simulated annealing of Potts models using a half-hot constraint}},
url = {http://arxiv.org/abs/1904.01522},
year = {2019}
}
@article{Hornak2006,
abstract = {The ff94 force field that is commonly associated with the Amber simulation package is one of the most widely used parameter sets for biomolecular simulation. After a decade of extensive use and testing, limitations in this force field, such as over-stabilization of $\alpha$-helices, were reported by us and other researchers. This led to a number of attempts to improve these parameters, resulting in a variety of "Amber" force fields and significant difficulty in determining which should be used for a particular application. We show that several of these continue to suffer from inadequate balance between different secondary structure elements. In addition, the approach used in most of these studies neglected to account for the existence in Amber of two sets of backbone $\phi$/$\psi$ dihedral terms. This led to parameter sets that provide unreasonable conformational preferences for glycine. We report here an effort to improve the $\phi$/$\psi$ dihedral terms in the ff99 energy function. Dihedral term parameters are based on fitting the energies of multiple conformations of glycine and alanine tetrapeptides from high level ab initio quantum mechanical calculations. The new parameters for backbone dihedrals replace those in the existing ff99 force field. This parameter set, which we denote ff99SB, achieves a better balance of secondary structure elements as judged by improved distribution of backbone dihedrals for glycine and alanine with respect to PDB survey data. It also accomplishes improved agreement with published experimental data for conformational preferences of short alanine peptides and better accord with experimental NMR relaxation data of test protein systems. {\textcopyright} 2006 Wiley-Liss, Inc.},
archivePrefix = {arXiv},
arxivId = {q-bio/0605018},
author = {Hornak, Viktor and Abel, Robert and Okur, Asim and Strockbine, Bentley and Roitberg, Adrian and Simmerling, Carlos},
doi = {10.1002/prot.21123},
eprint = {0605018},
isbn = {0887-3585},
issn = {08873585},
journal = {Proteins: Structure, Function and Genetics},
keywords = {Decoy analysis,Dihedral parameters,Molecular dynamics,Molecular mechanics,NMR order parameters,Trialanine,$\alpha$-helix},
month = {nov},
number = {3},
pages = {712--725},
pmid = {16981200},
primaryClass = {q-bio},
title = {{Comparison of multiple amber force fields and development of improved protein backbone parameters}},
url = {http://doi.wiley.com/10.1002/prot.21123 http://arxiv.org/abs/q-bio/0605018},
volume = {65},
year = {2006}
}
@article{Baldi2015,
abstract = {The Higgs boson is thought to provide the interaction that imparts mass to the fundamental fermions, but while measurements at the Large Hadron Collider (LHC) are consistent with this hypothesis, current analysis techniques lack the statistical power to cross the traditional 5$\sigma$ significance barrier without more data. Deep learning techniques have the potential to increase the statistical power of this analysis by automatically learning complex, high-level data representations. In this work, deep neural networks are used to detect the decay of the Higgs boson to a pair of tau leptons. A Bayesian optimization algorithm is used to tune the network architecture and training algorithm hyperparameters, resulting in a deep network of eight nonlinear processing layers that improves upon the performance of shallow classifiers even without the use of features specifically engineered by physicists for this application. The improvement in discovery significance is equivalent to an increase in the accumulated data set of 25%.},
archivePrefix = {arXiv},
arxivId = {1410.3469},
author = {Baldi, P. and Sadowski, P. and Whiteson, D.},
doi = {10.1103/PhysRevLett.114.111801},
eprint = {1410.3469},
issn = {10797114},
journal = {Physical Review Letters},
number = {11},
pages = {1--5},
pmid = {25839260},
title = {{Enhanced Higgs boson to $\tau$+$\tau$- search with deep learning}},
volume = {114},
year = {2015}
}
@article{Weiner1984,
abstract = {We present the development of a force field for simulation of nucleic acids and proteins. Our approach began by obtaining equilibrium bond lengths and angles from microwave, neutron diffraction, and prior molecular mechanical calculations, torsional constants from microwave, NMR, and molecular mechanical studies, nonbonded parameters from crystal packing calculations, and atomic charges from the fit of a partial charge model to electrostatic potentials calculated by ab initio quantum mechanical theory. The parameters were then refined with molecular mechanical studies on the structures and energies of model compounds. For nucleic acids, we focused on methyl ethyl ether, tetrahydrofuran, deoxyadenosine, dimethyl phosphate, 9-methylguanine-1-methylcytosine hydrogen-bonded complex, 9-methyladenine-1-methylthymine hydrogen-bonded complex, and 1,3-dimethyluracil base-stacked dimer. Bond, angle, torsional, nonbonded, and hydrogen-bond parameters were varied to optimize the agreement between calculated and experimental values for sugar pucker energies and structures, vibrational frequencies of dimethyl phosphate and tetrahydrofuran, and energies for base pairing and base stacking. For proteins, we focused on ϕ,ѱ maps of glycyl and alanyl dipeptides, hydrogen-bonding interactions involving the various protein polar groups, and energy refinement calculations on insulin. Unlike the models for hydrogen bonding involving nitrogen and oxygen electron donors, an adequate description of sulfur hydrogen bonding required explicit inclusion of lone pairs. {\textcopyright} 1984, American Chemical Society. All rights reserved.},
author = {Weiner, Scott J. and Kollman, Peter A. and Singh, U. Chandra and Case, David A. and Ghio, Caterina and Alagona, Giuliano and Profeta, Salvatore and Weiner, Paul},
doi = {10.1021/ja00315a051},
isbn = {0002-7863},
issn = {15205126},
journal = {Journal of the American Chemical Society},
month = {feb},
number = {3},
pages = {765--784},
title = {{A New Force Field for Molecular Mechanical Simulation of Nucleic Acids and Proteins}},
url = {http://pubs.acs.org/doi/abs/10.1021/ja00315a051},
volume = {106},
year = {1984}
}
@article{Babin2012,
abstract = {A full-dimensional model of water, HBB2-pol, derived entirely from "first-principles", is introduced and employed in computer simulations ranging from the dimer to the liquid. HBB2-pol provides excellent agreement with the measured second and third virial coefficients and, by construction, reproduces the dimer vibration-rotation-tunneling spectrum. The model also predicts the relative energy differences between isomers of small water clusters within the accuracy of highly correlated electronic structure methods. Importantly, when combined with simulation methods that explicitly include zero-point energy and quantum thermal motion, HBB2-pol accurately describes both structural and dynamical properties of the liquid phase. {\textcopyright} 2012 American Chemical Society.},
archivePrefix = {arXiv},
arxivId = {1210.7022},
author = {Babin, Volodymyr and Medders, Gregory R. and Paesani, Francesco},
doi = {10.1021/jz3017733},
eprint = {1210.7022},
issn = {19487185},
journal = {Journal of Physical Chemistry Letters},
keywords = {General Theory,Molecular Structure,Quantum Chemistry},
month = {dec},
number = {24},
pages = {3765--3769},
title = {{Toward a universal water model: First principles simulations from the dimer to the liquid phase}},
url = {http://pubs.acs.org/doi/abs/10.1021/jz3017733},
volume = {3},
year = {2012}
}
@article{Kohn1995,
abstract = {The standard Kohn‐Sham formulation of density functional theory (DFT) is limited, for practical reasons, to systems of less than about 50‐100 atoms. The computational effort scales as N at$\alpha$, where Nat is the number of atoms and 2 < $\alpha$ > 3. (By comparison, conventional configuration interaction methods are limited to 5‐10 atom systems.) This article deals with the prospect of practical methods that scale linearly in Nat and may thus allow calculations for systems of 103‐104 atoms. The physical reason (“near‐sightedness”) for linear scaling is presented. Implementations of linear scaling DFT by the use of generalized Wannier functions or the one‐particle density matrix are discussed. {\textcopyright} 1995 John Wiley & Sons, Inc. Copyright {\textcopyright} 1995 John Wiley & Sons, Inc.},
author = {Kohn, W.},
doi = {10.1002/qua.560560407},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
month = {nov},
number = {4},
pages = {229--232},
title = {{Density functional theory for systems of very many atoms}},
url = {http://doi.wiley.com/10.1002/qua.560560407},
volume = {56},
year = {1995}
}
@article{Finnila1994,
abstract = {Quantum annealing is a new method for finding extrema of multidimensional functions. Based on an extension of classical, simulated annealing, this approach appears robust with respect to avoiding local minima. Further, unlike some of its predecessors, it does not require an approximation to a wavefunction. We apply the technique to the problem of finding the lowest energy configurations of Lennard-Jones clusters of up to 19 particles (roughly 105 local minima). This early success suggests that this method may complement the widely implemented technique of simulated annealing. {\textcopyright} 1994.},
archivePrefix = {arXiv},
arxivId = {chem-ph/9404003},
author = {Finnila, A. B. and Gomez, M. A. and Sebenik, C. and Stenson, C. and Doll, J. D.},
doi = {10.1016/0009-2614(94)00117-0},
eprint = {9404003},
issn = {00092614},
journal = {Chemical Physics Letters},
month = {mar},
number = {5-6},
pages = {343--348},
primaryClass = {chem-ph},
title = {{Quantum annealing: A new method for minimizing multidimensional functions}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0009261494001170},
volume = {219},
year = {1994}
}
@incollection{6302929,
abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
booktitle = {Readings in Cognitive Science: A Perspective from Psychology and Artificial Intelligence},
doi = {10.1016/B978-1-4832-1446-7.50035-2},
isbn = {1558600132},
pages = {399--421},
publisher = {MIT Press},
title = {{Learning Internal Representations by Error Propagation}},
url = {https://ieeexplore.ieee.org/document/6302929},
year = {2013}
}
@inproceedings{schulman2015trust,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.05477},
isbn = {9781510810587},
pages = {1889--1897},
title = {{Trust region policy optimization}},
volume = {3},
year = {2015}
}
@article{HohenbergKohn,
abstract = {This paper deals with the ground state of an interacting electron gas in an external potential v(r). It is proved that there exists a universal functional of the density, F[n(r)], independent of v(r), such that the expression Ev(r)n(r)dr+F[n(r)] has as its minimum value the correct ground-state energy associated with v(r). The functional F[n(r)] is then discussed for two situations: (1) n(r)=n0+n(r), nn01, and (2) n(r)=(rr0) with arbitrary and r0→. In both cases F can be expressed entirely in terms of the correlation energy and linear and higher order electronic polarizabilities of a uniform electron gas. This approach also sheds some light on generalized Thomas-Fermi methods and their limitations. Some new extensions of these methods are presented. {\textcopyright} 1964 The American Physical Society.},
author = {Hohenberg, P. and Kohn, W.},
doi = {10.1103/PhysRev.136.B864},
isbn = {0163-1829},
issn = {0031899X},
journal = {Physical Review},
month = {nov},
number = {3B},
pages = {B864--B871},
pmid = {14995397},
title = {{Inhomogeneous electron gas}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.7.1912 https://link.aps.org/doi/10.1103/PhysRev.136.B864},
volume = {136},
year = {1964}
}
@article{Zhu2017a,
abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.},
archivePrefix = {arXiv},
arxivId = {1709.01907},
author = {Zhu, Lingxue and Laptev, Nikolay},
doi = {10.1109/ICDMW.2017.19},
eprint = {1709.01907},
file = {::},
isbn = {9781538614808},
issn = {23759259},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Anomaly detection,Bayesian neural networks,Predictive uncertainty,Time series},
month = {sep},
pages = {103--110},
publisher = {IEEE Computer Society},
title = {{Deep and Confident Prediction for Time Series at Uber}},
url = {http://arxiv.org/abs/1709.01907 http://dx.doi.org/10.1109/ICDMW.2017.19},
volume = {2017-Novem},
year = {2017}
}
@article{Krzywinski2013,
abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
author = {Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.2613},
isbn = {1548-7091$\$n1548-7105},
issn = {15487091},
journal = {Nature Methods},
number = {9},
pages = {809--810},
pmid = {24143821},
publisher = {Nature Publishing Group},
title = {{Points of significance: Importance of being uncertain}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.2613},
volume = {10},
year = {2013}
}
@article{Farhi2019,
abstract = {The Quantum Approximate Optimization Algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization problems whose performance can only improve with the number of layers p. While QAOA holds promise as an algorithm that can be run on near-term quantum computers, its computational power has not been fully explored. In this work, we study the QAOA applied to the Sherrington-Kirkpatrick (SK) model, which can be understood as energy minimization of n spins with all-to-all random signed couplings. There is a recent classical algorithm [9] that can efficiently find an approximate solution for a typical instance of the SK model to within (1 − $\epsilon$) times the ground state energy, so we can only hope to match its performance with the QAOA. Our main result is a novel technique that allows us to evaluate the typical-instance energy of the QAOA applied to the SK model. We produce a formula for the expected value of the energy, as a function of the 2p QAOA parameters, in the infinite size limit that can be evaluated on a computer with O(16p) complexity. We found optimal parameters up to p = 8 running on a laptop. Moreover, we show concentration: With probability tending to one as n → ∞, measurements of the QAOA will produce strings whose energies concentrate at our calculated value. As an algorithm running on a quantum computer, there is no need to search for optimal parameters on an instance-by-instance basis since we can determine them in advance. What we have here is a new framework for analyzing the QAOA, and our techniques can be of broad interest for evaluating its performance on more general problems.},
archivePrefix = {arXiv},
arxivId = {1910.08187},
author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam and Zhou, Leo},
eprint = {1910.08187},
issn = {23318422},
journal = {arXiv},
pages = {1--31},
title = {{The quantum approximate optimization algorithm and the sherrington-kirkpatrick model at infinite size}},
url = {http://arxiv.org/abs/1910.08187},
year = {2019}
}
@misc{Junger,
author = {{University of Cologne}},
title = {{Spin Glass Server}},
url = {http://www.informatik.uni-koeln.de/spinglass/}
}
@book{VanLaarhoven1987,
abstract = {It isn't that they can't see the solution. It is Approach your problems from the right end and begin with the answers. Then one day, that they can't see the problem. perhaps you will find the final question. O. K. Chesterton. The Scandal of Father 'The Hermit Clad in Crane Feathers' in R. Brown 'The point of a Pin'. van Oulik's The Chinese Maze Murders. Growing specialization and diversification have brought a host of monographs and textbooks or increasingly specialized topics. However, the "tree" of knowledg$\sim$ of mathematics and related fields does not grow only by putting forth new branches. It also {\textperiodcentered}happens, quite often in fact, that branches which were thought to be completely disparate are suddenly seen to be related. Further, the $\sim$d and level of sophistication of mathematics applied in various sciences has changed drastically in recent years: measure theory is used (non-trivially) in regional and theoretical economics; algebraic geometry interacts with physics; the Minkowsky lemma, coding theory and the structure of water meet one another in packing and covering theory; quantum fields, crystal defects and mathematical programming profit from homotopy theory; Lie algebras are relevant to filtering; and prediction and electrical engineering can use Stein spaces. And in addition to this there are such new emerging subdisciplines as "experimental mathematics", "CFD", "completely integrable systems", "chaos, synergetics and large-scale order", which are almost impossible to fit into the existing classification schemes. They draw upon widely different sections of mathematics.},
address = {Dordrecht},
author = {van Laarhoven, Peter J. M. and Aarts, Emile H. L.},
booktitle = {Simulated Annealing: Theory and Applications},
doi = {10.1007/978-94-015-7744-1},
isbn = {978-90-481-8438-5},
publisher = {Springer Netherlands},
title = {{Simulated Annealing: Theory and Applications}},
url = {http://link.springer.com/10.1007/978-94-015-7744-1},
year = {1987}
}
@article{Mills2017,
abstract = {We have trained a deep (convolutional) neural network to predict the ground-state energy of an electron in four classes of confining two-dimensional electrostatic potentials. On randomly generated potentials, for which there is no analytic form for either the potential or the ground-state energy, the model was able to predict the ground-state energy to within chemical accuracy, with a median absolute error of 1.49 mHa. We also investigated the performance of the model in predicting other quantities such as the kinetic energy and the first excited-state energy.},
archivePrefix = {arXiv},
arxivId = {1702.01361},
author = {Mills, Kyle and Spanner, Michael and Tamblyn, Isaac},
doi = {10.1103/PhysRevA.96.042113},
eprint = {1702.01361},
issn = {24699934},
journal = {Physical Review A},
month = {oct},
number = {4},
pages = {042113},
title = {{Deep learning and the Schr{\"{o}}dinger equation}},
url = {http://arxiv.org/abs/1702.01361 https://link.aps.org/doi/10.1103/PhysRevA.96.042113},
volume = {96},
year = {2017}
}
@article{Izotov2011,
abstract = {Using a convolutional neural network as an example, we discuss specific aspects of implementing a learning algorithm of pattern recognition on the GPU graphics card using NVIDIA CUDA architecture. The training time of the neural network on a video-adapter is decreased by a factor of 5.96 and the recognition time of a test set is decreased by a factor of 8.76 when compared with the implementation of an optimized algorithm on a central processing unit (CPU). We show that the implementation of the neural network algorithms on graphics processors holds promise. {\textcopyright} 2011 Allerton Press, Inc.},
author = {Izotov, P. Yu and Kazanskiy, N. L. and Golovashkin, D. L. and Sukhanov, S. V.},
doi = {10.3103/S1060992X11020032},
issn = {1060992X},
journal = {Optical Memory and Neural Networks (Information Optics)},
keywords = {CUBLAS,CUDA,GPU,NVIDIA,back propagation of error (BPE) method,convolutional neural network,multiplication of matrices,neural network learning,parallel computing,pattern recognition},
number = {2},
pages = {98--106},
title = {{CUDA-enabled implementation of a neural network algorithm for handwritten digit recognition}},
url = {http://www.springerlink.com/index/10.3103/S1060992X11020032},
volume = {20},
year = {2011}
}
@article{Krizhevsky2012,
abstract = {Abstract To address control difficulties in laser welding, we propose the idea of a self-learning and self-improving laser welding system that combines three modern machine learning techniques. We first show the ability of a deep neural network to extract meaningful, low-dimensional features from high-dimensional laser-welding camera data. These features are then used by a temporal-difference learning algorithm to predict and anticipate important aspects of the system's sensor data. The third part of our proposed architecture suggests using these features and predictions to learn to deliver situation-appropriate welding power; preliminary control results are demonstrated using a laser-welding simulator. The intelligent laser-welding architecture introduced in this work has the capacity to improve its performance without further human assistance and therefore addresses key requirements of modern industry. To our knowledge, it is the first demonstrated combination of deep learning and Nexting with general value functions and also the first usage of deep learning for laser welding specifically and production engineering in general. This work also provides a unique example of how predictions can be explicitly learned using reinforcement learning to support laser welding. We believe that it would be straightforward to adapt our approach to other production engineering applications.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {G{\"{u}}nther, Johannes and Pilarski, Patrick M. and Helfrich, Gerhard and Shen, Hao and Diepold, Klaus},
doi = {10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {22120173},
journal = {Procedia Technology},
keywords = {Deep learning,control,laser welding,prediction,reinforcement learning},
month = {feb},
number = {0},
pages = {474--483},
pmid = {7491034},
title = {{First Steps Towards an Intelligent Laser Welding Architecture Using Deep Neural Networks and Reinforcement Learning}},
url = {http://www.sciencedirect.com/science/article/pii/S2212017314001224 http://arxiv.org/abs/1102.0183},
volume = {15},
year = {2014}
}
@article{Haarnoja2018a,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy; that is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as challenging real-world tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
archivePrefix = {arXiv},
arxivId = {1812.05905},
author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
eprint = {1812.05905},
file = {::},
issn = {23318422},
journal = {arXiv},
month = {dec},
publisher = {arXiv},
title = {{Soft Actor-Critic Algorithms and Applications}},
url = {http://arxiv.org/abs/1812.05905},
year = {2018}
}
@article{ChNg2017,
abstract = {Machine learning offers an unprecedented perspective for the problem of classifying phases in condensed matter physics. We employ neural-network machine learning techniques to distinguish finite-temperature phases of the strongly correlated fermions on cubic lattices. We show that a three-dimensional convolutional network trained on auxiliary field configurations produced by quantum Monte Carlo simulations of the Hubbard model can correctly predict the magnetic phase diagram of the model at the average density of one (half filling). We then use the network, trained at half filling, to explore the trend in the transition temperature as the system is doped away from half filling. This transfer learning approach predicts that the instability to the magnetic phase extends to at least 5% doping in this region. Our results pave the way for other machine learning applications in correlated quantum many-body systems.},
archivePrefix = {arXiv},
arxivId = {1609.02552},
author = {Ch'Ng, Kelvin and Carrasquilla, Juan and Melko, Roger G. and Khatami, Ehsan},
doi = {10.1103/PhysRevX.7.031038},
eprint = {1609.02552},
file = {::},
issn = {21603308},
journal = {Physical Review X},
month = {aug},
number = {3},
pages = {031038},
publisher = {American Physical Society},
title = {{Machine learning phases of strongly correlated fermions}},
url = {https://journals-aps-org.uproxy.library.dc-uoit.ca/prx/abstract/10.1103/PhysRevX.7.031038},
volume = {7},
year = {2017}
}
@article{Boixo2014,
abstract = {Quantum technology is maturing to the point where quantum devices, such as quantum communication systems, quantum random number generators and quantum simulators may be built with capabilities exceeding classical computers. A quantum annealer, in particular, solves optimization problems by evolving a known initial configuration at non-zero temperature towards the ground state of a Hamiltonian encoding a given problem. Here, we present results from tests on a 108 qubit D-Wave One device based on superconducting flux qubits. By studying correlations we find that the device performance is inconsistent with classical annealing or that it is governed by classical spin dynamics. In contrast, we find that the device correlates well with simulated quantum annealing. We find further evidence for quantum annealing in the form of small-gap avoided level crossings characterizing the hard problems. To assess the computational power of the device we compare it against optimized classical algorithms. {\textcopyright} 2014 Macmillan Publishers Limited.},
archivePrefix = {arXiv},
arxivId = {1304.4595},
author = {Boixo, Sergio and R{\o}nnow, Troels F. and Isakov, Sergei V. and Wang, Zhihui and Wecker, David and Lidar, Daniel A. and Martinis, John M. and Troyer, Matthias},
doi = {10.1038/nphys2900},
eprint = {1304.4595},
issn = {17452481},
journal = {Nature Physics},
number = {3},
pages = {218--224},
title = {{Evidence for quantum annealing with more than one hundred qubits}},
volume = {10},
year = {2014}
}
@techreport{adagrad,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. The adaptation, in essence, allows us to find needles in haystacks in the form of very predictive yet rarely observed features. Our paradigm stems from recent advances in online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies the task of setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We corroborate our theoretical results with experiments on a text classification task, showing substantial improvements for classification with sparse datasets.},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
booktitle = {COLT 2010 - The 23rd Conference on Learning Theory},
file = {::},
isbn = {9780982252925},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {257--269},
title = {{Adaptive subgradient methods for online learning and stochastic optimization}},
volume = {12},
year = {2010}
}
@article{Monterola2003b,
abstract = {We present a practical method for estimating the upper error bound in the neural network (NN) solution of the nonlinear Schr{\"{o}}dinger equation (NLSE) under different degrees of nonlinearity. The error bound is a function of the nonnegative energy E value that is minimized when the NN is trained to solve the NLSE. The form of E is derived from the NLSE expression and the NN solution becomes identical with the true NLSE solution only when the E value is reduced exactly to zero. In practice, machines with finite floating-point range and accuracy are used for training and E is not decreased exactly to zero. Knowledge of the error bound permits the estimation of the maximum average error in the NN solution without prior knowledge of the true NLSE solution - a crucial factor in the practical applications of the NN technique. The error bound is verified for both the linear time - independent Schr{\"{o}}dinger equation for a free particle, and the NLSE. We also discuss the conditions where the error bound formulation is valid. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
author = {Monterola, Christopher and Saloma, Caesar},
doi = {10.1016/S0030-4018(03)01570-0},
issn = {00304018},
journal = {Optics Communications},
keywords = {Neural networks,Nonlinear Schr{\"{o}}dinger equation,Numerical approximation and analysis,Numerical simulation,Solution of equations},
month = {jul},
number = {1-6},
pages = {331--339},
pmid = {19421275},
title = {{Solving the nonlinear Schr{\"{o}}dinger equation with an unsupervised neural network: Estimation of error in solution}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0030401803015700},
volume = {222},
year = {2003}
}
@article{Salimans2016,
abstract = {Recently there has been an interest in the potential of learning generative models from a single image, as opposed to from a large dataset. This task is of practical significance, as it means that generative models can be used in domains where collecting a large dataset is not feasible. However, training a model capable of generating realistic images from only a single sample is a difficult problem. In this work, we conduct a number of experiments to understand the challenges of training these methods and propose some best practices that we found allowed us to generate improved results over previous work in this space. One key piece is that unlike prior single image generation methods, we concurrently train several stages in a sequential multi-stage manner, allowing us to learn models with fewer stages of increasing image resolution. Compared to a recent state of the art baseline, our model is up to six times faster to train, has fewer parameters, and can better capture the global structure of images.},
archivePrefix = {arXiv},
arxivId = {2003.11512},
author = {Hinz, Tobias and Fisher, Matthew and Wang, Oliver and Wermter, Stefan},
eprint = {2003.11512},
issn = {23318422},
journal = {arXiv},
number = {Nips},
pages = {1--9},
title = {{Improved techniques for training single-image GANs}},
year = {2020}
}
@inproceedings{Simard2003,
abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple "do-it-yourself" implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, John C.},
booktitle = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
doi = {10.1109/ICDAR.2003.1227801},
isbn = {0769519601},
issn = {15205363},
keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,Support vector machines,Text analysis},
pages = {958--963},
publisher = {IEEE Comput. Soc},
title = {{Best practices for convolutional neural networks applied to visual document analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1227801},
volume = {2003-Janua},
year = {2003}
}
@article{Artrith2016,
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler-Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network ({\ae}net) package. The construction and application of ANN potentials using {\ae}net is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential's capabilities for the prediction of the high-pressure phases columbite ($\alpha$-PbO2 structure) and baddeleyite (ZrO2 structure).},
author = {Artrith, Nongnuch and Urban, Alexander},
doi = {10.1016/j.commatsci.2015.11.047},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {Artificial neural networks,Atomistic simulations,Behler-Parrinello,Machine learning,Titanium dioxide (TiO2)},
pages = {135--150},
title = {{An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2}},
volume = {114},
year = {2016}
}
@article{VonRueden,
abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process, which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. First, we provide a definition and propose a concept for informed machine learning, which illustrates its building blocks and distinguishes it from conventional machine learning. Second, we introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Third, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.},
archivePrefix = {arXiv},
arxivId = {1903.12394},
author = {{Von Rueden}, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Walczak, Micha{\l} and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
eprint = {1903.12394},
file = {::},
issn = {23318422},
journal = {arXiv},
keywords = {Expert Knowledge,Hybrid,Informed,Machine Learning,Prior Knowledge,Survey},
month = {mar},
title = {{Informed machine learning - a taxonomy and survey of integrating knowledge into learning systems}},
url = {http://arxiv.org/abs/1903.12394},
year = {2019}
}
@article{Khorshidi2016,
abstract = {Electronic structure calculations, such as those employing Kohn–Sham density functional theory or ab initio wavefunction theories, have allowed for atomistic-level understandings of a wide variety of phenomena and properties of matter at small scales. However, the computational cost of electronic structure methods drastically increases with length and time scales, which makes these methods difficult for long time-scale molecular dynamics simulations or large-sized systems. Machine-learning techniques can provide accurate potentials that can match the quality of electronic structure calculations, provided sufficient training data. These potentials can then be used to rapidly simulate large and long time-scale phenomena at similar quality to the parent electronic structure approach. Machine-learning potentials usually take a bias-free mathematical form and can be readily developed for a wide variety of systems. Electronic structure calculations have favorable properties–namely that they are noiseless and targeted training data can be produced on-demand–that make them particularly well-suited for machine learning. This paper discusses our modular approach to atomistic machine learning through the development of the open-source Atomistic Machine-learning Package (Amp), which allows for representations of both the total and atom-centered potential energy surface, in both periodic and non-periodic systems. Potentials developed through the atom-centered approach are simultaneously applicable for systems with various sizes. Interpolation can be enhanced by introducing custom descriptors of the local environment. We demonstrate this in the current work for Gaussian-type, bispectrum, and Zernike-type descriptors. Amp has an intuitive and modular structure with an interface through the python scripting language yet has parallelizable fortran components for demanding tasks; it is designed to integrate closely with the widely used Atomic Simulation Environment (ASE), which makes it compatible with a wide variety of commercial and open-source electronic structure codes. We finally demonstrate that the neural network model inside Amp can accurately interpolate electronic structure energies as well as forces of thousands of multi-species atomic systems. Program summary Program title: Amp Catalogue identifier: AFAK_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AFAK_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: yes No. of lines in distributed program, including test data, etc.: 21239 No. of bytes in distributed program, including test data, etc.: 1412975 Distribution format: tar.gz Programming language: Python, Fortran. Computer: PC, Mac. Operating system: Linux, Mac, Windows. Has the code been vectorized or parallelized?: Yes RAM: Variable, depending on the number and size of atomic systems. Classification: 16.1, 2.1. External routines: ASE, NumPy, SciPy, f2py, matplotlib Nature of problem: Atomic interactions within many-body systems typically have complicated functional forms, difficult to represent in simple pre-decided closed-forms. Solution method: Machine learning provides flexible functional forms that can be improved as new situations are encountered. Typically, interatomic potentials yield from machine learning simultaneously apply to different system sizes. Unusual features: Amp is as modular as possible, providing a framework for the user to create atomic environment descriptor and regression model at will. Moreover, it has Atomic Simulation Environment (ASE) interface, facilitating interactive collaboration with other electronic structure calculators within ASE. Running time: Variable, depending on the number and size of atomic systems.},
author = {Khorshidi, Alireza and Peterson, Andrew A.},
doi = {10.1016/j.cpc.2016.05.010},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Atomic Simulation Environment (ASE),Density functional theory,Neural networks,Potential energy surface,Zernike polynomials},
pages = {310--324},
publisher = {Elsevier B.V.},
title = {{Amp: A modular approach to machine learning in atomistic simulations}},
url = {http://dx.doi.org/10.1016/j.cpc.2016.05.010},
volume = {207},
year = {2016}
}
@techreport{Hintona,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
booktitle = {Neural Computation},
doi = {10.1162/neco.2006.18.7.1527},
file = {::},
issn = {08997667},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@article{Mnih2013,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://arxiv.org/abs/1312.5602},
volume = {518},
year = {2015}
}
@inproceedings{Werbos:81sensitivity,
abstract = {The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting „Sensitivity Analysis Methods for Nonlinear Systems`` from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.},
author = {Werbos, Paul J.},
booktitle = {System Modeling and Optimization},
doi = {10.1007/bfb0006203},
keywords = {juergen},
pages = {762--770},
title = {{Applications of advances in nonlinear sensitivity analysis}},
year = {2005}
}
@article{Equations1965,
abstract = {From a theory of Hohenberg and Kohn, approximation methods for treating an inhomogeneous system of interacting electrons are developed. These methods are exact for systems of slowly varying or high density. For the ground state, they lead to self-consistent equations analogous to the Hartree and Hartree-Fock equations, respectively. In these equations the exchange and correlation portions of the chemical potential of a uniform electron gas appear as additional effective potentials. (The exchange portion of our effective potential differs from that due to Slater by a factor of 23.) Electronic systems at finite temperatures and in magnetic fields are also treated by similar methods. An appendix deals with a further correction for systems with short-wavelength density oscillations. {\textcopyright} 1965 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {10.1103/PhysRev.140.A1133},
author = {Kohn, W. and Sham, L. J.},
doi = {10.1103/PhysRev.140.A1133},
eprint = {PhysRev.140.A1133},
isbn = {9783540373674},
issn = {0031899X},
journal = {Physical Review},
number = {4A},
pmid = {20432011},
primaryClass = {10.1103},
title = {{Self-consistent equations including exchange and correlation effects}},
volume = {140},
year = {1965}
}
@article{Snyder2013a,
abstract = {Using a one-dimensional model, we explore the ability of machine learning to approximate the non-interacting kinetic energy density functional of diatomics. This nonlinear interpolation between Kohn-Sham reference calculations can (i) accurately dissociate a diatomic, (ii) be systematically improved with increased reference data and (iii) generate accurate self-consistent densities via a projection method that avoids directions with no data. With relatively few densities, the error due to the interpolation is smaller than typical errors in standard exchange-correlation functionals. {\textcopyright} 2013 AIP Publishing LLC.},
archivePrefix = {arXiv},
arxivId = {1306.1812},
author = {Snyder, John C. and Rupp, Matthias and Hansen, Katja and Blooston, Leo and M{\"{u}}ller, Klaus Robert and Burke, Kieron},
doi = {10.1063/1.4834075},
eprint = {1306.1812},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {22},
pmid = {24329053},
title = {{Orbital-free bond breaking via machine learning}},
volume = {139},
year = {2013}
}
@article{Schutt2017,
abstract = {Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks, which leads to size-extensive and uniformly accurate (1 kcal mol -1) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the model reveals a classification of aromatic rings with respect to their stability. Further applications of our model for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the potential of machine learning for revealing insights into complex quantum-chemical systems.},
archivePrefix = {arXiv},
arxivId = {1609.08259},
author = {Sch{\"{u}}tt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and M{\"{u}}ller, Klaus R. and Tkatchenko, Alexandre},
doi = {10.1038/ncomms13890},
eprint = {1609.08259},
issn = {20411723},
journal = {Nature Communications},
month = {jan},
pages = {13890},
pmid = {28067221},
title = {{Quantum-chemical insights from deep tensor neural networks}},
url = {http://www.nature.com/doifinder/10.1038/ncomms13890},
volume = {8},
year = {2017}
}
@article{Campbell2002,
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: a single-chip chess search engine, a massively parallel system with multiple levels of parallelism, a strong emphasis on search extensions, a complex evaluation function, and effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue. {\textcopyright} 2001 Elsevier Science B.V. All rights reserved.},
author = {Campbell, Murray and Hoane, A. Joseph and Hsu, Feng Hsiung},
doi = {10.1016/S0004-3702(01)00129-1},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
month = {jan},
number = {1-2},
pages = {57--83},
title = {{Deep Blue}},
url = {http://search2.scholarsportal.info.myaccess.library.utoronto.ca/ids70/view_record.php?id=112&recnum=111&SID=760717fafbf1a1c5a4539da053640d93&mark_id=search%3A112%3A17%2C100%2C150},
volume = {134},
year = {2002}
}
@article{Jetchev2016,
abstract = {Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.},
archivePrefix = {arXiv},
arxivId = {1611.08207},
author = {Jetchev, Nikolay and Bergmann, Urs and Vollgraf, Roland},
eprint = {1611.08207},
month = {nov},
number = {ii},
title = {{Texture Synthesis with Spatial Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.08207},
year = {2016}
}
@article{Kucharski1992,
abstract = {The coupled‐cluster method with multidimensional reference space is studied in the case of the incomplete active space (IAS ). The latter was chosen as a subspace of the Hilbert space corresponding to a fixed number of valence particles. Two different approaches for the normalization condition are analyzed. When not imposing intermediate normalization, the cancellation of disconnected terms is proven, ensuring that extensive energies are obtained. {\textcopyright} 1992 John Wiley & Sons, Inc. Copyright {\textcopyright} 1992 John Wiley & Sons, Inc.},
author = {Kucharski, Stanislaw A. and Bartlett, Rodney J.},
doi = {10.1002/qua.560440810},
isbn = {1097461X},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
month = {mar},
number = {26 S},
pages = {107--115},
title = {{Coupled‐cluster method for an incomplete model space}},
url = {http://doi.wiley.com/10.1002/qua.560440810},
volume = {44},
year = {1992}
}
@article{Salimans2016a,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
doi = {arXiv:1504.01391},
eprint = {1606.03498},
isbn = {0924-6495},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2234--2242},
pmid = {23259955},
title = {{Improved techniques for training GANs}},
url = {http://arxiv.org/abs/1606.03498},
year = {2016}
}
@inproceedings{Szegedy2014,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
keywords = {GoogLeNet},
mendeley-tags = {GoogLeNet},
month = {jun},
pages = {1--9},
pmid = {24920543},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
url = {http://ieeexplore.ieee.org/document/7298594/},
volume = {07-12-June},
year = {2015}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
isbn = {1212.5701},
journal = {arXiv},
month = {dec},
pages = {6},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Ryczko2018a,
abstract = {We introduce a new method, called CNNAS (convolutional neural networks for atomistic systems), for calculating the total energy of atomic systems which rivals the computational cost of empirical potentials while maintaining the accuracy of ab initio calculations. This method uses deep convolutional neural networks (CNNs), where the input to these networks are simple representations of the atomic structure. We use this approach to predict energies obtained using density functional theory (DFT) for 2D hexagonal lattices of various types. Using a dataset consisting of graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures, with and without defects, we trained a deep CNN that is capable of predicting DFT energies to an extremely high accuracy, with a mean absolute error (MAE) of 0.198 meV/atom (maximum absolute error of 16.1 meV/atom). To explore our new methodology, we investigate the ability of a deep neural network (DNN) in predicting a Lennard-Jones energy and separation distance for a dataset of dimer molecules in both two and three dimensions. In addition, we systematically investigate the flexibility of the deep learning models by performing interpolation and extrapolation tests.},
archivePrefix = {arXiv},
arxivId = {1706.09496},
author = {Ryczko, Kevin and Mills, Kyle and Luchak, Iryna and Homenick, Christa and Tamblyn, Isaac},
doi = {10.1016/j.commatsci.2018.03.005},
eprint = {1706.09496},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {2D materials,Convolutional neural networks,Deep learning,Density functional theory,Dimer molecules},
pages = {134--142},
title = {{Convolutional neural networks for atomistic systems}},
volume = {149},
year = {2018}
}
@phdthesis{Waals1873,
abstract = {De gewaarwordingen, die ik ondervind, nu ik geroepen ben dit proefschrift te verdedigen zijn van verschillenden aard. Heeft erkentelijkheid den boventoon jegens een Faculteit, aan welke ik mijn vorming heb te danken, en in het bijzonder jegens U, Hooggeleerde RIJKE, ..},
author = {van der Waals},
pages = {136},
title = {{De continuiteit van den gasen Vloeistoftoestand}},
year = {1873}
}
@article{Oord2016a,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {{Van Den Oord}, A{\"{a}}ron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1606.05328},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {4797--4805},
title = {{Conditional image generation with PixelCNN decoders}},
url = {http://arxiv.org/abs/1606.05328},
year = {2016}
}
@article{Casert2020,
abstract = {Machine learning provides a novel avenue for the study of experimental realizations of many-body systems, and has recently been proven successful in analyzing properties of experimental data of ultracold quantum gases. We here show that deep learning succeeds in the more challenging task of modelling such an experimental data distribution. Our generative model (RUGAN) is able to produce snapshots of a doped two-dimensional Fermi-Hubbard model that are indistinguishable from previously reported experimental realizations. Importantly, it is capable of accurately generating snapshots at conditions for which it did not observe any experimental data, such as at higher doping values. On top of that, our generative model extracts relevant patterns from small-scale examples and can use these to construct new configurations at a larger size that serve as a precursor to observations at scales that are currently experimentally inaccessible. The snapshots created by our model-which come at effectively no cost-are extremely useful as they can be employed to quantitatively test new theoretical developments under conditions that have not been explored experimentally, parameterize phenomenological models, or train other, more data-intensive, machine learning methods. We provide predictions for experimental observables at unobserved conditions and benchmark these against modern theoretical frameworks. The deep learning method we develop here is broadly applicable and can be used for the efficient large-scale simulation of equilibrium and nonequilibrium physical systems.},
archivePrefix = {arXiv},
arxivId = {2002.07055},
author = {Casert, Corneel and Mills, Kyle and Vieijra, Tom and Ryckebusch, Jan and Tamblyn, Isaac},
eprint = {2002.07055},
issn = {23318422},
journal = {arXiv},
pages = {1--11},
title = {{Optical lattice experiments at unobserved conditions and scales through generative adversarial deep learning}},
year = {2020}
}
@article{Mills2017a,
abstract = {We demonstrate the capability of a convolutional deep neural network in predicting the nearest-neighbor energy of the 4×4 Ising model. Using its success at this task, we motivate the study of the larger 8×8 Ising model, showing that the deep neural network can learn the nearest-neighbor Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. We then demonstrate the ability of the neural network to learn other spin models, teaching the convolutional deep neural network to accurately predict the long-range interaction of a screened Coulomb Hamiltonian, a sinusoidally attenuated screened Coulomb Hamiltonian, and a modified Potts model Hamiltonian. In the case of the long-range interaction, we demonstrate the ability of the neural network to recover the phase transition with equivalent accuracy to the numerically exact method. Furthermore, in the case of the long-range interaction, the benefits of the neural network become apparent; it is able to make predictions with a high degree of accuracy, and do so 1600 times faster than a CUDA-optimized exact calculation. Additionally, we demonstrate how the neural network succeeds at these tasks by looking at the weights learned in a simplified demonstration.},
archivePrefix = {arXiv},
arxivId = {1706.09779v1},
author = {Mills, Kyle and Tamblyn, Isaac},
doi = {10.1103/PhysRevE.97.032119},
eprint = {1706.09779v1},
issn = {24700053},
journal = {Physical Review E},
number = {3},
pmid = {29776084},
title = {{Deep neural networks for direct, featureless learning through observation: The case of two-dimensional spin models}},
volume = {97},
year = {2018}
}
@techreport{Huang,
abstract = {Deep learning based image-to-image translation methods aim at learning the joint distribution of the two domains and finding transformations between them. Despite recent GAN (Generative Adversarial Network) based methods have shown compelling results, they are prone to fail at preserving image-objects and maintaining translation consistency, which reduces their practicality on tasks such as generating large-scale training data for different domains. To address this problem, we purpose a structure-aware image-to-image translation network, which is composed of encoders, generators, discriminators and parsing nets for the two domains, respectively, in a unified framework. The purposed network generates more visually plausible images compared to competing methods on different image-translation tasks. In addition, we quantitatively evaluate different methods by training Faster-RCNN and YOLO with datasets generated from the image-translation results and demonstrate significant improvement on the detection accuracies by using the proposed image-object preserving network.},
author = {Huang, Sheng Wei and Lin, Che Tsung and Chen, Shu Ping and Wu, Yen Yi and Hsu, Po Hao and Lai, Shang Hong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01240-3_44},
file = {::},
isbn = {9783030012397},
issn = {16113349},
keywords = {Domain adaptation,Generative adversarial network,Image-to-image translation,Object detection,Semantic segmentation},
pages = {731--744},
title = {{AugGAN: Cross domain adaptation with GAN-based data augmentation}},
volume = {11213 LNCS},
year = {2018}
}
@article{Albarr_n_Arriagada_2020,
abstract = {The characterization of an operator by its eigenvectors and eigenvalues allows us to know its action over any quantum state. Here, we propose a protocol to obtain an approximation of the eigenvectors of an arbitrary Hermitian quantum operator. This protocol is based on measurement and feedback processes, which characterize a reinforcement learning protocol. Our proposal is composed of two systems, a black box named environment and a quantum state named agent. The role of the environment is to change any quantum state by a unitary matrix {\^{U}}E = e−i$\tau${\^{O}}E where {\^{O}}E is a Hermitian operator, and $\tau$ is a real parameter. The agent is a quantum state which adapts to some eigenvector of {\^{O}}E by repeated interactions with the environment, feedback process, and semi-random rotations. With this proposal, we can obtain an approximation of the eigenvectors of a random qubit operator with average fidelity over 90% in less than 10 iterations, and surpass 98% in less than 300 iterations. Moreover, for the two-qubit cases, the four eigenvectors are obtained with fidelities above 89% in 8000 iterations for a random operator, and fidelities of 99% for an operator with the Bell states as eigenvectors. This protocol can be useful to implement semi-autonomous quantum devices which should be capable of extracting information and deciding with minimal resources and without human intervention.},
archivePrefix = {arXiv},
arxivId = {1906.06702},
author = {Albarr{\'{a}}n-Arriagada, F. and Retamal, J. C. and Solano, E. and Lamata, L.},
doi = {10.1088/2632-2153/ab43b4},
eprint = {1906.06702},
issn = {23318422},
journal = {arXiv},
month = {feb},
number = {1},
pages = {15002},
publisher = {{IOP} Publishing},
title = {{Reinforcement learning for semi-autonomous approximate quantum eigensolver}},
url = {https://doi.org/10.1088%2F2632-2153%2Fab43b4},
volume = {1},
year = {2019}
}
@techreport{Kingma,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
file = {::},
title = {{Auto-encoding variational bayes}},
year = {2014}
}
@article{Samek,
abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
archivePrefix = {arXiv},
arxivId = {1708.08296},
author = {Samek, Wojciech and Wiegand, Thomas and M{\"{u}}ller, Klaus Robert},
eprint = {1708.08296},
file = {::},
issn = {23318422},
journal = {arXiv},
keywords = {Black box models,Deep neural networks,Index Terms— Artificial intelligence,Interpretability,Layer-wise relevance propagation,Sensitivity analysis},
month = {aug},
title = {{Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models}},
url = {http://arxiv.org/abs/1708.08296},
year = {2017}
}
@article{Srivastava2017,
abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
archivePrefix = {arXiv},
arxivId = {1705.07761},
author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
eprint = {1705.07761},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {may},
pages = {3309--3319},
title = {{VEEGAN: Reducing mode collapse in GANs using implicit variational learning}},
url = {http://arxiv.org/abs/1705.07761},
volume = {2017-Decem},
year = {2017}
}
@article{Hubel1968,
abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded. 2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive‐field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross‐sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent. 3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other. 4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two‐thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven. 5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers. {\textcopyright} 1968 The Physiological Society},
author = {Hubel, D. H. and Wiesel, T. N.},
doi = {10.1113/jphysiol.1968.sp008455},
isbn = {0022-3077 (Print)\r0022-3077 (Linking)},
issn = {14697793},
journal = {The Journal of Physiology},
keywords = {Classified neurons in area 17 of macaques as simpl,VC MP 102,XXa few color sens.cells found.Columns with cells,monkey,rfhb1},
month = {mar},
number = {1},
pages = {215--243},
pmid = {4966457},
title = {{Receptive fields and functional architecture of monkey striate cortex}},
url = {http://doi.wiley.com/10.1113/jphysiol.1968.sp008455},
volume = {195},
year = {1968}
}
@article{Nguyen2015,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call 'fooling images' (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {427--436},
pmid = {24309266},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
volume = {07-12-June},
year = {2015}
}
@article{Shirvany2008,
abstract = {We present a method to solve boundary value problems using artificial neural networks (ANN). A trial solution of the differential equation is written as a feed-forward neural network containing adjustable parameters (the weights and biases). From the differential equation and its boundary conditions we prepare the energy function which is used in the back-propagation method with momentum term to update the network parameters. We improved energy function of ANN which is derived from Schrodinger equation and the boundary conditions. With this improvement of energy function we can use unsupervised training method in the ANN for solving the equation. Unsupervised training aims to minimize a non-negative energy function. We used the ANN method to solve Schrodinger equation for few quantum systems. Eigenfunctions and energy eigenvalues are calculated. Our numerical results are in agreement with their corresponding analytical solution and show the efficiency of ANN method for solving eigenvalue problems. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Shirvany, Yazdan and Hayati, Mohsen and Moradian, Rostam},
doi = {10.1016/j.cnsns.2007.04.024},
issn = {10075704},
journal = {Communications in Nonlinear Science and Numerical Simulation},
keywords = {Differential equation,Eigenfunction,Eigenvalue,Energy function,Feed-forward neural network,Schrodinger equation},
month = {dec},
number = {10},
pages = {2132--2145},
title = {{Numerical solution of the nonlinear Schrodinger equation by feedforward neural networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1007570407001037},
volume = {13},
year = {2008}
}
@article{Mills2017d,
abstract = {We demonstrate that a generative adversarial network can be trained to produce Ising model configurations in distinct regions of phase space. In training a generative adversarial network, the discriminator neural network becomes very good a discerning examples from the training set and examples from the testing set. We demonstrate that this ability can be used as an “anomaly detector”, producing estimations of operator values along with a confidence in the prediction.},
archivePrefix = {arXiv},
arxivId = {1710.08053},
author = {Mills, Kyle and Tamblyn, Isaac},
eprint = {1710.08053},
file = {::},
issn = {23318422},
journal = {arXiv},
month = {oct},
publisher = {arXiv},
title = {{Phase space sampling and operator confidence with generative adversarial networks}},
url = {http://arxiv.org/abs/1710.08053},
year = {2017}
}
@techreport{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. {\textcopyright} 2012 James Bergstra and Yoshua Bengio.},
author = {Bergstra, James and Bengio, Yoshua},
booktitle = {Journal of Machine Learning Research},
file = {::},
issn = {15324435},
keywords = {Deep learning,Global optimization,Model selection,Neural networks,Response surface modeling},
pages = {281--305},
title = {{Random search for hyper-parameter optimization}},
url = {http://scikit-learn.sourceforge.net.},
volume = {13},
year = {2012}
}
@article{Yao2016,
abstract = {We demonstrate a convolutional neural network trained to reproduce the Kohn-Sham kinetic energy of hydrocarbons from an input electron density. The output of the network is used as a nonlocal correction to conventional local and semilocal kinetic functionals. We show that this approximation qualitatively reproduces Kohn-Sham potential energy surfaces when used with conventional exchange correlation functionals. The density which minimizes the total energy given by the functional is examined in detail. We identify several avenues to improve on this exploratory work, by reducing numerical noise and changing the structure of our functional. Finally we examine the features in the density learned by the neural network to anticipate the prospects of generalizing these models.},
archivePrefix = {arXiv},
arxivId = {1509.00062},
author = {Yao, Kun and Parkhill, John},
doi = {10.1021/acs.jctc.5b01011},
eprint = {1509.00062},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {3},
pages = {1139--1147},
pmid = {26812530},
title = {{Kinetic Energy of Hydrocarbons as a Function of Electron Density and Convolutional Neural Networks}},
volume = {12},
year = {2016}
}
@article{Silver2018,
abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1126/science.aar6404},
issn = {10959203},
journal = {Science},
number = {6419},
pages = {1140--1144},
pmid = {30523106},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
volume = {362},
year = {2018}
}
@article{Heim215,
abstract = {Quantum annealers use quantum fluctuations to escape local minima and find low-energy configurations of a physical system. Strong evidence for superiority of quantum annealing (QA) has come from comparing QA implemented through quantum Monte Carlo (QMC) simulations to classical annealing. Motivated by recent experiments, we revisit the question of when quantum speedup may be expected. Although a better scaling is seen for QA in two-dimensional Ising spin glasses, this advantage is due to time discretization artifacts and measurements that are not possible on a physical quantum annealer. Simulations in the physically relevant continuous time limit, on the other hand, do not show superiority. Our results imply that care must be taken when using QMC simulations to assess the potential for quantum speedup.},
archivePrefix = {arXiv},
arxivId = {1411.5693},
author = {Heim, Bettina and R{\o}nnow, Troels F. and Isakov, Sergei V. and Troyer, Matthias},
doi = {10.1126/science.aaa4170},
eprint = {1411.5693},
issn = {10959203},
journal = {Science},
number = {6231},
pages = {215--217},
publisher = {American Association for the Advancement of Science},
title = {{Quantum versus classical annealing of Ising spin glasses}},
url = {https://science.sciencemag.org/content/348/6231/215},
volume = {348},
year = {2015}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchange-correlation functionals, PBE and RPBE, as reference methods. We find that the binding energy errors of the NN potentials with respect to DFT are significantly lower than the typical uncertainties of DFT calculations arising from the choice of the exchange-correlation functional. Further, we examine the role of van der Waals interactions, which are not properly described by GGA functionals. Specifically, we incorporate the D3 scheme suggested by Grimme (J. Chem. Phys. 2010, 132, 154104) in our potentials and demonstrate that it can be applied to GGA-based NN potentials in the same way as to DFT calculations without modification. Our results show that the description of small water clusters provided by the RPBE functional is significantly improved if van der Waals interactions are included, while in case of the PBE functional, which is well-known to yield stronger binding than RPBE, van der Waals corrections lead to overestimated binding energies. {\textcopyright} 2013 American Chemical Society.},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
volume = {117},
year = {2013}
}
@inproceedings{Raina2009,
abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recentlybeen applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods. Copyright 2009.},
address = {New York, New York, USA},
author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1553374.1553486},
isbn = {9781605585161},
issn = {12258687},
pages = {873--880},
pmid = {17394762},
publisher = {ACM Press},
title = {{Large-scale deep unsupervised learning using graphics processors}},
url = {http://dl.acm.org/citation.cfm?id=1553374.1553486 http://portal.acm.org/citation.cfm?doid=1553374.1553486},
volume = {382},
year = {2009}
}
@misc{Casert2020a,
abstract = {Machine learning provides a novel avenue for the study of experimental realizations of many-body systems, and has recently been proven successful in analyzing properties of experimental data of ultracold quantum gases. We here show that deep learning succeeds in the more challenging task of modelling such an experimental data distribution. Our generative model (RUGAN) is able to produce snapshots of a doped two-dimensional Fermi-Hubbard model that are indistinguishable from previously reported experimental realizations. Importantly, it is capable of accurately generating snapshots at conditions for which it did not observe any experimental data, such as at higher doping values. On top of that, our generative model extracts relevant patterns from small-scale examples and can use these to construct new configurations at a larger size that serve as a precursor to observations at scales that are currently experimentally inaccessible. The snapshots created by our model-which come at effectively no cost-are extremely useful as they can be employed to quantitatively test new theoretical developments under conditions that have not been explored experimentally, parameterize phenomenological models, or train other, more data-intensive, machine learning methods. We provide predictions for experimental observables at unobserved conditions and benchmark these against modern theoretical frameworks. The deep learning method we develop here is broadly applicable and can be used for the efficient large-scale simulation of equilibrium and nonequilibrium physical systems.},
archivePrefix = {arXiv},
arxivId = {2002.07055},
author = {Casert, Corneel and Mills, Kyle and Vieijra, Tom and Ryckebusch, Jan and Tamblyn, Isaac},
booktitle = {arXiv},
eprint = {2002.07055},
issn = {23318422},
title = {{Optical lattice experiments at unobserved conditions and scales through generative adversarial deep learning}},
year = {2020}
}
@article{McGeoch2013,
abstract = {This paper describes an experimental study of a novel computing system (algorithm plus platform) that carries out quantum annealing, a type of adiabatic quantum computation, to solve optimization problems. We compare this system to three conventional software solvers, using instances from three NP-hard problem domains. We also describe experiments to learn how performance of the quantum annealing algorithm depends on input. Copyright 2013 ACM.},
author = {McGeoch, Catherine C. and Wang, Cong},
doi = {10.1145/2482767.2482797},
isbn = {9781450320535},
journal = {Proceedings of the ACM International Conference on Computing Frontiers, CF 2013},
keywords = {Adiabatic quantum computing,D-Wave,Heuristics,Quantum annealing},
title = {{Experimental evaluation of an adiabiatic quantum system for combinatorial optimization}},
year = {2013}
}
@article{Bergmann2017,
abstract = {This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014), and call this technique Periodic Spatial GAN (PS-GAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures, periodic or non-periodic, from datasets of one or more complex large images. Second, we show that the image generation with PS-GANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources, and the method is highly scalable and can generate output images of arbitrary large size.},
archivePrefix = {arXiv},
arxivId = {1705.06566},
author = {Bergmann, Urs and Jetchev, Nikolay and Vollgraf, Roland},
eprint = {1705.06566},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {722--730},
title = {{Learning texture manifolds with the periodic spatial GAN}},
url = {http://arxiv.org/abs/1705.06566},
volume = {1},
year = {2017}
}
@article{Jaramillo-Botero2014,
abstract = {First-principles-based force fields prepared from large quantum mechanical data sets are now the norm in predictive molecular dynamics simulations for complex chemical processes, as opposed to force fields fitted solely from phenomenological data. In principle, the former allow improved accuracy and transferability over a wider range of molecular compositions, interactions, and environmental conditions unexplored by experiments. That is, assuming they have been optimally prepared from a diverse training set. The trade-off has been force field engines that are functionally complex, with a large number of nonbonded and bonded analytical forms that give rise to rather large parameter search spaces. To address this problem, we have developed GARFfield (genetic algorithm-based reactive force field optimizer method), a hybrid multiobjective Pareto-optimal parameter development scheme based on genetic algorithms, hill-climbing routines and conjugate-gradient minimization. To demonstrate the capabilities of GARFfield we use it to develop two very different force fields: (1) the ReaxFF reactive force field for modeling the adiabatic reactive dynamics of silicon carbide growth from an methyltrichlorosilane precursor and (2) the SiC electron force field with effective core pseudopotentials for modeling nonadiabatic dynamic phenomena with highly excited electronic states. The flexible and open architecture of GARFfield enables efficient and fast parallel optimization of parameters from quantum mechanical data sets for demanding applications like ReaxFF, electronic fast forward (or electron force field), and others including atomistic reactive charge-optimized many-body interatomic potentials, Morse, and coarse-grain force fields. {\textcopyright} 2014 American Chemical Society.},
author = {Jaramillo-Botero, Andres and Naserifar, Saber and Goddard, William A.},
doi = {10.1021/ct5001044},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {4},
pages = {1426--1439},
pmid = {26580361},
title = {{General multiobjective force field optimization framework, with application to reactive force fields for silicon carbide}},
volume = {10},
year = {2014}
}
@article{Dieleman2015,
abstract = {Measuring the morphological parameters of galaxies is a key requirement for studying their formation and evolution. Surveys such as the Sloan Digital Sky Survey have resulted in the availability of very large collections of images, which have permitted population-wide analyses of galaxy morphology. Morphological analysis has traditionally been carried out mostly via visual inspection by trained experts, which is time consuming and does not scale to large (≳10<sup>4</sup>) numbers of images. Although attempts have been made to build automated classification systems, these have not been able to achieve the desired level of accuracy. The Galaxy Zoo project successfully applied a crowdsourcing strategy, inviting online users to classify images by answering a series of questions. Unfortunately, even this approach does not scale well enough to keep up with the increasing availability of galaxy images. We present a deep neural network model for galaxy morphology classification which exploits translational and rotational symmetry. It was developed in the context of the Galaxy Challenge, an international competition to build the best model for morphology classification based on annotated images from the Galaxy Zoo project. For images with high agreement among the Galaxy Zoo participants, our model is able to reproduce their consensus with near-perfect accuracy (>99 per cent) for most questions. Confident model predictions are highly accurate, which makes the model suitable for filtering large collections of images and forwarding challenging images to experts for manual annotation. This approach greatly reduces the experts' workload without affecting accuracy. The application of these algorithms to larger sets of training data will be critical for analysing results from future surveys such as the Large Synoptic Survey Telescope.},
archivePrefix = {arXiv},
arxivId = {1503.07077},
author = {Dieleman, Sander and Willett, Kyle W. and Dambre, Joni},
doi = {10.1093/mnras/stv632},
eprint = {1503.07077},
isbn = {0035-8711},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Catalogues,Galaxies: general,Methods: data analysis,Techniques: image processing},
number = {2},
pages = {1441--1459},
pmid = {7491034},
title = {{Rotation-invariant convolutional neural networks for galaxy morphology prediction}},
volume = {450},
year = {2015}
}
@article{Wang2016a,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in large data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many-body systems. Starting with raw spin configurations of a prototypical Ising model, we use principal component analysis to extract relevant low-dimensional representations of the original data and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds physical concepts such as the order parameter and structure factor to be indicators of a phase transition. We discuss the future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
issn = {24699969},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@article{Malshe2008,
abstract = {A generalized method that permits the parameters of an arbitrary empirical potential to be efficiently and accurately fitted to a database is presented. The method permits the values of a subset of the potential parameters to be considered as general functions of the internal coordinates that define the instantaneous configuration of the system. The parameters in this subset are computed by a generalized neural network (NN) with one or more hidden layers and an input vector with at least 3n-6 elements, where n is the number of atoms in the system. The Levenberg-Marquardt algorithm is employed to efficiently affect the optimization of the weights and biases of the NN as well as all other potential parameters being treated as constants rather than as functions of the input coordinates. In order to effect this minimization, the usual Jacobian employed in NN operations is modified to include the Jacobian of the computed errors with respect to the parameters of the potential function. The total Jacobian employed in each epoch of minimization is the concatenation of two Jacobians, one containing derivatives of the errors with respect to the weights and biases of the network, and the other with respect to the constant parameters of the potential function. The method provides three principal advantages. First, it obviates the problem of selecting the form of the functional dependence of the parameters upon the system's coordinates by employing a NN. If this network contains a sufficient number of neurons, it will automatically find something close to the best functional form. This is the case since Hornik, [Neural Networks 2, 359 (1989)] have shown that two-layer NNs with sigmoid transfer functions in the first hidden layer and linear functions in the output layer are universal approximators for analytic functions. Second, the entire fitting procedure is automated so that excellent fits are obtained rapidly with little human effort. Third, the method provides a procedure to avoid local minima in the multidimensional parameter hyperspace. As an illustrative example, the general method has been applied to the specific case of fitting the ab initio energies of Si5 clusters that are observed in a molecular dynamics (MD) simulation of the machining of a silicon workpiece. The energies of the Si5 configurations obtained in the MD calculations are computed using the B3LYP procedure with a 6-31 G** basis set. The final ab initio database, which comprises the density functional theory energies of 10 202 Si5 clusters, is fitted to an empirical Tersoff potential containing nine adjustable parameters, two of which are allowed to be the functions of the Si5 configuration. The fitting error averaged over all 10 202 points is 0.0148 eV (1.43 kJ mol-1). This result is comparable to the accuracy achieved by more general fitting methods that do not rely on an assumed functional form for the potential surface. {\textcopyright} 2008 American Institute of Physics.},
author = {Malshe, M. and Narulkar, R. and Raff, L. M. and Hagan, M. and Bukkapatnam, S. and Komanduri, R.},
doi = {10.1063/1.2957490},
isbn = {0021-9606},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {4},
title = {{Parametrization of analytic interatomic potential functions using neural networks}},
volume = {129},
year = {2008}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2242--2251},
title = {{Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
volume = {2017-Octob},
year = {2017}
}
@techreport{Krizhevskya,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Communications of the ACM},
doi = {10.1145/3065386},
file = {::},
issn = {15577317},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://code.google.com/p/cuda-convnet/},
volume = {60},
year = {2017}
}
@article{Li2011,
abstract = {We propose a generalized regression neural network (GRNN) approach based on grey relational analysis (GRA) and principal component analysis (PCA) (GP-GRNN) to improve the accuracy of density functional theory (DFT) calculation for homolysis bond dissociation energies (BDE) of Y-NO bond. As a demonstration, this combined quantum chemistry calculation with the GP-GRNN approach has been applied to evaluate the homolysis BDE of 92 Y-NO organic molecules. The results show that the full-descriptor GRNN without GRA and PCA (F-GRNN) and with GRA (G-GRNN) approaches reduce the root-mean-square (RMS) of the calculated homolysis BDE of 92 organic molecules from 5.31 to 0.49 and 0.39 kcal mol -1 for the B3LYP/6-31G (d) calculation. Then the newly developed GP-GRNN approach further reduces the RMS to 0.31 kcal mol -1. Thus, the GP-GRNN correction on top of B3LYP/6-31G (d) can improve the accuracy of calculating the homolysis BDE in quantum chemistry and can predict homolysis BDE which cannot be obtained experimentally. {\textcopyright} 2011 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Li, Hong Zhi and Tao, Wei and Gao, Ting and Li, Hui and Lu, Ying Hua and Su, Zhong Min},
doi = {10.3390/ijms12042242},
isbn = {1661-6596},
issn = {14220067},
journal = {International Journal of Molecular Sciences},
keywords = {Density functional theory,Generalized regression neural network,Grey relational analysis,Homolysis bond dissociation energy,Principal component analysis,Y-NO bond},
number = {4},
pages = {2242--2261},
pmid = {21731439},
title = {{Improving the accuracy of density functional theory (DFT) calculation for homolysis bond dissociation energies of Y-NO bond: Generalized regression neural network based on grey relational analysis and principal component analysis}},
volume = {12},
year = {2011}
}
@article{Farimani2017,
abstract = {We have developed a new data-driven paradigm for the rapid inference, modeling and simulation of the physics of transport phenomena by deep learning. Using conditional generative adversarial networks (cGAN), we train models for the direct generation of solutions to steady state heat conduction and incompressible fluid flow purely on observation without knowledge of the underlying governing equations. Rather than using iterative numerical methods to approximate the solution of the constitutive equations, cGANs learn to directly generate the solutions to these phenomena, given arbitrary boundary conditions and domain, with high test accuracy (MAE<1%) and state-of-the-art computational performance. The cGAN framework can be used to learn causal models directly from experimental observations where the underlying physical model is complex or unknown.},
archivePrefix = {arXiv},
arxivId = {1709.02432},
author = {{Barati Farimani}, Amir and Gomes, Joseph and Pande, Vijay S.},
eprint = {1709.02432},
issn = {23318422},
journal = {arXiv},
title = {{Deep Learning the Physics of Transport Phenomena}},
url = {http://arxiv.org/abs/1709.02432},
volume = {94305},
year = {2017}
}
@article{Bunyk2014,
abstract = {We have developed a quantum annealing processor, based on an array of tunable coupled rf-SQUID flux qubits, fabricated in a superconducting integrated circuit process. Implementing this type of processor at a scale of 512 qubits and 1472 programmable interqubit couplers and operating at ∼ 20 mK has required attention to a number of considerations that one may ignore at the smaller scale of a few dozen or so devices. Here, we discuss some of these considerations, and the delicate balance necessary for the construction of a practical processor that respects the demanding physical requirements imposed by a quantum algorithm. In particular, we will review some of the design tradeoffs at play in the floor planning of the physical layout, driven by the desire to have an algorithmically useful set of interqubit couplers, and the simultaneous need to embed programmable control circuitry into the processor fabric. In this context, we have developed a new ultralow-power embedded superconducting digital-to-analog flux converter (DAC) used to program the processor with zero static power dissipation, optimized to achieve maximum flux storage density per unit area. The 512 single-stage, 3520 two-stage, and 512 three-stage flux DACs are controlled with an XYZ addressing scheme requiring 56 wires. Our estimate of on-chip dissipated energy for worst-case reprogramming of the whole processor is ∼65 fJ. Several chips based on this architecture have been fabricated and operated successfully at our facility, as well as two outside facilities (see, for example, the recent reporting by Jones).},
archivePrefix = {arXiv},
arxivId = {1401.5504},
author = {Bunyk, P. I. and Hoskinson, Emile M. and Johnson, Mark W. and Tolkacheva, Elena and Altomare, Fabio and Berkley, Andrew J. and Harris, Richard and Hilton, Jeremy P. and Lanting, Trevor and Przybysz, Anthony J. and Whittaker, Jed},
doi = {10.1109/TASC.2014.2318294},
eprint = {1401.5504},
issn = {10518223},
journal = {IEEE Transactions on Applied Superconductivity},
keywords = {Computational physics,quantum computing,superconducting integrated circuits},
number = {4},
pages = {1--10},
publisher = {IEEE},
title = {{Architectural Considerations in the Design of a Superconducting Quantum Annealing Processor}},
volume = {24},
year = {2014}
}
@article{Sherrington1975,
abstract = {We consider an Ising model in which the spins are coupled by infinite-ranged random interactions independently distributed with a Gaussian probability density. Both "spinglass" and ferromagnetic phases occur. The competition between the phases and the type of order present in each are studied. {\textcopyright} 1975 The American Physical Society.},
author = {Sherrington, David and Kirkpatrick, Scott},
doi = {10.1103/PhysRevLett.35.1792},
issn = {00319007},
journal = {Physical Review Letters},
month = {dec},
number = {26},
pages = {1792--1796},
title = {{Solvable model of a spin-glass}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792},
volume = {35},
year = {1975}
}
@article{Schutt2017,
abstract = {Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks, which leads to size-extensive and uniformly accurate (1 kcal mol -1) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the model reveals a classification of aromatic rings with respect to their stability. Further applications of our model for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the potential of machine learning for revealing insights into complex quantum-chemical systems.},
archivePrefix = {arXiv},
arxivId = {1609.08259},
author = {Sch{\"{u}}tt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and M{\"{u}}ller, Klaus R. and Tkatchenko, Alexandre},
doi = {10.1038/ncomms13890},
eprint = {1609.08259},
issn = {20411723},
journal = {Nature Communications},
month = {jan},
pages = {13890},
pmid = {28067221},
title = {{Quantum-chemical insights from deep tensor neural networks}},
url = {http://www.nature.com/doifinder/10.1038/ncomms13890},
volume = {8},
year = {2017}
}
@article{Dieleman2016,
abstract = {Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.},
archivePrefix = {arXiv},
arxivId = {1602.02660},
author = {Dieleman, Sander and {De Fauw}, Jeffrey and Kavukcuoglu, Koray},
eprint = {1602.02660},
isbn = {9781510829008},
issn = {1938-7228},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2799--2808},
title = {{Exploiting cyclic symmetry in convolutional neural networks}},
url = {http://arxiv.org/abs/1602.02660},
volume = {4},
year = {2016}
}
@article{Pozun2012,
abstract = {We present a method for optimizing transition state theory dividing surfaces with support vector machines. The resulting dividing surfaces require no a priori information or intuition about reaction mechanisms. To generate optimal dividing surfaces, we apply a cycle of machine-learning and refinement of the surface by molecular dynamics sampling. We demonstrate that the machine-learned surfaces contain the relevant low-energy saddle points. The mechanisms of reactions may be extracted from the machine-learned surfaces in order to identify unexpected chemically relevant processes. Furthermore, we show that the machine-learned surfaces significantly increase the transmission coefficient for an adatom exchange involving many coupled degrees of freedom on a (100) surface when compared to a distance-based dividing surface. {\textcopyright} 2012 American Institute of Physics.},
author = {Pozun, Zachary D. and Hansen, Katja and Sheppard, Daniel and Rupp, Matthias and M{\"{u}}ller, Klaus Robert and Henkelman, Graeme},
doi = {10.1063/1.4707167},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pmid = {22583204},
title = {{Optimizing transition states via kernel-based machine learning}},
volume = {136},
year = {2012}
}
@article{Hautier2010,
abstract = {Finding new compounds and their crystal structures is an essential step to new materials discoveries. We demonstrate how this search can be accelerated using a combination of machine learning techniques and high-throughput ab initio computations. Using a probabilistic model built on an experimental crystal structure database, novel compositions that are most likely to form a compound, and their most-probable crystal structures, are identified and tested for stability by ab initio computations. We performed such a large-scale search for new ternary oxides, discovering 209 new compounds with a limited computational budget. A list of these predicted compounds is provided, and we discuss the chemistries in which high discovery rates can be expected. {\textcopyright} 2010 American Chemical Society.},
author = {Hautier, Geoffroy and Fischer, Christopher C. and Jain, Anubhav and Mueller, Tim and Ceder, Gerbrand},
doi = {10.1021/cm100795d},
isbn = {0897-4756},
issn = {08974756},
journal = {Chemistry of Materials},
month = {jun},
number = {12},
pages = {3762--3767},
title = {{Finding natures missing ternary oxide compounds using machine learning and density functional theory}},
url = {http://pubs.acs.org/doi/abs/10.1021/cm100795d},
volume = {22},
year = {2010}
}
@techreport{Raina2009a,
abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
booktitle = {Proceedings of the 26th International Conference On Machine Learning, ICML 2009},
file = {::},
isbn = {9781605585161},
pages = {873--880},
title = {{Large-scale deep unsupervised learning using graphics processors}},
year = {2009}
}
@article{Dickson2013,
abstract = {Efforts to develop useful quantum computers have been blocked primarily by environmental noise. Quantum annealing is a scheme of quantum computation that is predicted to be more robust against noise, because despite the thermal environment mixing the system's state in the energy basis, the system partially retains coherence in the computational basis, and hence is able to establish well-defined eigenstates. Here we examine the environment's effect on quantum annealing using 16 qubits of a superconducting quantum processor. For a problem instance with an isolated small-gap anticrossing between the lowest two energy levels, we experimentally demonstrate that, even with annealing times eight orders of magnitude longer than the predicted single-qubit decoherence time, the probabilities of performing a successful computation are similar to those expected for a fully coherent system. Moreover, for the problem studied, we show that quantum annealing can take advantage of a thermal environment to achieve a speedup factor of up to 1,000 over a closed system. {\textcopyright} 2013 Macmillan Publishers Limited. All rights reserved.},
author = {Dickson, N. G. and Johnson, M. W. and Amin, M. H. and Harris, R. and Altomare, F. and Berkley, A. J. and Bunyk, P. and Cai, J. and Chapple, E. M. and Chavez, P. and Cioata, F. and Cirip, T. and Debuen, P. and Drew-Brook, M. and Enderud, C. and Gildert, S. and Hamze, F. and Hilton, J. P. and Hoskinson, E. and Karimi, K. and Ladizinsky, E. and Ladizinsky, N. and Lanting, T. and Mahon, T. and Neufeld, R. and Oh, T. and Perminov, I. and Petroff, C. and Przybysz, A. and Rich, C. and Spear, P. and Tcaciuc, A. and Thom, M. C. and Tolkacheva, E. and Uchaikin, S. and Wang, J. and Wilson, A. B. and Merali, Z. and Rose, G.},
doi = {10.1038/ncomms2920},
issn = {20411723},
journal = {Nature Communications},
number = {May},
pages = {1--6},
title = {{Thermally assisted quantum annealing of a 16-qubit problem}},
volume = {4},
year = {2013}
}
@article{Ward2015,
abstract = {A very active area of materials research is to devise methods that use machine learning to automatically extract predictive models from existing materials data. While prior examples have demonstrated successful models for some applications, many more applications exist where machine learning can make a strong impact. To enable faster development of machine-learning-based models for such applications, we have created a framework capable of being applied to a broad range of materials data. Our method works by using a chemically diverse list of attributes, which we demonstrate are suitable for describing a wide variety of properties, and a novel method for partitioning the data set into groups of similar materials to boost the predictive accuracy. In this manuscript, we demonstrate how this new method can be used to predict diverse properties of crystalline and amorphous materials, such as band gap energy and glass-forming ability.},
archivePrefix = {arXiv},
arxivId = {1606.09551},
author = {Ward, Logan and Agrawal, Ankit and Choudhary, Alok and Wolverton, Christopher},
doi = {10.1038/npjcompumats.2016.28},
eprint = {1606.09551},
issn = {20573960},
journal = {npj Computational Materials},
month = {aug},
number = {July},
pages = {1--7},
title = {{A general-purpose machine learning framework for predicting properties of inorganic materials}},
url = {http://www.nature.com/articles/npjcompumats201628},
volume = {2},
year = {2016}
}
@article{Morawietz2013,
abstract = {The fundamental importance of water for many chemical processes has motivated the development of countless efficient but approximate water potentials for large-scale molecular dynamics simulations, from simple empirical force fields to very sophisticated flexible water models. Accurate and generally applicable water potentials should fulfill a number of requirements. They should have a quality close to quantum chemical methods, they should explicitly depend on all degrees of freedom including all relevant many-body interactions, and they should be able to describe molecular dissociation and recombination. In this work, we present a high-dimensional neural network (NN) potential for water clusters based on density-functional theory (DFT) calculations, which is constructed using clusters containing up to 10 monomers and is in principle able to meet all these requirements. We investigate the reliability of specific parametrizations employing two frequently used generalized gradient approximation (GGA) exchange-correlation functionals, PBE and RPBE, as reference methods. We find that the binding energy errors of the NN potentials with respect to DFT are significantly lower than the typical uncertainties of DFT calculations arising from the choice of the exchange-correlation functional. Further, we examine the role of van der Waals interactions, which are not properly described by GGA functionals. Specifically, we incorporate the D3 scheme suggested by Grimme (J. Chem. Phys. 2010, 132, 154104) in our potentials and demonstrate that it can be applied to GGA-based NN potentials in the same way as to DFT calculations without modification. Our results show that the description of small water clusters provided by the RPBE functional is significantly improved if van der Waals interactions are included, while in case of the PBE functional, which is well-known to yield stronger binding than RPBE, van der Waals corrections lead to overestimated binding energies. {\textcopyright} 2013 American Chemical Society.},
author = {Morawietz, Tobias and Behler, J{\"{o}}rg},
doi = {10.1021/jp401225b},
isbn = {10895639},
issn = {10895639},
journal = {Journal of Physical Chemistry A},
number = {32},
pages = {7356--7366},
pmid = {23557541},
title = {{A density-functional theory-based neural network potential for water clusters including van der waals corrections}},
volume = {117},
year = {2013}
}
@article{Amin2018,
abstract = {Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, we propose a new machine-learning approach based on quantum Boltzmann distribution of a quantum Hamiltonian. Because of the noncommutative nature of quantum mechanics, the training process of the quantum Boltzmann machine (QBM) can become nontrivial. We circumvent the problem by introducing bounds on the quantum probabilities. This allows us to train the QBM efficiently by sampling. We show examples of QBM training with and without the bound, using exact diagonalization, and compare the results with classical Boltzmann training. We also discuss the possibility of using quantum annealing processors for QBM training and application.},
archivePrefix = {arXiv},
arxivId = {1601.02036},
author = {Amin, Mohammad H. and Andriyash, Evgeny and Rolfe, Jason and Kulchytskyy, Bohdan and Melko, Roger},
doi = {10.1103/PhysRevX.8.021050},
eprint = {1601.02036},
file = {::},
issn = {21603308},
journal = {Physical Review X},
keywords = {doi:10.1103/PhysRevX.8.021050 url:https://doi.org/},
number = {2},
title = {{Quantum Boltzmann Machine}},
volume = {8},
year = {2018}
}
@article{Pilania2013,
abstract = {The materials discovery process can be significantly expedited and simplified if we can learn effectively from available knowledge and data. In the present contribution, we show that efficient and accurate prediction of a diverse set of properties of material systems is possible by employing machine (or statistical) learning methods trained on quantum mechanical computations in combination with the notions of chemical similarity. Using a family of one-dimensional chain systems, we present a general formalism that allows us to discover decision rules that establish a mapping between easily accessible attributes of a system and its properties. It is shown that fingerprints based on either chemo-structural (compositional and configurational information) or the electronic charge density distribution can be used to make ultra-fast, yet accurate, property predictions. Harnessing such learning paradigms extends recent efforts to systematically explore and mine vast chemical spaces, and can significantly accelerate the discovery of new application-specific materials.},
author = {Pilania, Ghanshyam and Wang, Chenchen and Jiang, Xun and Rajasekaran, Sanguthevar and Ramprasad, Ramamurthy},
doi = {10.1038/srep02810},
issn = {20452322},
journal = {Scientific Reports},
pages = {2810},
pmid = {24077117},
title = {{Accelerating materials property predictions using machine learning}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3786293&tool=pmcentrez&rendertype=abstract},
volume = {3},
year = {2013}
}
@article{Manzhos2009,
abstract = {We propose a neural network (NN) based algorithm for calculating vibrational energies and wave functions and apply it to problems in 2-, 4-, and 6-dimensions. By using neurons as basis functions and methods of nonlinear optimization, we are able to compute three states of a 6-D Hamiltonian using only 50 basis functions. In a standard direct product basis, thousands of basis functions would be necessary. Previous NN methods for solving the Schr{\"{o}}dinger equation computed one level at a time and optimized all of the parameters using expensive nonlinear optimization methods. Using our approach, linear coefficients in the NN representation of wave functions are determined with methods of linear algebra and many levels are computed at the same time from one set of nonlinear NN parameters. In addition, we use radial basis function neurons to ensure the correct boundary conditions. The use of linear algebra methods makes it possible to treat systems of higher dimensionality.},
author = {Manzhos, Sergei and Carrington, Tucker},
doi = {10.1139/V09-025},
isbn = {0008-4042},
issn = {00084042},
journal = {Canadian Journal of Chemistry},
keywords = {Neural networks,Numerical methods,Vibrational spectroscopy},
number = {7},
pages = {864--871},
title = {{An improved neural network method for solving the Schr{\"{o}}dinger equation1}},
volume = {87},
year = {2009}
}
@article{Barahona1982,
abstract = {In a spin glass with Ising spins, the problems of computing the magnetic partition function and finding a ground state are studied. In a finite two-dimensional lattice these problems can be solved by algorithms that require a number of steps bounded by a polynomial function of the size of the lattice. In contrast to this fact, the same problems are shown to belong to the class of NP-hard problems, both in the two-dimensional case within a magnetic field, and in the three-dimensional case. NP-hardness of a problem suggests that it is very unlikely that a polynomial algorithm could exist to solve it. {\textcopyright} 1982 The Japan Society of Applied Physics.},
author = {Barahona, F.},
doi = {10.1088/0305-4470/15/10/028},
issn = {13616447},
journal = {Journal of Physics A: Mathematical and General},
month = {oct},
number = {10},
pages = {3241--3253},
title = {{On the computational complexity of ising spin glass models}},
url = {http://stacks.iop.org/0305-4470/15/i=10/a=028?key=crossref.1fe57df6674a7c759374b69321415b44},
volume = {15},
year = {1982}
}
@article{Kearnes2016,
abstract = {Molecular “fingerprints” encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph—atoms, bonds, distances, etc.—which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.},
archivePrefix = {arXiv},
arxivId = {1603.00856},
author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
doi = {10.1007/s10822-016-9938-8},
eprint = {1603.00856},
issn = {15734951},
journal = {Journal of Computer-Aided Molecular Design},
keywords = {Artificial neural networks,Deep learning,Machine learning,Molecular descriptors,Virtual screening},
number = {8},
pages = {595--608},
pmid = {27558503},
publisher = {Springer International Publishing},
title = {{Molecular graph convolutions: moving beyond fingerprints}},
volume = {30},
year = {2016}
}
@article{Suzuki2016,
abstract = {Recently, machine learning has emerged as an alternative, powerful approach for predicting quantum-mechanical properties of molecules and solids. Here, using kernel ridge regression and atomic fingerprints representing local environments of atoms, we trained a machine-learning model on a crystalline silicon system to directly predict the atomic forces at a wide range of temperatures. Our idea is to construct a machine-learning model using a quantum-mechanical dataset taken from canonical-ensemble simulations at a higher temperature, or an upper bound of the temperature range. With our model, the force prediction errors were about 2% or smaller with respect to the corresponding force ranges, in the temperature region between 300 K and 1650 K. We also verified the applicability to a larger system, ensuring the transferability with respect to system size.},
archivePrefix = {arXiv},
arxivId = {1608.07374},
author = {Suzuki, Teppei and Tamura, Ryo and Miyazaki, Tsuyoshi},
doi = {10.1002/qua.25307},
eprint = {1608.07374},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
keywords = {force fields,kernel ridge regression,machine learning,materials simulation},
number = {1},
pages = {33--39},
title = {{Machine learning for atomic forces in a crystalline solid: Transferability to various temperatures}},
url = {http://arxiv.org/abs/1608.07374},
volume = {117},
year = {2017}
}
@article{Lee,
abstract = {Inferring a generative model from data is a fundamental problem in machine learning. It is well- known that the Ising model is the maximum entropy model for binary variables which reproduces the sample mean and pairwise correlations. Learning the parameters of the Ising model from data is the challenge. We establish an analogy between the inverse Ising problem and the Ornstein- Zernike formalism in liquid state physics. Rather than analytically deriving the closure relation, we use a deep neural network to learn the closure from simulations of the Ising model. We show, using simulations as well as biochemical datasets, that the deep neural network model outperforms systematic field-theoretic expansions, is more data-efficient than the pseudolikelihood method, and can generalize well beyond the parameter regime of the training data. The neural network is able to learn from synthetic data, which can be generated with relative ease, to give accurate predictions on real world datasets.},
archivePrefix = {arXiv},
arxivId = {1706.08466},
author = {Turi, Soma and Lee, Alpha A.},
eprint = {1706.08466},
issn = {23318422},
journal = {arXiv},
month = {jun},
number = {2},
title = {{Inverse Ising inference by combining Ornstein-Zernike theory with deep learning}},
url = {http://arxiv.org/abs/1706.08466},
year = {2017}
}
@article{Keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Nocedal, Jorge and Tang, Ping Tak Peter and Mudigere, Dheevatsa and Smelyanskiy, Mikhail},
eprint = {1609.04836},
file = {::},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{On large-batch training for deep learning: Generalization gap and sharp minima}},
url = {http://arxiv.org/abs/1609.04836},
year = {2017}
}
@article{Gharaati2010,
abstract = {A new confinement potential for spherical quantum dots, called the modified Gaussian potential (MGP), is studied. In the present work, the following problems are investigated within the effective-mass approximation: (i) the one-electron energy spectra, (ii) wave functions, (iii) the problem of existence of a bound electron state, and (iv) the binding energy of center and off-center hydrogenic donor impurities. For zero angular momentum (l=0), the new confinement potential is sufficiently flexible to obtain analytically the spectral energy and wave functions. The results obtained from the present work show that (i) the new potential is suitable for predicting the spectral energy and wave functions, and (ii) the geometrical sizes of the quantum dot play the important roles on the energy levels, wave functions, the binding energy, and the existence of a bound electron state. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
author = {Gharaati, A. and Khordad, R.},
doi = {10.1016/j.spmi.2010.06.014},
issn = {07496036},
journal = {Superlattices and Microstructures},
keywords = {Binding energy,Confinement potential,Energy levels,Quantum dots},
number = {3},
pages = {276--287},
publisher = {Elsevier Ltd},
title = {{A new confinement potential in spherical quantum dots: Modified Gaussian potential}},
url = {http://dx.doi.org/10.1016/j.spmi.2010.06.014},
volume = {48},
year = {2010}
}
@article{Carleo2016,
abstract = {The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.},
archivePrefix = {arXiv},
arxivId = {1606.02318},
author = {Carleo, Giuseppe and Troyer, Matthias},
doi = {10.1126/science.aag2302},
eprint = {1606.02318},
issn = {10959203},
journal = {Science},
month = {feb},
number = {6325},
pages = {602--606},
pmid = {28183973},
title = {{Solving the quantum many-body problem with artificial neural networks}},
url = {http://arxiv.org/abs/1606.02318 http://www.sciencemag.org/lookup/doi/10.1126/science.aag2302},
volume = {355},
year = {2017}
}
@article{Behler2016,
abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.},
author = {Behler, J{\"{o}}rg},
doi = {10.1063/1.4966192},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pages = {170901},
pmid = {27825224},
title = {{Perspective: Machine learning potentials for atomistic simulations}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966192},
volume = {145},
year = {2016}
}
@article{Mirza2014,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
month = {nov},
pages = {1--7},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
year = {2014}
}
@article{tensorflow,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
file = {::},
isbn = {9781931971331},
journal = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
month = {may},
pages = {265--283},
publisher = {USENIX Association},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
@article{Agostinelli2019,
abstract = {The Rubik's cube is a prototypical combinatorial puzzle that has a large state space with a single goal state. The goal state is unlikely to be accessed using sequences of randomly generated moves, posing unique challenges for machine learning. We solve the Rubik's cube with DeepCubeA, a deep reinforcement learning approach that learns how to solve increasingly difficult states in reverse from the goal state without any specific domain knowledge. DeepCubeA solves 100% of all test configurations, finding a shortest path to the goal state 60.3% of the time. DeepCubeA generalizes to other combinatorial puzzles and is able to solve the 15 puzzle, 24 puzzle, 35 puzzle, 48 puzzle, Lights Out and Sokoban, finding a shortest path in the majority of verifiable cases.},
author = {Agostinelli, Forest and McAleer, Stephen and Shmakov, Alexander and Baldi, Pierre},
doi = {10.1038/s42256-019-0070-z},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
month = {aug},
number = {8},
pages = {356--363},
title = {{Solving the Rubik's cube with deep reinforcement learning and search}},
url = {http://www.nature.com/articles/s42256-019-0070-z},
volume = {1},
year = {2019}
}
@article{Allinger1989,
abstract = {Understanding biochemical mechanisms and changes associated with disease conditions and, therefore, development of improved clinical treatments, is relying increasingly on various biochemical mapping and imaging techniques on tissue sections. However, it is essential to be able to ascertain whether the sampling used provides the full biochemical information relevant to the disease and is free from artefacts. A multi-modal micro-spectroscopic approach, including FTIR imaging and PIXE elemental mapping, has been used to study the molecular and elemental profile within cryofixed and formalin-fixed murine brain tissue sections. The results provide strong evidence that amino acids, carbohydrates, lipids, phosphates, proteins and ions, such as Cl- and K+, leach from tissue sections into the aqueous fixative medium during formalin fixation of the sections. Large changes in the concentrations and distributions of most of these components are also observed by washing in PBS even for short periods. The most likely source of the chemical species lost during fixation is the extra-cellular and intra-cellular fluid of tissues. The results highlight that, at best, analysis of formalin-fixed tissues gives only part of the complete biochemical "picture" of a tissue sample. Further, this investigation has highlighted that significant lipid peroxidation/oxidation may occur during formalin fixation and that the use of standard histological fixation reagents can result in significant and differential metal contamination of different regions of tissue sections. While a consistent and reproducible fixation method may be suitable for diagnostic purposes, the findings of this study strongly question the use of formalin fixation prior to spectroscopic studies of the molecular and elemental composition of biological samples, if the primary purpose is mechanistic studies of disease pathogenesis. {\textcopyright} The Royal Society of Chemistry 2011.},
author = {Hackett, Mark J. and McQuillan, James A. and El-Assaad, Fatima and Aitken, Jade B. and Levina, Aviva and Cohen, David D. and Siegele, Rainer and Carter, Elizabeth A. and Grau, Georges E. and Hunt, Nicholas H. and Lay, Peter A.},
doi = {10.1039/c0an00269k},
isbn = {00027863 (ISSN)},
issn = {13645528},
journal = {Analyst},
keywords = {aliphatic hydrocarbon,drug structure,energy,methodology,nonhuman,short survey,theoretical study,theory,thermodynamics},
number = {14},
pages = {2941--2952},
pmid = {21776693},
title = {{Chemical alterations to murine brain tissue induced by formalin fixation: Implications for biospectroscopic imaging and mapping studies of disease pathogenesis}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0024821263&partnerID=40&md5=7a3fca9b78a4242ea7de126833653326},
volume = {136},
year = {2011}
}
@article{LeCun1998b,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Brockherde2016,
abstract = {Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of density functional theory to solve electronic structure problems in a wide variety of scientific fields. Machine learning holds the promise of learning the energy functional via examples, bypassing the need to solve the Kohn-Sham equations. This should yield substantial savings in computer time, allowing larger systems and/or longer time-scales to be tackled, but attempts to machine-learn this functional have been limited by the need to find its derivative. The present work overcomes this difficulty by directly learning the density-potential and energy-density maps for test systems and various molecules. We perform the first molecular dynamics simulation with a machine-learned density functional on malonaldehyde and are able to capture the intramolecular proton transfer process. Learning density models now allows the construction of accurate density functionals for realistic molecular systems.},
archivePrefix = {arXiv},
arxivId = {1609.02815},
author = {Brockherde, Felix and Vogt, Leslie and Li, Li and Tuckerman, Mark E. and Burke, Kieron and M{\"{u}}ller, Klaus Robert},
doi = {10.1038/s41467-017-00839-3},
eprint = {1609.02815},
issn = {20411723},
journal = {Nature Communications},
month = {sep},
number = {1},
pages = {1--8},
pmid = {29021555},
title = {{Bypassing the Kohn-Sham equations with machine learning}},
url = {http://arxiv.org/abs/1609.02815},
volume = {8},
year = {2017}
}
@article{Farhi2014,
abstract = {We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.},
archivePrefix = {arXiv},
arxivId = {1411.4028},
author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
eprint = {1411.4028},
pages = {1--16},
title = {{A Quantum Approximate Optimization Algorithm}},
url = {http://arxiv.org/abs/1411.4028},
year = {2014}
}
@article{Wagner2012,
abstract = {Large strongly correlated systems provide a challenge to modern electronic structure methods, because standard density functionals usually fail and traditional quantum chemical approaches are too demanding. The density-matrix renormalization group method, an extremely powerful tool for solving such systems, has recently been extended to handle long-range interactions on real-space grids, but is most efficient in one dimension where it can provide essentially arbitrary accuracy. Such 1d systems therefore provide a theoretical laboratory for studying strong correlation and developing density functional approximations to handle strong correlation, if they mimic three-dimensional reality sufficiently closely. We demonstrate that this is the case, and provide reference data for exact and standard approximate methods, for future use in this area. This journal is {\textcopyright} the Owner Societies 2012.},
archivePrefix = {arXiv},
arxivId = {1202.4788},
author = {Wagner, Lucas O. and Stoudenmire, E. M. and Burke, Kieron and White, Steven R.},
doi = {10.1039/c2cp24118h},
eprint = {1202.4788},
issn = {14639076},
journal = {Physical Chemistry Chemical Physics},
number = {24},
pages = {8581--8590},
pmid = {22596085},
title = {{Reference electronic structure calculations in one dimension}},
url = {http://xlink.rsc.org/?DOI=c2cp24118h},
volume = {14},
year = {2012}
}
@article{Wang2016,
abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in large data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many-body systems. Starting with raw spin configurations of a prototypical Ising model, we use principal component analysis to extract relevant low-dimensional representations of the original data and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds physical concepts such as the order parameter and structure factor to be indicators of a phase transition. We discuss the future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
archivePrefix = {arXiv},
arxivId = {1606.00318},
author = {Wang, Lei},
doi = {10.1103/PhysRevB.94.195105},
eprint = {1606.00318},
issn = {24699969},
journal = {Physical Review B},
month = {nov},
number = {19},
pages = {195105},
title = {{Discovering phase transitions with unsupervised learning}},
url = {http://arxiv.org/abs/1606.00318 http://link.aps.org/doi/10.1103/PhysRevB.94.195105},
volume = {94},
year = {2016}
}
@article{Gao2016,
abstract = {Background: Non-covalent interactions (NCIs) play critical roles in supramolecular chemistries; however, they are difficult to measure. Currently, reliable computational methods are being pursued to meet this challenge, but the accuracy of calculations based on low levels of theory is not satisfactory and calculations based on high levels of theory are often too costly. Accordingly, to reduce the cost and increase the accuracy of low-level theoretical calculations to describe NCIs, an efficient approach is proposed to correct NCI calculations based on the benchmark databases S22, S66 and X40 (Hobza in Acc Chem Rev 45: 663-672, 2012; Řez{\'{a}}{\v{c}} et al. in J Chem Theory Comput 8:4285, 2012). Results: A novel type of NCI correction is presented for density functional theory (DFT) methods. In this approach, the general regression neural network machine learning method is used to perform the correction for DFT methods on the basis of DFT calculations. Various DFT methods, including M06-2X, B3LYP, B3LYP-D3, PBE, PBE-D3 and $\omega$B97XD, with two small basis sets (i.e., 6-31G∗and 6-31+G∗) were investigated. Moreover, the conductor-like polarizable continuum model with two types of solvents (i.e., water and pentylamine, which mimics a protein environment with ϵ = 4.2) were considered in the DFT calculations. With the correction, the root mean square errors of all DFT calculations were improved by at least 70 %. Relative to CCSD(T)/CBS benchmark values (used as experimental NCI values because of its high accuracy), the mean absolute error of the best result was 0.33 kcal/mol, which is comparable to high-level ab initio methods or DFT methods with fairly large basis sets. Notably, this level of accuracy is achieved within a fraction of the time required by other methods. For all of the correction models based on various DFT approaches, the validation parameters according to OECD principles (i.e., the correlation coefficient R 2, the predictive squared correlation coefficient q 2 and $$q-{cv}^{2}$$ q c v 2 from cross-validation) were >0.92, which suggests that the correction model has good stability, robustness and predictive power. Conclusions: The correction can be added following DFT calculations. With the obtained molecular descriptors, the NCIs produced by DFT methods can be improved to achieve high-level accuracy. Moreover, only one parameter is introduced into the correction model, which makes it easily applicable. Overall, this work demonstrates that the correction model may be an alternative to the traditional means of correcting for NCIs. Graphical abstract A machine learning correction model efficiently improved the accuracy of non-covalent interactions(NCIs) calculated by DFT methods. The application of the correction model is easy and flexible, so it may be an alternative correction means for NCIs by first-principle calculations.},
author = {Gao, Ting and Li, Hongzhi and Li, Wenze and Li, Lin and Fang, Chao and Li, Hui and Hu, Lihong and Lu, Yinghua and Su, Zhong Min},
doi = {10.1186/s13321-016-0133-7},
issn = {17582946},
journal = {Journal of Cheminformatics},
keywords = {Computational accuracy,Density functional theory,Feature selection,Machine learning correction,Non-covalent interactions},
number = {1},
pages = {24},
pmid = {27148408},
publisher = {Springer International Publishing},
title = {{A machine learning correction for DFT non-covalent interactions based on the S22, S66 and X40 benchmark databases}},
url = {http://jcheminf.springeropen.com/articles/10.1186/s13321-016-0133-7},
volume = {8},
year = {2016}
}
@article{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
file = {::},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Continuous control with deep reinforcement learning}},
url = {https://goo.gl/J4PIAz http://arxiv.org/abs/1509.02971},
year = {2016}
}
@article{Morningstar2017,
abstract = {It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.},
archivePrefix = {arXiv},
arxivId = {1708.04622},
author = {Morningstar, Alan and Melko, Roger G.},
eprint = {1708.04622},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep Boltzmann machine,Deep belief network,Deep learning,Restricted Boltzmann machine},
pages = {1--17},
title = {{Deep learning the ising model near criticality}},
url = {http://arxiv.org/abs/1708.04622},
volume = {18},
year = {2018}
}
@article{Kirkpatrick1984,
abstract = {Simulated annealing is a stochastic optimization procedure which is widely applicable and has been found effective in several problems arising in computeraided circuit design. This paper derives the method in the context of traditional optimization heuristics and presents experimental studies of its computational efficiency when applied to graph partitioning and traveling salesman problems. {\textcopyright} 1984 Plenum Publishing Corporation.},
author = {Kirkpatrick, Scott},
doi = {10.1007/BF01009452},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Spin glasses,algorithms,graph partitioning,optimization},
month = {mar},
number = {5-6},
pages = {975--986},
title = {{Optimization by simulated annealing: Quantitative studies}},
url = {http://link.springer.com/10.1007/BF01009452},
volume = {34},
year = {1984}
}
@article{Ciresan2011b,
abstract = {We study a family of 'classical' orthogonal polynomials which satisfy (apart from a three-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl type. These polynomials can be obtained from the little q-Jacobi polynomials in the limit q = -1. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for q = -1. {\textcopyright} 2011 IOP Publishing Ltd.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Vinet, Luc and Zhedanov, Alexei},
doi = {10.1088/1751-8113/44/8/085201},
eprint = {1011.1669},
isbn = {9788578110796},
issn = {17518113},
journal = {Journal of Physics A: Mathematical and Theoretical},
keywords = {Flooding,Geographic information systems,Physical diagnosis,Runoff},
month = {feb},
number = {8},
pages = {369--381},
pmid = {25246403},
title = {{A 'missing' family of classical orthogonal polynomials}},
url = {http://arxiv.org/abs/1011.1669 http://dx.doi.org/10.1088/1751-8113/44/8/085201 http://stacks.iop.org/1751-8121/44/i=8/a=085201?key=crossref.abc74c979a75846b3de48a5587bf708f},
volume = {44},
year = {2011}
}
@misc{SwiftKey2015,
author = {SwiftKey},
booktitle = {SwiftKey Blog},
title = {{Introducing the world's first neural network keyboard}},
url = {https://blog.swiftkey.com/neural-networks-a-meaningful-leap-for-mobile-typing/},
year = {2015}
}
@article{Fortunato2017,
abstract = {In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks. Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80%. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks. We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.},
archivePrefix = {arXiv},
arxivId = {1704.02798},
author = {Fortunato, Meire and Blundell, Charles and Vinyals, Oriol},
eprint = {1704.02798},
file = {::},
issn = {23318422},
journal = {arXiv},
month = {apr},
publisher = {arXiv},
title = {{Bayesian recurrent neural networks}},
url = {http://arxiv.org/abs/1704.02798},
year = {2017}
}
@techreport{Salakhutdinov,
abstract = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.},
author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1273496.1273596},
file = {::},
pages = {791--798},
title = {{Restricted Boltzmann machines for collaborative filtering}},
volume = {227},
year = {2007}
}
@article{VanBeest1990,
abstract = {We address the problem of finding interatomic force fields for silicas from ab initio calculations on small clusters. It is shown that the force field cannot be determined from cluster data alone; incorporation of bulk-system information into the force field remains essential. Bearing this in mind, we derive a force field based on both microscopic (ab initio) and macroscopic (experimental) data. This force field combines accuracy with transferability to other polymorphs. The possibility of parametrizing other elements is also demonstrated. {\textcopyright} 1990 The American Physical Society.},
author = {{Van Beest}, B. W.H. and Kramer, G. J. and {Van Santen}, R. A.},
doi = {10.1103/PhysRevLett.64.1955},
isbn = {0031-9007},
issn = {00319007},
journal = {Physical Review Letters},
number = {16},
pages = {1955--1958},
pmid = {10041537},
title = {{Force fields for silicas and aluminophosphates based on ab initio calculations}},
volume = {64},
year = {1990}
}
@article{Bian2014,
abstract = {This paper discusses techniques for solving discrete optimization problems using quantum annealing. Practical issues likely to affect the computation include precision limitations, finite temperature, bounded energy range, sparse connectivity, and small numbers of qubits. To address these concerns we propose a way of finding energy representations with large classical gaps between ground and first excited states, efficient algorithms for mapping non-compatible Ising models into the hardware, and the use of decomposition methods for problems that are too large to fit in hardware. We validate the approach by describing experiments with D-Wave quantum hardware for low density parity check decoding with up to 1000 variables.},
author = {Bian, Zhengbing and Chudak, Fabian and Israel, Robert and Lackey, Brad and Macready, William G. and Roy, Aidan},
doi = {10.3389/fphy.2014.00056},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Discrete optimization,Penalty functions,Quantum annealing,Sparse Ising model},
number = {September},
pages = {1--10},
title = {{Discrete optimization using quantum annealing on sparse Ising models}},
volume = {2},
year = {2014}
}
@article{Snyder2012,
abstract = {Machine learning is used to approximate density functionals. For the model problem of the kinetic energy of noninteracting fermions in 1D, mean absolute errors below 1kcal/mol on test densities similar to the training set are reached with fewer than 100 training densities. A predictor identifies if a test density is within the interpolation region. Via principal component analysis, a projected functional derivative finds highly accurate self-consistent densities. The challenges for application of our method to real electronic structure problems are discussed. {\textcopyright} 2012 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1112.5441},
author = {Snyder, John C. and Rupp, Matthias and Hansen, Katja and M{\"{u}}ller, Klaus Robert and Burke, Kieron},
doi = {10.1103/PhysRevLett.108.253002},
eprint = {1112.5441},
isbn = {8572533357217},
issn = {00319007},
journal = {Physical Review Letters},
month = {jun},
number = {25},
pages = {253002},
pmid = {23004593},
title = {{Finding density functionals with machine learning}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.108.253002 https://link.aps.org/doi/10.1103/PhysRevLett.108.253002},
volume = {108},
year = {2012}
}
@article{Meredig2014,
abstract = {Typically, computational screens for new materials sharply constrain the compositional search space, structural search space, or both, for the sake of tractability. To lift these constraints, we construct a machine learning model from a database of thousands of density functional theory (DFT) calculations. The resulting model can predict the thermodynamic stability of arbitrary compositions without any other input and with six orders of magnitude less computer time than DFT. We use this model to scan roughly 1.6 million candidate compositions for novel ternary compounds (AxByCz), and predict 4500 new stable materials. Our method can be readily applied to other descriptors of interest to accelerate domain-specific materials discovery. {\textcopyright} 2014 American Physical Society.},
author = {Meredig, B. and Agrawal, A. and Kirklin, S. and Saal, J. E. and Doak, J. W. and Thompson, A. and Zhang, K. and Choudhary, A. and Wolverton, C.},
doi = {10.1103/PhysRevB.89.094104},
isbn = {1098-0121},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {9},
pages = {1--7},
title = {{Combinatorial screening for new materials in unconstrained composition space with machine learning}},
volume = {89},
year = {2014}
}
@inproceedings{Frid-Adar2018,
abstract = {In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results significantly increased to 85.7% sensitivity and 92.4% specificity.},
archivePrefix = {arXiv},
arxivId = {1801.02385},
author = {Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2018.8363576},
eprint = {1801.02385},
isbn = {9781538636367},
issn = {19458452},
keywords = {Data augmentation,Generative adversarial network,Image synthesis,Lesion classification,Liver lesions},
month = {may},
pages = {289--293},
publisher = {IEEE Computer Society},
title = {{Synthetic data augmentation using GAN for improved liver lesion classification}},
volume = {2018-April},
year = {2018}
}
@article{Wang2001,
abstract = {A new, general, efficient Monte Carlo (MC) algorithm that offers substantial advantages over existing approaches is presented. For demonstration purposes, the application of the algorithm to the 2D ten state Potts model and Ising model is discussed.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0011174},
author = {Wang, Fugao and Landau, D. P.},
doi = {10.1103/PhysRevLett.86.2050},
eprint = {0011174},
issn = {00319007},
journal = {Physical Review Letters},
number = {10},
pages = {2050--2053},
primaryClass = {cond-mat},
title = {{Efficient, multiple-range random walk algorithm to calculate the density of states}},
volume = {86},
year = {2001}
}
@techreport{Larochelle,
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1273496.1273556},
file = {::},
pages = {473--480},
title = {{An empirical evaluation of deep architectures on problems with many factors of variation}},
volume = {227},
year = {2007}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/bf00992698},
file = {::},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {0$\sim$-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
month = {may},
number = {3-4},
pages = {279--292},
publisher = {Springer Science and Business Media LLC},
title = {{Q-learning}},
url = {https://link.springer.com/article/10.1007/BF00992698},
volume = {8},
year = {1992}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {::},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
month = {may},
pages = {1613--1622},
publisher = {International Machine Learning Society (IMLS)},
title = {{Weight uncertainty in neural networks}},
url = {http://arxiv.org/abs/1505.05424},
volume = {2},
year = {2015}
}
@article{Baldi2016,
abstract = {At the extreme energies of the Large Hadron Collider, massive particles can be produced at such high velocities that their hadronic decays are collimated and the resulting jets overlap. Deducing whether the substructure of an observed jet is due to a low-mass single particle or due to multiple decay objects of a massive particle is an important problem in the analysis of collider data. Traditional approaches have relied on expert features designed to detect energy deposition patterns in the calorimeter, but the complexity of the data make this task an excellent candidate for the application of machine learning tools. The data collected by the detector can be treated as a two-dimensional image, lending itself to the natural application of image classification techniques. In this work, we apply deep neural networks with a mixture of locally connected and fully connected nodes. Our experiments demonstrate that without the aid of expert features, such networks match or modestly outperform the current state-of-the-art approach for discriminating between jets from single hadronic particles and overlapping jets from pairs of collimated hadronic particles, and that such performance gains persist in the presence of pileup interactions.},
archivePrefix = {arXiv},
arxivId = {1603.09349},
author = {Baldi, Pierre and Bauer, Kevin and Eng, Clara and Sadowski, Peter and Whiteson, Daniel},
doi = {10.1103/PhysRevD.93.094034},
eprint = {1603.09349},
issn = {24700029},
journal = {Physical Review D},
number = {9},
pages = {1--12},
title = {{Jet substructure classification in high-energy physics with deep neural networks}},
volume = {93},
year = {2016}
}
@article{Kingma2014,
abstract = {Though based on abstractions of nature, current evolutionary algorithms and artificial life models lack the drive to complexity characteristic of natural evolution. Thus this paper argues that the prevalent fitness-pressure-based abstraction does not capture how natural evolution discovers complexity. Alternatively, this paper proposes that natural evolution can be abstracted as a process that discovers many ways to express the same functionality. That is, all successful organisms must meet the same minimal criteria of survival and reproduction. This abstraction leads to the key idea in this paper: Searching for novel ways of meeting the same minimal criteria, which is an accelerated model of this new abstraction, may be an effective search algorithm. Thus the existing novelty search method, which rewards any new behavior, is extended to enforce minimal criteria. Such minimal criteria novelty search prunes the space of viable behaviors and may often be more efficient than the search for novelty alone. In fact, when compared to the raw search for novelty and traditional fitness-based search in the two maze navigation experiments in this paper, minimal criteria novelty search evolves solutions more consistently. It is possible that refining the evolutionary computation abstraction in this way may lead to solving more ambitious problems and evolving more complex artificial organisms. Copyright 2010 ACM.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Lehman, Joel},
doi = {10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
journal = {Proceedings of the 12th Annual Genetic and Evolutionary Computation Conference, GECCO '10},
keywords = {Artificial life,Evolution of complexity,NEAT,Novelty search},
month = {dec},
pages = {1031--1038},
title = {{Revising the evolutionary computation abstraction: Minimal criteria novelty search}},
url = {http://arxiv.org/abs/1412.6980},
year = {2010}
}
@article{GoogleResearch2015,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
journal = {None},
month = {mar},
number = {212},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
volume = {1},
year = {2016}
}
@article{Berner2019,
abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
archivePrefix = {arXiv},
arxivId = {1912.06680},
author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\c{e}}biak, Przemys{\l}aw Psyho and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'{o}}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and {De Oliveira Pinto}, Henrique Pond{\'{e}} and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
eprint = {1912.06680},
issn = {23318422},
journal = {arXiv},
title = {{Dota 2 with large scale deep reinforcement learning}},
year = {2019}
}
@article{pytorch,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
file = {::},
issn = {23318422},
journal = {arXiv},
month = {dec},
title = {{PyTorch: An imperative style, high-performance deep learning library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
@inproceedings{Erhan,
abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
address = {Montreal},
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
booktitle = {Bernoulli},
number = {1341},
pages = {1--13},
title = {{Visualizing higher-layer features of a deep network}},
url = {http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf},
year = {2009}
}
@article{Chng2016,
abstract = {Machine learning offers an unprecedented perspective for the problem of classifying phases in condensed matter physics. We employ neural-network machine learning techniques to distinguish finite-temperature phases of the strongly correlated fermions on cubic lattices. We show that a three-dimensional convolutional network trained on auxiliary field configurations produced by quantum Monte Carlo simulations of the Hubbard model can correctly predict the magnetic phase diagram of the model at the average density of one (half filling). We then use the network, trained at half filling, to explore the trend in the transition temperature as the system is doped away from half filling. This transfer learning approach predicts that the instability to the magnetic phase extends to at least 5% doping in this region. Our results pave the way for other machine learning applications in correlated quantum many-body systems.},
archivePrefix = {arXiv},
arxivId = {1609.02552},
author = {Ch'Ng, Kelvin and Carrasquilla, Juan and Melko, Roger G. and Khatami, Ehsan},
doi = {10.1103/PhysRevX.7.031038},
eprint = {1609.02552},
issn = {21603308},
journal = {Physical Review X},
month = {aug},
number = {3},
pages = {031038},
title = {{Machine learning phases of strongly correlated fermions}},
url = {http://arxiv.org/abs/1609.02552 https://link.aps.org/doi/10.1103/PhysRevX.7.031038},
volume = {7},
year = {2017}
}
@article{Carrasquilla2017,
abstract = {Condensed-matter physics is the study of the collective behaviour of infinitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits. This complexity is reflected in the size of the state space, which grows exponentially with the number of particles, reminiscent of the 'curse of dimensionality' commonly encountered in machine learning. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recognize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern software libraries, neural networks can be trained to detect multiple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state configurations sampled with Monte Carlo.},
archivePrefix = {arXiv},
arxivId = {1605.01735},
author = {Carrasquilla, Juan and Melko, Roger G.},
doi = {10.1038/nphys4035},
eprint = {1605.01735},
file = {::},
issn = {17452481},
journal = {Nature Physics},
keywords = {Phase transitions and critical phenomena,Statistical physics},
month = {may},
number = {5},
pages = {431--434},
publisher = {Nature Publishing Group},
title = {{Machine learning phases of matter}},
url = {www.nature.com/naturephysics},
volume = {13},
year = {2017}
}
@article{Ikeda2019,
abstract = {Quantum annealing is a promising heuristic method to solve combinatorial optimization problems, and efforts to quantify performance on real-world problems provide insights into how this approach may be best used in practice. We investigate the empirical performance of quantum annealing to solve the Nurse Scheduling Problem (NSP) with hard constraints using the D-Wave 2000Q quantum annealing device. NSP seeks the optimal assignment for a set of nurses to shifts under an accompanying set of constraints on schedule and personnel. After reducing NSP to a novel Ising-type Hamiltonian, we evaluate the solution quality obtained from the D-Wave 2000Q against the constraint requirements as well as the diversity of solutions. For the test problems explored here, our results indicate that quantum annealing recovers satisfying solutions for NSP and suggests the heuristic method is potentially achievable for practical use. Moreover, we observe that solution quality can be greatly improved through the use of reverse annealing, in which it is possible to refine returned results by using the annealing process a second time. We compare the performance of NSP using both forward and reverse annealing methods and describe how this approach might be used in practice.},
archivePrefix = {arXiv},
arxivId = {1904.12139},
author = {Ikeda, Kazuki and Nakamura, Yuma and Humble, Travis S.},
doi = {10.1038/s41598-019-49172-3},
eprint = {1904.12139},
isbn = {4159801949172},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--10},
pmid = {31492936},
title = {{Application of Quantum Annealing to Nurse Scheduling Problem}},
volume = {9},
year = {2019}
}
@inproceedings{kakade2002approximately,
abstract = {Abstract In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used. In this paper, we present the conservative policy iteration algorithm which finds an" approximately" optimal policy, given access to a restart ... \n},
author = {Kakade, Sham and Langford, John},
booktitle = {Proceedings of the 19th International Conference on Machine Learning},
isbn = {1-55860-873-7},
pages = {267--274},
title = {{Approximately Optimal Approximate Reinforcement Learning}},
url = {http://www.cs.cmu.edu/afs/cs/Web/People/jcl/papers/aoarl/Final.pdf},
volume = {2},
year = {2002}
}
@article{Tolstikhin2017,
abstract = {Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
archivePrefix = {arXiv},
arxivId = {1701.02386},
author = {Tolstikhin, Ilya and Gelly, Sylvain and Bousquet, Olivier and Simon-Gabriel, Carl Johann and Sch{\"{o}}lkopf, Bernhard},
eprint = {1701.02386},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {5425--5434},
title = {{AdaGAN: Boosting generative models}},
url = {http://arxiv.org/abs/1701.02386},
volume = {2017-Decem},
year = {2017}
}
@article{Leleu2019,
abstract = {The relaxation of binary spins to analog values has been the subject of much debate in the field of statistical physics, neural networks, and more recently quantum computing, notably because the benefits of using an analog state for finding lower energy spin configurations are usually offset by the negative impact of the improper mapping of the energy function that results from the relaxation. We show that it is possible to destabilize trapping sets of analog states that correspond to local minima of the binary spin Hamiltonian by extending the phase space to include error signals that correct amplitude inhomogeneity of the analog spin states and controlling the divergence of their velocity. Performance of the proposed analog spin system in finding lower energy states is competitive against state-of-the-art heuristics.},
archivePrefix = {arXiv},
arxivId = {1810.12565},
author = {Leleu, Timoth{\'{e}}e and Yamamoto, Yoshihisa and McMahon, Peter L. and Aihara, Kazuyuki},
doi = {10.1103/PhysRevLett.122.040607},
eprint = {1810.12565},
issn = {10797114},
journal = {Physical Review Letters},
month = {feb},
number = {4},
pages = {40607},
pmid = {30768355},
publisher = {American Physical Society},
title = {{Destabilization of Local Minima in Analog Spin Systems by Correction of Amplitude Heterogeneity}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.122.040607},
volume = {122},
year = {2019}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
journal = {arXiv},
month = {jan},
title = {{Wasserstein GaN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@misc{Silver2016a,
abstract = {Games are a great testing ground for developing smarter, more flexible algorithms that have the ability to tackle problems in ways similar to humans. Creating programs that are able to play games better than the best humans has a long history - the first classic game mastered by a computer was noughts and crosses (also known as tic-tac-toe) in 1952 as a PhD candidate's project. Then fell checkers in 1994. Chess was tackled by Deep Blue in 1997. The success isn't limited to board games, either - IBM's Watson won first place on Jeopardy in 2011, and in 2014 our own algorithms learned to play dozens of Atari games just from the raw pixel inputs.},
author = {Hassabis, Demis and Deepmind, Google},
booktitle = {Google Research Blog},
title = {{AlphaGo : Mastering the ancient game of Go with}},
url = {https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html},
year = {2016}
}
@article{Zhang2019,
abstract = {Real Time Strategy (RTS) games require macro strategies as well as micro strategies to obtain satisfactory performance, since it has large state space, action space, and hidden information. This paper presents a novel hierarchical reinforcement learning model for mastering Multiplayer Online Battle Arena (MOBA) games, a sub-genre of RTS games. The contributions are: (1) proposing a hierarchical framework, where agents execute macro strategies by imitation learning and carry out micromanipulations through reinforcement learning, (2) developing a simple self-learning method to get better sample efficiency for training, and (3) designing a dense reward function for multi-agent cooperation in the absence of game engine or Application Programming Interface (API). Finally, various experiments have been performed to validate the superior performance of the proposed method over other state-of-the-art reinforcement learning algorithms. Agents successfully learn to combat and defeat bronze-level built-in AI with 100% win rate, and experiments show that our method can create a competitive multi-agent for a kind of mobile MOBA game King of Glory in 5v5 mode.},
archivePrefix = {arXiv},
arxivId = {1901.08004},
author = {Zhang, Zhijian and Li, Haozheng and Zhang, Luo and Zheng, Tianyin and Zhang, Ting and Hao, Xiong and Chen, Xiaoxin and Chen, Min and Xiao, Fangxu and Zhou, Wei},
eprint = {1901.08004},
issn = {23318422},
journal = {arXiv},
title = {{Hierarchical reinforcement learning for multi-agent MOBA game}},
url = {http://arxiv.org/abs/1901.08004},
year = {2019}
}
@article{Jasrasaria2016,
abstract = {A fundamental problem in applying machine learning techniques for chemical problems is to find suitable representations for molecular and crystal structures. While the structure representations based on atom connectivities are prevalent for molecules, two-dimensional descriptors are not suitable for describing molecular crystals. In this work, we introduce the SFC-M family of feature representations, which are based on Morton space-filling curves, as an alternative means of representing crystal structures. Latent Semantic Indexing (LSI) was employed in a novel setting to reduce sparsity of feature representations. The quality of the SFC-M representations were assessed by using them in combination with artificial neural networks to predict Density Functional Theory (DFT) single point, Ewald summed, lattice, and many-body dispersion energies of 839 organic molecular crystal unit cells from the Cambridge Structural Database that consist of the elements C, H, N, and O. Promising initial results suggest that the SFC-M representations merit further exploration to improve its ability to predict solid-state properties of organic crystal structures},
archivePrefix = {arXiv},
arxivId = {1608.05747},
author = {Jasrasaria, Dipti and Pyzer-Knapp, Edward O. and Rappoport, Dmitrij and Aspuru-Guzik, Alan},
eprint = {1608.05747},
title = {{Space-Filling Curves as a Novel Crystal Structure Representation for Machine Learning Models}},
url = {http://arxiv.org/abs/1608.05747},
year = {2016}
}
@article{Masters2018,
abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size m. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between m = 2 and m = 32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
archivePrefix = {arXiv},
arxivId = {1804.07612},
author = {Masters, Dominic and Luschi, Carlo},
eprint = {1804.07612},
file = {::},
issn = {23318422},
journal = {arXiv},
month = {apr},
publisher = {arXiv},
title = {{Revisiting small batch training for deep neural networks}},
url = {http://arxiv.org/abs/1804.07612},
year = {2018}
}
@article{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
file = {::},
month = {sep},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@article{Kucharski1992,
abstract = {A general implementation of the coupled-cluster (CC) single, double, triple, and quadruple excitation (CCSDTQ) method is presented and applied to several molecules, including BH, HF, H2O, and CO with DZP basis sets. Comparisons with full CI show average errors to be 14 $\mu$hartree at equilibrium and 26 $\mu$hartree at twice Re. CCSDTQ is exact for four electrons and is the first CC method correct through sixth order in perturbation theory. {\textcopyright} 1992 American Institute of Physics.},
author = {Kucharski, Stanislaw A. and Bartlett, Rodney J.},
doi = {10.1063/1.463930},
issn = {00219606},
journal = {The Journal of Chemical Physics},
keywords = {BORON HYDRIDES,CARBON MONOXIDE,COUPLING,ELECTRON CORRELATION,ERRORS,HYDROFLUORIC ACID,MANY-BODY PROBLEM,WATER,WAVE FUNCTIONS},
month = {sep},
number = {6},
pages = {4282--4288},
title = {{The coupled-cluster single, double, triple, and quadruple excitation method}},
url = {http://scitation.aip.org/content/aip/journal/jcp/97/6/10.1063/1.463930 http://aip.scitation.org/doi/10.1063/1.463930},
volume = {97},
year = {1992}
}
@article{Behler2014,
abstract = {The development of interatomic potentials employing artificial neural networks has seen tremendous progress in recent years. While until recently the applicability of neural network potentials (NNPs) has been restricted to low-dimensional systems, this limitation has now been overcome and high-dimensional NNPs can be used in large-scale molecular dynamics simulations of thousands of atoms. NNPs are constructed by adjusting a set of parameters using data from electronic structure calculations, and in many cases energies and forces can be obtained with very high accuracy. Therefore, NNP-based simulation results are often very close to those gained by a direct application of first-principles methods. In this review, the basic methodology of high-dimensional NNPs will be presented with a special focus on the scope and the remaining limitations of this approach. The development of NNPs requires substantial computational effort as typically thousands of reference calculations are required. Still, if the problem to be studied involves very large systems or long simulation times this overhead is regained quickly. Further, the method is still limited to systems containing about three or four chemical elements due to the rapidly increasing complexity of the configuration space, although many atoms of each species can be present. Due to the ability of NNPs to describe even extremely complex atomic configurations with excellent accuracy irrespective of the nature of the atomic interactions, they represent a general and therefore widely applicable technique, e.g. for addressing problems in materials science, for investigating properties of interfaces, and for studying solvation processes. {\textcopyright} 2014 IOP Publishing Ltd.},
author = {Behler, J.},
doi = {10.1088/0953-8984/26/18/183001},
isbn = {0953-8984},
issn = {1361648X},
journal = {Journal of Physics Condensed Matter},
keywords = {interatomic potentials,molecular dynamics,neural networks},
number = {18},
pages = {183001},
pmid = {24758952},
title = {{Representing potential energy surfaces by high-dimensional neural network potentials}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24758952%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/24758952%5Cnhttp://iopscience.iop.org/0953-8984/26/18/183001%5Cnhttp://stacks.iop.org/0953-8984/26/i=18/a=183001?key=crossref.e2110f4e5f0e5ce5600d2eb9e27e4391},
volume = {26},
year = {2014}
}
@article{Ac2016,
abstract = {In this paper, four artificial neural network (ANN) models [i.e., feed-forward neural network (FFNN), function fitting neural network (FITNET), cascade-forward neural network (CFNN) and generalized regression neural network] have been developed for atomic coordinate prediction of carbon nanotubes (CNTs). The research reported in this study has two primary objectives: (1) to develop ANN prediction models that calculate atomic coordinates of CNTs instead of using any simulation software and (2) to use results of the ANN models as an initial value of atomic coordinates for reducing number of iterations in calculation process. The dataset consisting of 10,721 data samples was created by combining the atomic coordinates of elements and chiral vectors using BIOVIA Materials Studio CASTEP (CASTEP) software. All prediction models yield very low mean squared normalized error and mean absolute error rates. Multiple correlation coefficient (R) results of FITNET, FFNN and CFNN models are close to 1. Compared with CASTEP, calculation times decrease from days to minutes. It would seem possible to predict CNTs' atomic coordinates using ANN models can be successfully used instead of mathematical calculations.},
author = {Acı, Mehmet and Avcı, Mutlu},
doi = {10.1007/s00339-016-0153-1},
issn = {14320630},
journal = {Applied Physics A: Materials Science and Processing},
number = {7},
title = {{Artificial neural network approach for atomic coordinate prediction of carbon nanotubes}},
volume = {122},
year = {2016}
}
@article{Tanaka2016,
abstract = {A convolutional neural network (CNN) is designed to study correlation between the temperature and the spin configuration of the two-dimensional Ising model. Our CNN is able to find the characteristic feature of the phase transition without prior knowledge. Also a novel order parameter on the basis of the CNN is introduced to identify the location of the critical temperature; the result is found to be consistent with the exact value.},
archivePrefix = {arXiv},
arxivId = {1609.09087},
author = {Tanaka, Akinori and Tomiya, Akio},
doi = {10.7566/JPSJ.86.063001},
eprint = {1609.09087},
issn = {13474073},
journal = {Journal of the Physical Society of Japan},
number = {6},
pages = {1--7},
title = {{Detection of phase transition via convolutional neural networks}},
url = {http://arxiv.org/abs/1609.09087},
volume = {86},
year = {2017}
}
@techreport{Crawford,
abstract = {We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs are trained more effectively than restricted Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This method also outperforms the reinforcement learning method that uses RBMs.},
archivePrefix = {arXiv},
arxivId = {1612.05695},
author = {Crawford, Daniel and Levit, Anna and Ghadermarzy, Navid and Oberoi, Jaspreet S. and Ronagh, Pooya},
booktitle = {Quantum Information and Computation},
doi = {10.26421/qic18.1-2-3},
eprint = {1612.05695},
file = {::},
issn = {15337146},
keywords = {Deep Boltzmann machine,General Boltzmann machine,Machine learning,Markov decision process,Neuro-dynamic programming,Quantum Boltzmann machine,Quantum monte carlo simulation,Reinforcement learning,Restricted Boltzmann machine,Simulated quantum annealing},
number = {1-2},
pages = {51--74},
title = {{Reinforcement learning using quantum boltzmann machines}},
volume = {18},
year = {2018}
}
@book{Hebb1949,
abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists--the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology--a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
author = {Hebb, D.O.},
booktitle = {The Organization of Behavior},
doi = {10.4324/9781410612403},
isbn = {978-0805843002},
publisher = {Wiley},
title = {{The Organization of Behavior}},
year = {2005}
}
@article{Kim2017,
abstract = {While humans easily recognize relations between data from diff{\'{e}}rent domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations when given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
eprint = {1703.05192},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {2941--2949},
title = {{Learning to discover cross-domain relations with generative adversarial networks}},
url = {http://arxiv.org/abs/1703.05192},
volume = {4},
year = {2017}
}
@article{Ciresan2010,
abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning. {\textcopyright} 2010 Massachusetts Institute of Technology.},
author = {Cireşan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/NECO_a_00052},
issn = {08997667},
journal = {Neural Computation},
month = {dec},
number = {12},
pages = {3207--3220},
pmid = {20858131},
title = {{Deep, big, simple neural nets for handwritten digit recognition}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00052},
volume = {22},
year = {2010}
}
@article{Ludwig2007,
abstract = {We outline a hybrid multiscale approach for the construction of ab initio potential energy surfaces (PESs) useful for performing six-dimensional (6D) classical or quantum mechanical molecular dynamics (MD) simulations of diatomic molecules reacting at single crystal surfaces. The algorithm implements concepts from the corrugation reduction procedure, which reduces energetic variation in the PES, and uses neural networks for interpolation of smoothed ab initio data. A novelty sampling scheme is implemented and used to identify configurations that are most likely to be predicted inaccurately by the neural network. This hybrid multiscale approach, which couples PES construction at the electronic structure level to MD simulations at the atomistic scale, reduces the number of density functional theory (DFT) calculations needed to specify an accurate PES. Due to the iterative nature of the novelty sampling algorithm, it is possible to obtain a quantitative measure of the convergence of the PES with respect to the number of ab initio calculations used to train the neural network. We demonstrate the algorithm by first applying it to two analytic potentials, which model the H2 Pt (111) and H2 Cu (111) systems. These potentials are of the corrugated London-Eyring-Polanyi-Sato form, which are based on DFT calculations, but are not globally accurate. After demonstrating the convergence of the PES using these simple potentials, we use DFT calculations directly and obtain converged semiclassical trajectories for the H2 Pt (111) system at the PW91/generalized gradient approximation level. We obtain a converged PES for a 6D hydrogen-surface dissociation reaction using novelty sampling coupled directly to DFT. These results, in excellent agreement with experiments and previous theoretical work, are compared to previous simulations in order to explore the sensitivity of the PES (and therefore MD) to the choice of exchange and correlation functional. Despite having a lower energetic corrugation in our PES, we obtain a broader reaction probability curve than previous simulations, which is attributed to increased geometric corrugation in the PES and the effect of nonparallel dissociation pathways. {\textcopyright} 2007 American Institute of Physics.},
author = {Ludwig, Jeffery and Vlachos, Dionisios G.},
doi = {10.1063/1.2794338},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {15},
pages = {154716},
pmid = {17949200},
title = {{Ab initio molecular dynamics of hydrogen dissociation on metal surfaces using neural networks and novelty sampling}},
volume = {127},
year = {2007}
}
@article{Kmills2017,
abstract = {We present a physically-motivated topology of a deep neural network that can efficiently infer extensive parameters (such as energy, entropy, or number of particles) of arbitrarily large systems, doing so with scaling. We use a form of domain decomposition for training and inference, where each sub-domain (tile) is comprised of a non-overlapping focus region surrounded by an overlapping context region. The size of these regions is motivated by the physical interaction length scales of the problem. We demonstrate the application of EDNNs to three physical systems: the Ising model and two hexagonal/graphene-like datasets. In the latter, an EDNN was able to make total energy predictions of a 60 atoms system, with comparable accuracy to density functional theory (DFT), in 57 milliseconds. Additionally EDNNs are well suited for massively parallel evaluation, as no communication is necessary during neural network evaluation. We demonstrate that EDNNs can be used to make an energy prediction of a two-dimensional 35.2 million atom system, over 1.0 $\mu$m2 of material, at an accuracy comparable to DFT, in under 25 minutes. Such a system exists on a length scale visible with optical microscopy and larger than some living organisms.},
archivePrefix = {arXiv},
arxivId = {1708.06686},
author = {Mills, Kyle and Ryczko, Kevin and Luchak, Iryna and Domurad, Adam and Beeler, Chris and Tamblyn, Isaac},
doi = {10.1039/C8SC04578J},
eprint = {1708.06686},
issn = {20416539},
journal = {Chemical Science},
month = {aug},
number = {15},
pages = {4129--4140},
title = {{Extensive deep neural networks for transferring small scale learning to large scale systems}},
url = {https://arxiv.org/pdf/1708.06686.pdf http://arxiv.org/abs/1708.06686 http://xlink.rsc.org/?DOI=C8SC04578J},
volume = {10},
year = {2019}
}
@article{Linnainmaa1970,
author = {Linnainmaa, Seppo},
institution = {University of Helsinki},
title = {{Alogritmin kumulatiivinen py{\"{o}}ristysvirhe yksitt{\"{a}}isten py{\"{o}}ristysvirheiden Taylor-kehitelm{\"{a}}n{\"{a}}}},
year = {1970}
}
@article{Worrall2016,
abstract = {Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch. H-Nets use a rich, parameter-efficient and fixed computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.},
archivePrefix = {arXiv},
arxivId = {1612.04642},
author = {Worrall, Daniel E. and Garbin, Stephan J. and Turmukhambetov, Daniyar and Brostow, Gabriel J.},
doi = {10.1109/CVPR.2017.758},
eprint = {1612.04642},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
month = {dec},
pages = {7168--7177},
title = {{Harmonic deep: Networks translation and rotation equivariance}},
url = {http://arxiv.org/abs/1612.04642},
volume = {2017-Janua},
year = {2017}
}
@article{Baldi2014,
abstract = {Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification problems, hence machine-learning approaches are often used. Standard approaches have relied on 'shallow' machine-learning models that have a limited capacity to learn complex nonlinear functions of the inputs, and rely on a painstaking search through manually constructed nonlinear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Here, using benchmark data sets, we show that deep-learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8% over the best current approaches. This demonstrates that deep-learning approaches can improve the power of collider searches for exotic particles. {\textcopyright} 2014 Macmillan Publishers Limited.},
archivePrefix = {arXiv},
arxivId = {1402.4735},
author = {Baldi, P. and Sadowski, P. and Whiteson, D.},
doi = {10.1038/ncomms5308},
eprint = {1402.4735},
isbn = {2041-1723 (Electronic)2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
pages = {4308},
pmid = {24986233},
publisher = {Nature Publishing Group},
title = {{Searching for exotic particles in high-energy physics with deep learning}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24986233},
volume = {5},
year = {2014}
}
@article{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
isbn = {9781450330633},
issn = {10636919},
journal = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
keywords = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
pages = {675--678},
pmid = {18267787},
title = {{Caffe: Convolutional architecture for fast feature embedding}},
url = {http://arxiv.org/abs/1408.5093},
year = {2014}
}
@article{Onsager1944,
abstract = {The partition function of a two-dimensional "ferromagnetic" with scalar "spins" (Ising model) is computed rigorously for the case of vanishing field. The eigenwert problem involved in the corresponding computation for a long strip crystal of finite width (n atoms), joined straight to itself around a cylinder, is solved by direct product decomposition; in the special case n= an integral replaces a sum. The choice of different interaction energies (J,J) in the (0 1) and (1 0) directions does not complicate the problem. The two-way infinite crystal has an order-disorder transition at a temperature T=Tc given by the condition sinh(2JkTc) sinh(2JkTc)=1. The energy is a continuous function of T; but the specific heat becomes infinite as -log |T-Tc|. For strips of finite width, the maximum of the specific heat increases linearly with log n. The order-converting dual transformation invented by Kramers and Wannier effects a simple automorphism of the basis of the quaternion algebra which is natural to the problem in hand. In addition to the thermodynamic properties of the massive crystal, the free energy of a (0 1) boundary between areas of opposite order is computed; on this basis the mean ordered length of a strip crystal is (exp (2JkT) tanh(2JkT))n. {\textcopyright} 1944 The American Physical Society.},
author = {Onsager, Lars},
doi = {10.1103/PhysRev.65.117},
isbn = {0031-899X$\$r1536-6065},
issn = {0031899X},
journal = {Physical Review},
number = {3-4},
pages = {117--149},
pmid = {18556531},
title = {{Crystal statistics. I. A two-dimensional model with an order-disorder transition}},
volume = {65},
year = {1944}
}
@article{Gomez2008,
abstract = {We have performed Hartree-Fock calculations of the electronic structure of N ≤ 10 electrons in a quantum dot modeled with a confining Gaussian potential well. We discuss the conditions for the stability of N bound electrons in the system. We show that the most relevant parameter determining the number of bound electrons is V0 R2. Such a feature arises from widely valid scaling properties of the confining potential. Gaussian Quantum dots having N = 2, 5, and 8 electrons are particularly stable in agreement with the Hund rule. The shell structure becomes less and less noticeable as the well radius increases. {\textcopyright} Versita Warsaw and Springer-Verlag Berlin Heidelberg 2009.},
archivePrefix = {arXiv},
arxivId = {0804.1961},
author = {Gomez, Sergio S. and Romero, Rodolfo H.},
doi = {10.2478/s11534-008-0132-z},
eprint = {0804.1961},
issn = {18951082},
journal = {Central European Journal of Physics},
keywords = {Electronic structure,Gaussian potential,Quantum dots},
number = {1},
pages = {12--21},
title = {{Few-electron semiconductor quantum dots with gaussian confinement}},
url = {http://arxiv.org/abs/0804.1961},
volume = {7},
year = {2009}
}
@article{Martonak2002,
abstract = {Quantum annealing was recently found experimentally in a disordered spin- (formula presented) magnet to be more effective than its classical, thermal counterpart. We use the random two-dimensional Ising model as a test example and perform on it both classical and quantum (path-integral) Monte Carlo annealing. A systematic study of the dependence of the final residual energy on the annealing Monte Carlo time quantitatively demonstrates the superiority of quantum relative to classical annealing in this system. In order to determine the parameter regime for optimal efficiency of the quantum annealing procedure we explore a range of values of Trotter slice number P and temperature T. This identifies two different regimes of freezing with respect to efficiency of the algorithm, and leads to useful guidelines for the optimal choice of quantum annealing parameters. {\textcopyright} 2002 The American Physical Society.},
author = {Martoň{\'{a}}k, Roman and Santoro, Giuseppe E. and Tosatti, Erio},
doi = {10.1103/PhysRevB.66.094203},
issn = {1550235X},
journal = {Physical Review B - Condensed Matter and Materials Physics},
number = {9},
pages = {1--8},
title = {{Quantum annealing by the path-integral Monte Carlo method: The two-dimensional random Ising model}},
volume = {66},
year = {2002}
}
@article{Li2012,
abstract = {A DFT-SOFM-RBFNN method is proposed to improve the accuracy of DFT calculations on Y-NO (Y = C, N, O, S) homolysis bond dissociation energies (BDE) by combining density functional theory (DFT) and artificial intelligence/machine learning methods, which consist of self-organizing feature mapping neural networks (SOFMNN) and radial basis function neural networks (RBFNN). A descriptor refinement step including SOFMNN clustering analysis and correlation analysis is implemented. The SOFMNN clustering analysis is applied to classify descriptors, and the representative descriptors in the groups are selected as neural network inputs according to their closeness to the experimental values through correlation analysis. Redundant descriptors and intuitively biased choices of descriptors can be avoided by this newly introduced step. Using RBFNN calculation with the selected descriptors, chemical accuracy (≤1 kcal{\textperiodcentered}mol-1) is achieved for all 92 calculated organic Y-NO homolysis BDE calculated by DFT-B3LYP, and the mean absolute deviations (MADs) of the B3LYP/6-31G(d) and B3LYP/STO-3G methods are reduced from 4.45 and 10.53 kcal{\textperiodcentered}mol-1 to 0.15 and 0.18 kcal{\textperiodcentered}mol-1, respectively. The improved results for the minimal basis set STO-3G reach the same accuracy as those of 6-31G(d), and thus B3LYP calculation with the minimal basis set is recommended to be used for minimizing the computational cost and to expand the applications to large molecular systems. Further extrapolation tests are performed with six molecules (two containing Si-NO bonds and two containing fluorine), and the accuracy of the tests was within 1 kcal{\textperiodcentered}mol-1. This study shows that DFT-SOFM-RBFNN is an efficient and highly accurate method for Y-NO homolysis BDE. The method may be used as a tool to design new NO carrier molecules. {\textcopyright} 2012 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Li, Hong Zhi and Hu, Li Hong and Tao, Wei and Gao, Ting and Li, Hui and Lu, Ying Hua and Su, Zhong Min},
doi = {10.3390/ijms13078051},
isbn = {1661-6596},
issn = {16616596},
journal = {International Journal of Molecular Sciences},
keywords = {Density functional theory,Homolysis bond dissociation energies,Radial basis function neural network,Self-organizing feature mapping neural network,Y-NO bond},
number = {7},
pages = {8051--8070},
pmid = {22942689},
title = {{A Promising tool to achieve chemical accuracy for density functional theory calculations on Y-NO homolysis bond dissociation energies}},
volume = {13},
year = {2012}
}
@article{Venturelli2015,
abstract = {A quantum annealing solver for the renowned job-shop scheduling problem (JSP) is presented in detail. After formulating the problem as a time-indexed quadratic unconstrained binary optimization problem, several pre-processing and graph embedding strategies are employed to compile optimally parametrized families of the JSP for scheduling instances of up to six jobs and six machines on the D-Wave Systems Vesuvius processor. Problem simplifications and partitioning algorithms, including variable pruning and running strategies that consider tailored binary searches, are discussed and the results from the processor are compared against state-of-the-art global-optimum solvers.},
archivePrefix = {arXiv},
arxivId = {1506.08479},
author = {Venturelli, Davide and Marchand, Dominic J. J. and Rojo, Galo},
eprint = {1506.08479},
month = {jun},
pages = {1--15},
title = {{Quantum Annealing Implementation of Job-Shop Scheduling}},
url = {http://arxiv.org/abs/1506.08479},
year = {2015}
}
@article{Aurisano2016a,
abstract = {Convolutional neural networks (CNNs) have been widely applied in the computer vision community to solve complex problems in image recognition and analysis. We describe an application of the CNN technology to the problem of identifying particle interactions in sampling calorimeters used commonly in high energy physics and high energy neutrino physics in particular. Following a discussion of the core concepts of CNNs and recent innovations in CNN architectures related to the field of deep learning, we outline a specific application to the NOvA neutrino detector. This algorithm, CVN (Convolutional Visual Network) identifies neutrino interactions based on their topology without the need for detailed reconstruction and outperforms algorithms currently in use by the NOvA collaboration.},
archivePrefix = {arXiv},
arxivId = {1604.01444},
author = {Aurisano, A. and Radovic, A. and Rocco, D. and Himmel, A. and Messier, M. D. and Niner, E. and Pawloski, G. and Psihas, F. and Sousa, A. and Vahle, P.},
doi = {10.1088/1748-0221/11/09/P09001},
eprint = {1604.01444},
file = {:home/kmills/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aurisano et al. - 2016 - A convolutional neural network neutrino event classifier.pdf:pdf},
issn = {17480221},
journal = {Journal of Instrumentation},
keywords = {Neutrino detectors,Particle identification methods,Particle tracking detectors,Pattern recognition, cluster finding, calibration,Pattern recognition, cluster finding, calibration },
number = {9},
pages = {P09001--P09001},
title = {{A convolutional neural network neutrino event classifier}},
url = {http://arxiv.org/abs/1604.01444%5Cnhttp://dx.doi.org/10.1088/1748-0221/11/09/P09001%5Cnhttp://stacks.iop.org/1748-0221/11/i=09/a=P09001?key=crossref.6cf3f0079498e2663f11218fe22ef9b7},
volume = {11},
year = {2016}
}
@article{Ponder2003,
abstract = {The chapter focuses on a general description of the force fields that are most commonly used at present, and it gives an indication of the directions of current research that may yield better functions in the near future. After a brief survey of current models, mostly generated during the 1990s, the focus of the chapter is on the general directions the field is taking in developing new models. The most commonly used protein force fields incorporate a relatively simple potential energy function: The emphasis is on the use of continuum methods to model the electrostatic effects of hydration and the introduction of polarizability to model the electronic response to changes in the environment. Some of the history and performance of widely used protein force fields based on an equation on simplest potential energy function or closely related equations are reviewed. The chapter outlines some promising developments that go beyond this, primarily by altering the way electrostatic interactions are treated. The use of atomic multipoles and off-center charge distributions, as well as attempts to incorporate electronic polarizability, are also discussed in the chapter.},
author = {Ponder, Jay W. and Case, David A.},
doi = {10.1016/S0065-3233(03)66002-X},
isbn = {9780120342662},
issn = {00653233},
journal = {Advances in Protein Chemistry},
pages = {27--85},
pmid = {14631816},
title = {{Force fields for protein simulations}},
volume = {66},
year = {2003}
}
@article{Lin2016,
abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through “cheap learning” with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various “no-flattening theorems” showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss; for example, we show that n variables cannot be multiplied using fewer than 2 n neurons in a single hidden layer.},
archivePrefix = {arXiv},
arxivId = {1608.08225},
author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
doi = {10.1007/s10955-017-1836-5},
eprint = {1608.08225},
isbn = {9781627480031},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Artificial neural networks,Deep learning,Statistical physics},
month = {aug},
number = {6},
pages = {1223--1247},
pmid = {25246403},
publisher = {Springer New York LLC},
title = {{Why Does Deep and Cheap Learning Work So Well?}},
url = {https://link.springer.com/article/10.1007/s10955-017-1836-5 http://arxiv.org/abs/1608.08225 http://ieeexplore.ieee.org/document/726791/},
volume = {168},
year = {2017}
}
@techreport{Hafner,
abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
archivePrefix = {arXiv},
arxivId = {1811.04551},
author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1811.04551},
file = {::},
isbn = {9781510886988},
pages = {4528--4547},
title = {{Learning latent dynamics for planning from pixels}},
volume = {2019-June},
year = {2019}
}
@techreport{Zhang,
abstract = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
archivePrefix = {arXiv},
arxivId = {1710.10916},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N.},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2018.2856256},
eprint = {1710.10916},
file = {::},
issn = {19393539},
keywords = {Generative models,generative adversarial networks (GANs),multi-distribution approximation,multi-stage GANs,photo-realistic image generation,text-to-image synthesis},
number = {8},
pages = {1947--1962},
pmid = {30010548},
title = {{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {https://github.com/hanzhanggit/StackGAN.},
volume = {41},
year = {2019}
}
@techreport{Glorot,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. Copyright 2010 by the authors.},
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Journal of Machine Learning Research},
file = {::},
issn = {15324435},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://www.iro.umontreal.},
volume = {9},
year = {2010}
}
@article{Johnson2011,
abstract = {Many interesting but practically intractable problems can be reduced to that of finding the ground state of a system of interacting spins; however, finding such a ground state remains computationally difficult. It is believed that the ground state of some naturally occurring spin systems can be effectively attained through a process called quantum annealing. If it could be harnessed, quantum annealing might improve on known methods for solving certain types of problem. However, physical investigation of quantum annealing has been largely confined to microscopic spins in condensed-matter systems. Here we use quantum annealing to find the ground state of an artificial Ising spin system comprising an array of eight superconducting flux quantum bits with programmable spin-spin couplings. We observe a clear signature of quantum annealing, distinguishable from classical thermal annealing through the temperature dependence of the time at which the system dynamics freezes. Our implementation can be configured in situ to realize a wide variety of different spin networks, each of which can be monitored as it moves towards a low-energy configuration. This programmable artificial spin network bridges the gap between the theoretical study of ideal isolated spin networks and the experimental investigation of bulk magnetic samples. Moreover, with an increased number of spins, such a system may provide a practical physical means to implement a quantum algorithm, possibly allowing more-effective approaches to solving certain classes of hard combinatorial optimization problems. {\textcopyright} 2011 Macmillan Publishers Limited. All rights reserved.},
author = {Johnson, M. W. and Amin, M. H.S. and Gildert, S. and Lanting, T. and Hamze, F. and Dickson, N. and Harris, R. and Berkley, A. J. and Johansson, J. and Bunyk, P. and Chapple, E. M. and Enderud, C. and Hilton, J. P. and Karimi, K. and Ladizinsky, E. and Ladizinsky, N. and Oh, T. and Perminov, I. and Rich, C. and Thom, M. C. and Tolkacheva, E. and Truncik, C. J.S. and Uchaikin, S. and Wang, J. and Wilson, B. and Rose, G.},
doi = {10.1038/nature10012},
issn = {00280836},
journal = {Nature},
number = {7346},
pages = {194--198},
title = {{Quantum annealing with manufactured spins}},
volume = {473},
year = {2011}
}
@techreport{Hinton,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
file = {::},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Kirkpatrick1983a,
abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
doi = {10.1126/science.220.4598.671},
issn = {00368075},
journal = {Science},
month = {may},
number = {4598},
pages = {671--680},
pmid = {17813860},
title = {{Optimization by simulated annealing}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.220.4598.671},
volume = {220},
year = {1983}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Communications of the ACM},
doi = {10.1145/3065386},
issn = {15577317},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
volume = {60},
year = {2017}
}
@article{Ballard2016,
abstract = {Methods developed to explore and characterise potential energy landscapes are applied to the corresponding landscapes obtained from optimisation of a cost function in machine learning. We consider neural network predictions for the outcome of local geometry optimisation in a triatomic cluster, where four distinct local minima exist. The accuracy of the predictions is compared for fits using data from single and multiple points in the series of atomic configurations resulting from local geometry optimisation and for alternative neural networks. The machine learning solution landscapes are visualised using disconnectivity graphs, and signatures in the effective heat capacity are analysed in terms of distributions of local minima and their properties.},
author = {Ballard, Andrew J. and Stevenson, Jacob D. and Das, Ritankar and Wales, David J.},
doi = {10.1063/1.4944672},
isbn = {1089-7690 (Electronic)\r0021-9606 (Linking)},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {12},
pages = {124119},
pmid = {27036439},
title = {{Energy landscapes for a machine learning application to series data}},
url = {http://scitation.aip.org/content/aip/journal/jcp/144/12/10.1063/1.4944672},
volume = {144},
year = {2016}
}
@article{Metz2016,
abstract = {We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.},
archivePrefix = {arXiv},
arxivId = {1606.07536},
author = {Liu, Ming Yu and Tuzel, Oncel},
eprint = {1606.07536},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {469--477},
pmid = {202927},
title = {{Coupled generative adversarial networks}},
url = {http://arxiv.org/abs/1611.02163},
year = {2016}
}
@techreport{Snoek,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1206.2944},
file = {::},
isbn = {9781627480031},
issn = {10495258},
pages = {2951--2959},
title = {{Practical Bayesian optimization of machine learning algorithms}},
volume = {4},
year = {2012}
}
@article{Vinyals2019,
abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.},
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
doi = {10.1038/s41586-019-1724-z},
issn = {14764687},
journal = {Nature},
month = {oct},
number = {7782},
pages = {350--354},
pmid = {31666705},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {http://www.nature.com/articles/s41586-019-1724-z},
volume = {575},
year = {2019}
}
@article{Mills2020,
abstract = {We demonstrate the use of a regressive upscaling generative adversarial network (RUGAN) as an effective way to sample state space for hexagonal porous graphene sheets. The RUGAN can, after being trained on a set of small-scale examples, generate new, energetically relevant microstates (atomic configurations). The RUGAN can generate configurations across a continuum of total energy values and produce configurations at requested energy values. The microstates produced respect periodic boundary conditions, and importantly, the fully convolutional nature of the generator allows the generation of arbitrarily large microstates, after being trained on only a small-scale data set.},
author = {Mills, Kyle and Casert, Corneel and Tamblyn, Isaac},
doi = {10.1021/acs.jpcc.0c06673},
issn = {19327455},
journal = {Journal of Physical Chemistry C},
month = {oct},
number = {42},
pages = {23158--23163},
title = {{Adversarial Generation of Mesoscale Surfaces from Small-Scale Chemical Motifs}},
url = {https://pubs.acs.org/doi/10.1021/acs.jpcc.0c06673},
volume = {124},
year = {2020}
}
@article{Perez2007,
abstract = {We present here the parmbsc0 force field, a refinement of the AMBER parm99 force field, where emphasis has been made on the correct representation of the $\alpha$/$\gamma$ concerted rotation in nucleic acids (NAs). The modified force field corrects overpopulations of the $\alpha$/$\gamma$ = (g+,t) backbone that were seen in long (more than 10 ns) simulations with previous AMBER parameter sets (parm94-99). The force field has been derived by fitting to high-level quantum mechanical data and verified by comparison with very high-level quantum mechanical calculations and by a very extensive comparison between simulations and experimental data. The set of validation simulations includes two of the longest trajectories published to date for the DNA duplex (200 ns each) and the largest variety of NA structures studied to date (15 different NA families and 97 individual structures). The total simulation time used to validate the force field includes near 1 $\mu$s of state-of-the-art molecular dynamics simulations in aqueous solution. {\textcopyright} 2007 by the Biophysical Society.},
author = {P{\'{e}}rez, Alberto and March{\'{a}}n, Iv{\'{a}}n and Svozil, Daniel and Sponer, Jiri and Cheatham, Thomas E. and Laughton, Charles A. and Orozco, Modesto},
doi = {10.1529/biophysj.106.097782},
isbn = {0006-3495 (Print)\n0006-3495 (Linking)},
issn = {00063495},
journal = {Biophysical Journal},
keywords = {Computer Simulation,DNA,DNA: chemistry,Models, Molecular,Nucleic Acid Conformation,RNA,RNA: chemistry},
month = {jun},
number = {11},
pages = {3817--3829},
pmid = {17351000},
title = {{Refinement of the AMBER force field for nucleic acids: Improving the description of $\alpha$/$\gamma$ conformers}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1868997&tool=pmcentrez&rendertype=abstract},
volume = {92},
year = {2007}
}
@article{Kotsiantis2007,
abstract = {Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
author = {Kotsiantis, S. B.},
doi = {10.31449/inf.v31i3.148},
issn = {03505596},
journal = {Informatica (Ljubljana)},
keywords = {Classifiers,Data mining techniques,Intelligent data analysis,Learning algorithms},
number = {3},
pages = {249--268},
title = {{Supervised machine learning: A review of classification techniques}},
volume = {31},
year = {2007}
}
@article{Gal2015a,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
isbn = {9781510829008},
issn = {1938-7228},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {1651--1660},
pmid = {88045},
title = {{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}},
url = {http://arxiv.org/abs/1506.02142},
volume = {3},
year = {2016}
}
@techreport{Carreira-Perpinan,
abstract = {Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called "contrastive divergence" (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution.},
author = {Carreira-Perpi{\~{n}}{\'{a}}n, Miguel {\'{A}} and Hinton, Geoffrey E.},
booktitle = {AISTATS 2005 - Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics},
file = {::},
isbn = {097273581X},
pages = {33--40},
title = {{On contrastive divergence learning}},
year = {2005}
}
@article{Carrasquilla2016,
abstract = {Condensed-matter physics is the study of the collective behaviour of infinitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits. This complexity is reflected in the size of the state space, which grows exponentially with the number of particles, reminiscent of the 'curse of dimensionality' commonly encountered in machine learning. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recognize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern software libraries, neural networks can be trained to detect multiple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state configurations sampled with Monte Carlo.},
archivePrefix = {arXiv},
arxivId = {1605.01735},
author = {Carrasquilla, Juan and Melko, Roger G.},
doi = {10.1038/nphys4035},
eprint = {1605.01735},
issn = {17452481},
journal = {Nature Physics},
month = {feb},
number = {5},
pages = {431--434},
title = {{Machine learning phases of matter}},
url = {http://arxiv.org/abs/1605.01735 http://www.nature.com/doifinder/10.1038/nphys4035},
volume = {13},
year = {2017}
}
@article{dirac,
abstract = { The general theory of quantum mechanics is now almost complete, the imperfections that still remain being in connection with the exact fitting in of the theory with relativity ideas. These give rise to difficulties only when high-speed particles are involved, and are therefore of no importance in the consideration of atomic and molecular structure and ordinary chemical reactions, in which it is, indeed, usually sufficiently accurate if one neglects relativity variation of mass with velocity and assumes only Coulomb forces between the various electrons and atomic nuclei. The underlying physical laws necessary for the mathematical theory of a large part of physics and the whole of chemistry are thus completely known, and the difficulty is only that the exact application of these laws leads to equations much too complicated to be soluble. It there fore becomes desirable that approximate practical methods of applying quantum mechanics should be developed, which can lead to an explanation of the main features of complex atomic systems without too much computation. Already before the arrival of quantum mechanics there existed a theory of atomic structure, based on Bohr's ideas of quantised orbits, which was fairly successful in a wide field. To get agreement with experiment it was found necessary to introduce the spin of the electron, giving a doubling in the number of orbits of an electron in an atom. With the help of this spin and Pauli's exclusion principle, a satisfactory theory of multiplet terms was obtained when one made the additional assumption that the electrons in an atom all set themselves with their spins parallel or antiparallel. If s denoted the magnitude of the resultant spin angular momentum, this s was combined vectorially with the resultant orbital angular momentum l to give a multiplet of multiplicity 2 s + 1. The fact that one had to make this additional assumption was, however, a serious disadvantage, as no theoretical reasons to support it could be given. It seemed to show that there were large forces coupling the spin vectors of the electrons in an atom, much larger forces than could be accounted for as due to the interaction of the magnetic moments of the electrons. The position was thus that there was empirical evidence in favour of these large forces, but that their theoretical nature was quite unknown. },
author = {Dirac, Paul A. M.},
doi = {10.1098/rspa.1929.0094},
issn = {0950-1207},
journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
month = {apr},
number = {792},
pages = {714--733},
title = {{Quantum mechanics of many-electron systems}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1983.0054 http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1929.0094},
volume = {123},
year = {1929}
}
@article{DellaSala2016,
abstract = {We present the theory of semilocal exchange-correlation (XC) energy functionals which depend on the Kohn–Sham kinetic energy density (KED), including the relevant class of meta-generalized gradient approximation (meta-GGA) functionals. Thanks to the KED ingredient, meta-GGA functionals can satisfy different exact constraints for XC energy and can be made one-electron self-correlation free. This leads to a better accuracy over a wider range of properties with respect to GGAs, often reaching the accuracy of hybrid functionals, but at much reduced computational cost. An extensive survey of the relevant literature on existing KED dependent XC functionals is provided, considering nonempirical, semi-empirical, and fully empirical ones. A deeper analysis and a wide benchmark are presented for functionals derived considering only exact constraints and parameters obtained from model and/or atomic systems.},
author = {{Della Sala}, Fabio and Fabiano, Eduardo and Constantin, Lucian A.},
doi = {10.1002/qua.25224},
issn = {1097461X},
journal = {International Journal of Quantum Chemistry},
keywords = {density functional theory,kinetic energy,meta-GGA},
month = {nov},
number = {22},
pages = {1641--1694},
title = {{Kinetic-energy-density dependent semilocal exchange-correlation functionals}},
url = {http://doi.wiley.com/10.1002/qua.25224},
volume = {116},
year = {2016}
}
@article{Seko2014,
abstract = {A combination of systematic density-functional theory (DFT) calculations and machine learning techniques has a wide range of potential applications. This study presents an application of the combination of systematic DFT calculations and regression techniques to the prediction of the melting temperature for single and binary compounds. Here we adopt the ordinary least-squares regression, partial least-squares regression, support vector regression, and Gaussian process regression. Among the four kinds of regression techniques, SVR provides the best prediction. The inclusion of physical properties computed by the DFT calculation to a set of predictor variables makes the prediction better. In addition, limitation of the predictive power is shown when extrapolation from the training dataset is required. Finally, a simulation to find the highest melting temperature toward the efficient materials design using kriging is demonstrated. The kriging design finds the compound with the highest melting temperature much faster than random designs. This result may stimulate the application of kriging to efficient materials design for a broad range of applications. {\textcopyright} 2014 American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1310.1546},
author = {Seko, Atsuto and Maekawa, Tomoya and Tsuda, Koji and Tanaka, Isao},
doi = {10.1103/PhysRevB.89.054303},
eprint = {1310.1546},
issn = {10980121},
journal = {Physical Review B - Condensed Matter and Materials Physics},
keywords = {64.70.dj,89.20.Ff},
number = {5},
pages = {1--9},
title = {{Machine learning with systematic density-functional theory calculations: Application to melting temperatures of single- and binary-component solids}},
volume = {89},
year = {2014}
}
@book{minsky69perceptrons,
abstract = {Perceptrons were invented in the fifties when “learning machine” was an exciting new concept. For a decade thereafter, there has been much describing, experimenting, and speculating about what perceptrons can and cannot do. Discussions of this topic were typically lively and vague, because the underlying model and the concepts used were rarely completely defined. Copyright {\textcopyright} 1969 by The Institute of Electrical and Electronics Engineers, Inc.},
address = {Cambridge, MA, USA},
author = {Nievergelt, J.},
booktitle = {IEEE Transactions on Computers},
doi = {10.1109/T-C.1969.222718},
issn = {00189340},
keywords = {linear-classification neural-networks seminal},
number = {6},
pages = {572},
publisher = {MIT Press},
title = {{R69-13 Perceptrons: An Introduction to Computational Geometry}},
volume = {C-18},
year = {1969}
}
@article{Weiss1992,
abstract = {Arthur Lee Samuel's (1901-90) early life, education, and career are described. Before World War II, at Bell Telephone Laboratories, he was a leading designer of microwave tubes, of which his TR radar switch, the Samuel tube, was the most widely used. At the University of Illinois he launched the ILLIAC team. He was one of those who guided IBM into computers and into real research, and he initiated its solid-state laboratory. He made a major improvement in the Williams storage tube. He invented hashing. He was chairman of the Defense Department Advisory Group on Electron Devices for 18 years. He started IBMs Zurich Laboratory and was instrumental in founding the IBM Journal of Research and Development},
author = {Weiss, E.A.},
doi = {10.1109/85.150082},
issn = {1058-6180},
journal = {IEEE Annals of the History of Computing},
number = {3},
pages = {55--69},
title = {{Biographies: Eloge: Arthur Lee Samuel (1901-90)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=150082},
volume = {14},
year = {2002}
}
@article{Bengio2012,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. {\textcopyright} 1979-2012 IEEE.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
month = {jun},
number = {8},
pages = {1798--1828},
pmid = {23787338},
title = {{Representation learning: A review and new perspectives}},
url = {http://arxiv.org/abs/1206.5538},
volume = {35},
year = {2013}
}
@article{Lecun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@article{Ryczko2017,
abstract = {We introduce a new method, called CNNAS (convolutional neural networks for atomistic systems), for calculating the total energy of atomic systems which rivals the computational cost of empirical potentials while maintaining the accuracy of ab initio calculations. This method uses deep convolutional neural networks (CNNs), where the input to these networks are simple representations of the atomic structure. We use this approach to predict energies obtained using density functional theory (DFT) for 2D hexagonal lattices of various types. Using a dataset consisting of graphene, hexagonal boron nitride (hBN), and graphene-hBN heterostructures, with and without defects, we trained a deep CNN that is capable of predicting DFT energies to an extremely high accuracy, with a mean absolute error (MAE) of 0.198 meV/atom (maximum absolute error of 16.1 meV/atom). To explore our new methodology, we investigate the ability of a deep neural network (DNN) in predicting a Lennard-Jones energy and separation distance for a dataset of dimer molecules in both two and three dimensions. In addition, we systematically investigate the flexibility of the deep learning models by performing interpolation and extrapolation tests.},
archivePrefix = {arXiv},
arxivId = {1706.09496},
author = {Ryczko, Kevin and Mills, Kyle and Luchak, Iryna and Homenick, Christa and Tamblyn, Isaac},
doi = {10.1016/j.commatsci.2018.03.005},
eprint = {1706.09496},
issn = {09270256},
journal = {Computational Materials Science},
keywords = {2D materials,Convolutional neural networks,Deep learning,Density functional theory,Dimer molecules},
month = {jun},
pages = {134--142},
title = {{Convolutional neural networks for atomistic systems}},
url = {http://arxiv.org/abs/1706.09496},
volume = {149},
year = {2018}
}
@article{Portman2016,
abstract = {In this paper, we build and explore supervised learning models of ferromagnetic system behavior, using Monte-Carlo sampling of the spin configuration space generated by the 2D Ising model. Given the enormous size of the space of all possible Ising model realizations, the question arises as to how to choose a reasonable number of samples that will form physically meaningful and non-intersecting training and testing datasets. Here, we propose a sampling technique called “ID-MH” that uses the Metropolis–Hastings algorithm creating Markov process across energy levels within the predefined configuration subspace. We show that application of this method retains phase transitions in both training and testing datasets and serves the purpose of validation of a machine learning algorithm. For larger lattice dimensions, ID-MH is not feasible as it requires knowledge of the complete configuration space. As such, we develop a new “block-ID” sampling strategy: it decomposes the given structure into square blocks with lattice dimension N≤5 and uses ID-MH sampling of candidate blocks. Further comparison of the performance of commonly used machine learning methods such as random forests, decision trees, k nearest neighbors and artificial neural networks shows that the PCA-based Decision Tree regressor is the most accurate predictor of magnetizations of the Ising model. For energies, however, the accuracy of prediction is not satisfactory, highlighting the need to consider more algorithmically complex methods (e.g., deep learning).},
archivePrefix = {arXiv},
arxivId = {1611.05891},
author = {Portman, Nataliya and Tamblyn, Isaac},
doi = {10.1016/j.jcp.2017.06.045},
eprint = {1611.05891},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Ising model,Machine learning,Monte-Carlo sampling},
month = {jul},
pages = {871--890},
title = {{Sampling algorithms for validation of supervised learning models for Ising-like systems}},
url = {http://arxiv.org/abs/1611.05891 http://linkinghub.elsevier.com/retrieve/pii/S0021999117304990},
volume = {350},
year = {2017}
}
@article{Mnih2013,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://www.nature.com/doifinder/10.1038/nature14236 http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@misc{Pfau2019,
abstract = {Given access to accurate solutions of the many-electron Schr{\"{o}}dinger equation, nearly all chemistry could be derived from first principles. Exact wavefunctions of interesting chemical systems are out of reach because they are NP-hard to compute in general, but approximations can be found using polynomially-scaling algorithms. The key challenge for many of these algorithms is the choice of wavefunction approximation, or Ansatz, which must trade off between efficiency and accuracy. Neural networks have shown impressive power as accurate practical function approximators and promise as a compact wavefunction Ansatz for spin systems, but problems in electronic structure require wavefunctions that obey Fermi-Dirac statistics. Here we introduce a novel deep learning architecture, the Fermionic Neural Network, as a powerful wavefunction Ansatz for many-electron systems. The Fermionic Neural Network is able to achieve accuracy beyond other variational Monte Carlo Ans{\"{a}}tze on a variety of atoms and small molecules. Using no data other than atomic positions and charges, we predict the dissociation curves of the nitrogen molecule and hydrogen chain, two challenging strongly-correlated systems, to significantly higher accuracy than the coupled cluster method, widely considered the gold standard for quantum chemistry. This demonstrates that deep neural networks can outperform existing ab-initio quantum chemistry methods, opening the possibility of accurate direct optimisation of wavefunctions for previously intractable molecules and solids.},
archivePrefix = {arXiv},
arxivId = {1909.02487},
author = {Pfau, David and Spencer, James S. and Alexander, Alexander G. and Foulkes, W. M.C.},
booktitle = {arXiv},
doi = {10.1103/physrevresearch.2.033429},
eprint = {1909.02487},
file = {::},
issn = {23318422},
keywords = {doi:10.1103/PhysRevResearch.2.033429 url:https://d},
month = {sep},
number = {3},
pages = {033429},
publisher = {arXiv},
title = {{Ab-initio solution of the many-electron schr{\"{o}}dinger equation with deep neural networks}},
url = {https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.033429},
volume = {2},
year = {2019}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {08997667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Mnih2013,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://www.nature.com/doifinder/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Metropolis1953,
abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion.},
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
doi = {10.1063/1.1699114},
issn = {00219606},
journal = {The Journal of Chemical Physics},
month = {jun},
number = {6},
pages = {1087--1092},
title = {{Equation of state calculations by fast computing machines}},
url = {http://aip.scitation.org/doi/10.1063/1.1699114},
volume = {21},
year = {1953}
}
@article{Arsenault2014,
abstract = {Machine learning methods are applied to finding the Green's function of the Anderson impurity model, a basic model system of quantum many-body condensed-matter physics. Different methods of parametrizing the Green's function are investigated; a representation in terms of Legendre polynomials is found to be superior due to its limited number of coefficients and its applicability to state of the art methods of solution. The dependence of the errors on the size of the training set is determined. The results indicate that a machine learning approach to dynamical mean-field theory may be feasible.},
archivePrefix = {arXiv},
arxivId = {1408.1143},
author = {Arsenault, Louis Fran{\c{c}}ois and Lopez-Bezanilla, Alejandro and {Von Lilienfeld}, O. Anatole and Millis, Andrew J.},
doi = {10.1103/PhysRevB.90.155136},
eprint = {1408.1143},
issn = {1550235X},
journal = {Physical Review B - Condensed Matter and Materials Physics},
month = {oct},
number = {15},
pages = {155136},
title = {{Machine learning for many-body physics: The case of the Anderson impurity model}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.90.155136},
volume = {90},
year = {2014}
}
@article{Brockman2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@inproceedings{Szegedy2014,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
keywords = {GoogLeNet},
mendeley-tags = {GoogLeNet},
month = {jun},
pages = {1--9},
pmid = {24920543},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
url = {http://ieeexplore.ieee.org/document/7298594/},
volume = {07-12-June},
year = {2015}
}
@book{Press2007,
abstract = {This study used a laboratory experiment with monetary incentives to test the impact of three personal factors (moral reasoning, value orientation and risk preference), and three situational factors (the presence/absence of audits, tax inequity, and peer reporting behavior), while controlling for the impact of other demographic characteristics, on tax compliance. Analysis of Covariance (ANCOVA) reveals that all the main effects analyzed are statistically significant and robustly influence tax compliance behavior. These results highlight the importance of obtaining a proper understanding of these factors for developing effective policies for increasing the level of compliance, and indicate that standard enforcement polices based on punishment alone should be supplemented by an information system that would acquaint tax payers with the compliance level of other tax payers; reinforce the concept of fairness of the tax system among tax payers; and develop programs that enhance and appeal to a taxpayer's moral conscience and reinforce social cohesion.},
address = {New York, NY, USA},
author = {Frolkovi{\v{c}}, Peter},
booktitle = {Acta Applicandae Mathematicae},
doi = {10.1007/bf01321860},
edition = {3},
isbn = {9780521880688},
issn = {0167-8019},
number = {3},
pages = {297--299},
publisher = {Cambridge University Press},
title = {{Numerical recipes: The art of scientific computing}},
url = {http://books.google.com/books?hl=en&lr=&id=1aAOdzK3FegC&pgis=1%5Cnhttp://books.google.com/books?hl=en&lr=&id=1aAOdzK3FegC&oi=fnd&pg=PA1&dq=Numerical+Recipes+3rd+Edition:+The+Art+of+Scientific+Computing&ots=3iUpDfIuki&sig=7OunhOxJ3NWR8JdLQtVAYfJ3QFQ%5Cnhtt},
volume = {19},
year = {1990}
}
@article{Li2013,
abstract = {The paper suggests a new method that combines the Kennard and Stone algorithm (Kenstone, KS), hierarchical clustering (HC), and ant colony optimization (ACO)-based extreme learning machine (ELM) (KS-HC/ACO-ELM) with the density functional theory (DFT) B3LYP/6-31G(d) method to improve the accuracy of DFT calculations for the Y-NO homolysis bond dissociation energies (BDE). In this method, Kenstone divides the whole data set into two parts, the training set and the test set; HC and ACO are used to perform the cluster analysis on molecular descriptors; correlation analysis is applied for selecting the most correlated molecular descriptors in the classes, and ELM is the nonlinear model for establishing the relationship between DFT calculations and homolysis BDE experimental values. The results show that the standard deviation of homolysis BDE in the molecular test set is reduced from 4.03 kcal mol-1 calculated by the DFT B3LYP/6-31G(d) method to 0.30, 0.28, 0.29, and 0.32 kcal mol-1 by the KS-ELM, KS-HC-ELM, and KS-ACO-ELM methods and the artificial neural network (ANN) combined with KS-HC, respectively. This method predicts accurate values with much higher efficiency when compared to the larger basis set DFT calculation and may also achieve similarly accurate calculation results for larger molecules. {\textcopyright} 2013 Hong Zhi Li et al.},
author = {Li, Hong Zhi and Li, Lin and Zhong, Zi Yan and Han, Yi and Hu, Lihong and Lu, Ying Hua},
doi = {10.1155/2013/860357},
issn = {1024123X},
journal = {Mathematical Problems in Engineering},
pages = {1--10},
title = {{An accurate and efficient method to predict Y-NO bond homolysis bond dissociation energies}},
url = {http://www.hindawi.com/journals/mpe/2013/860357/},
volume = {2013},
year = {2013}
}
@article{Behler2016,
abstract = {Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.},
author = {Behler, J{\"{o}}rg},
doi = {10.1063/1.4966192},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {17},
pages = {170901},
pmid = {27825224},
title = {{Perspective: Machine learning potentials for atomistic simulations}},
url = {http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966192},
volume = {145},
year = {2016}
}
@techreport{Srivastava2014a,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {::},
issn = {15337928},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
pages = {1929--1958},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
volume = {15},
year = {2014}
}
@article{Aramon2019,
abstract = {The Fujitsu Digital Annealer is designed to solve fully connected quadratic unconstrained binary optimization (QUBO) problems. It is implemented on application-specific CMOS hardware and currently solves problems of up to 1,024 variables. The Digital Annealer's algorithm is currently based on simulated annealing; however, it differs from it in its utilization of an efficient parallel-trial scheme and a dynamic escape mechanism. In addition, the Digital Annealer exploits the massive parallelization that custom application-specific CMOS hardware allows. We compare the performance of the Digital Annealer to simulated annealing and parallel tempering with isoenergetic cluster moves on two-dimensional and fully connected spin-glass problems with bimodal and Gaussian couplings. These represent the respective limits of sparse vs. dense problems, as well as high-degeneracy vs. low-degeneracy problems. Our results show that the Digital Annealer currently exhibits a time-to-solution speedup of roughly two orders of magnitude for fully connected spin-glass problems with bimodal or Gaussian couplings, over the single-core implementations of simulated annealing and parallel tempering Monte Carlo used in this study. The Digital Annealer does not appear to exhibit a speedup for sparse two-dimensional spin-glass problems, which we explain on theoretical grounds. We also benchmarked an early implementation of the Parallel Tempering Digital Annealer. Our results suggest an improved scaling over the other algorithms for fully connected problems of average difficulty with bimodal disorder. The next generation of the Digital Annealer is expected to be able to solve fully connected problems up to 8,192 variables in size. This would enable the study of fundamental physics problems and industrial applications that were previously inaccessible using standard computing hardware or special-purpose quantum annealing machines.},
archivePrefix = {arXiv},
arxivId = {1806.08815},
author = {Aramon, Maliheh and Rosenberg, Gili and Valiante, Elisabetta and Miyazawa, Toshiyuki and Tamura, Hirotaka and Katzgraber, Helmut G.},
doi = {10.3389/fphy.2019.00048},
eprint = {1806.08815},
issn = {2296424X},
journal = {Frontiers in Physics},
keywords = {Benchmarking,Custom application-specific CMOS hardware,Digital Annealer,Monte Carlo simulation,Optimization},
number = {APR},
title = {{Physics-inspired optimization for quadratic unconstrained problems using a digital Annealer}},
volume = {7},
year = {2019}
}
@article{Haarnoja2018b,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy-that is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actorcritic formulation, our method achieves state-ofthe-art performance on a range of continuous control benchmark tasks, outperforming prior onpolicy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {::},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
month = {jan},
pages = {2976--2989},
publisher = {International Machine Learning Society (IMLS)},
title = {{Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor}},
url = {http://arxiv.org/abs/1801.01290},
volume = {5},
year = {2018}
}
@techreport{Baker2019,
abstract = {Scientific Machine Learning (SciML) and Artificial Intelligence (AI) will have broad use and transformative effects across the Department of Energy. Accordingly, the January 2018 Basic Research Needs workshop identified six Priority Research Directions (PRDs). The first three PRDs describe foundational research themes that correspond to the need for domain-awareness (PRD #1), interpretability (PRD #2), and robustness (PRD #3). The other three PRDs describe capability research themes and correspond to the three major use cases for massive scientific data analysis (PRD #4), machine learning-enhanced modeling and simulation (PRD #5), and intelligent automation and decision-support for complex systems (PRD #6). The Priority Research Directions provide a sound basis for a coherent, long-term research and development strategy in SciML and AI. Over the last decade, DOE investments in applied mathematics have laid the groundwork for the type of basic research that will underpin key advances in the six PRDs. Such advances will build on the work from leading researchers in optimization, linear algebra, high-performance solvers and algorithms, multiscale modeling and simulation, complex systems research, uncertainty quantification, and the new basic research areas that will emerge from the pursuit of transformative technologies.},
author = {Baker, Nathan and Lee, Steven and Alexander, Frank and Bremer, Timo and Hagberg, Aric and Kevrekidis, Yannis and Najm, Habib and Parashar, Manish and Patra, Abani and Sethian, James and Wild, Stefan and Willcox, Karen and Lee, Steven},
doi = {10.2172/1484362},
institution = {U.S. Department of Energy, Office of Science},
title = {{Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence}},
url = {http://www.osti.gov/servlets/purl/1484362/%0Ahttp://www.osti.gov/servlets/purl/1478744/},
year = {2019}
}
@article{Silver2017,
abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
isbn = {3013372370},
issn = {14764687},
journal = {Nature},
keywords = {AlphaGo Zero},
mendeley-tags = {AlphaGo Zero},
number = {7676},
pages = {354--359},
pmid = {29052630},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@article{Cherukara2016,
abstract = {We introduce a bond order potential (BOP) for stanene based on an ab initio derived training data set. The potential is optimized to accurately describe the energetics, as well as thermal and mechanical properties of a free-standing sheet, and used to study diverse nanostructures of stanene, including tubes and ribbons. As a representative case study, using the potential, we perform molecular dynamics simulations to study stanene's structure and temperature-dependent thermal conductivity. We find that the structure of stanene is highly rippled, far in excess of other 2-D materials (e.g., graphene), owing to its low in-plane stiffness (stanene: ∼ 25 N/m; graphene: ∼ 480 N/m). The extent of stanene's rippling also shows stronger temperature dependence compared to that in graphene. Furthermore, we find that stanene based nanostructures have significantly lower thermal conductivity compared to graphene based structures owing to their softness (i.e., low phonon group velocities) and high anharmonic response. Our newly developed BOP will facilitate the exploration of stanene based low dimensional heterostructures for thermoelectric and thermal management applications.},
author = {Cherukara, Mathew J. and Narayanan, Badri and Kinaci, Alper and Sasikumar, Kiran and Gray, Stephen K. and Chan, Maria K.Y. and Sankaranarayanan, Subramanian K.R.S.},
doi = {10.1021/acs.jpclett.6b01562},
issn = {19487185},
journal = {Journal of Physical Chemistry Letters},
month = {oct},
number = {19},
pages = {3752--3759},
pmid = {27569053},
title = {{Ab Initio-Based Bond Order Potential to Investigate Low Thermal Conductivity of Stanene Nanostructures}},
url = {http://pubs.acs.org/doi/abs/10.1021/acs.jpclett.6b01562},
volume = {7},
year = {2016}
}
@article{Tiunov2019,
abstract = {The coherent Ising machine (CIM) enables efficient sampling of low-lying energy states of the Ising Hamiltonian with all-to-all connectivity by encoding the spins in the amplitudes of pulsed modes in an optical parametric oscillator (OPO). The interaction between the pulses is realized by means of measurement-based optoelectronic feedforward which enhances the gain for lower-energy spin configurations. We present an efficient method of simulating the CIM on a classical computer that outperforms the CIM itself as well as the noisy mean-field annealer in terms of both the quality of the samples and the computational speed. It is furthermore advantageous with respect to the CIM in that it can handle Ising Hamiltonians with arbitrary real-valued node coupling strengths. These results illuminate the nature of the faster performance exhibited by the CIM and may give rise to a new class of quantum-inspired algorithms of classical annealing that can successfully compete with existing methods.},
archivePrefix = {arXiv},
arxivId = {1901.08927},
author = {Tiunov, Egor S. and Ulanov, Alexander E. and Lvovsky, A. I.},
doi = {10.1364/oe.27.010288},
eprint = {1901.08927},
issn = {23318422},
journal = {arXiv},
keywords = {Computer simulation; Continuous variables; Neural},
month = {apr},
number = {7},
pages = {10288--10295},
pmid = {31045172},
publisher = {OSA},
title = {{Annealing by simulating the coherent Ising machine}},
url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-27-7-10288},
volume = {27},
year = {2019}
}
@article{Brooks1983,
abstract = {CHARMM (Chemistry at HARvard Macromolecular Mechanics) is a highly flexible computer program which uses empirical energy functions to model macromolecular systems. The program can read or model build structures, energy minimize them by first‐ or second‐derivative techniques, perform a normal mode or molecular dynamics simulation, and analyze the structural, equilibrium, and dynamic properties determined in these calculations. The operations that CHARMM can perform are described, and some implementation details are given. A set of parameters for the empirical energy function and a sample run are included. Copyright {\textcopyright} 1983 John Wiley & Sons, Inc.},
author = {Brooks, Bernard R. and Bruccoleri, Robert E. and Olafson, Barry D. and States, David J. and Swaminathan, S. and Karplus, Martin},
doi = {10.1002/jcc.540040211},
isbn = {0192-8651},
issn = {1096987X},
journal = {Journal of Computational Chemistry},
number = {2},
pages = {187--217},
pmid = {1},
title = {{CHARMM: A program for macromolecular energy, minimization, and dynamics calculations}},
volume = {4},
year = {1983}
}
@article{Ising1925,
author = {Ising, Ernst},
doi = {10.1007/BF02980577},
issn = {00443328},
journal = {Zeitschrift f{\"{u}}r Physik},
number = {1},
pages = {253--258},
title = {{Beitrag zur Theorie des Ferromagnetismus}},
volume = {31},
year = {1925}
}
@phdthesis{Hochreiter1991,
abstract = {We introduce the “exponential linear unit” (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.},
archivePrefix = {arXiv},
arxivId = {1511.07289},
author = {Clevert, Djork Arn{\'{e}} and Unterthiner, Thomas and Hochreiter, Sepp},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.07289},
issn = {18168957 18163459},
pages = {1--71},
school = {TU Munich},
title = {{Fast and accurate deep network learning by exponential linear units (ELUs)}},
type = {Diploma Thesis},
url = {http://people.idsia.ch/$\sim$juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf},
year = {2016}
}
@misc{stable-baselines,
abstract = {Stable Baselines is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines.},
author = {{Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu}, Yuhuai},
booktitle = {GitHub repository},
howpublished = {\url{https://github.com/hill-a/stable-baselines}},
publisher = {GitHub},
title = {{Github - Stable Baselines}},
url = {https://github.com/hill-a/stable-baselines},
year = {2018}
}
@article{OpenAI2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems.},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
eprint = {1910.07113},
issn = {23318422},
journal = {arXiv},
pages = {1--51},
title = {{Solving Rubik's cube with a robot hand}},
url = {http://arxiv.org/abs/1910.07113},
year = {2019}
}
@article{Simonyan2013,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
file = {:home/kmills/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Workshop Track Proceedings},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Deep inside convolutional networks: Visualising image classification models and saliency maps}},
url = {http://arxiv.org/abs/1312.6034},
year = {2014}
}
@article{Baehrens2009,
abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method. {\textcopyright} 2010 David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen and Klaus-Robert M{\"{u}}ller.},
archivePrefix = {arXiv},
arxivId = {0912.1128},
author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"{u}}ller, Klaus Robert},
eprint = {0912.1128},
file = {:home/kmills/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baehrens et al. - 2010 - How to Explain Individual Classification Decisions Timon Schroeter Klaus-Robert M ¨ uller.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Ames mutagenicity,Black box model,Explaining,Kernel methods,Nonlinear},
month = {dec},
pages = {1803--1831},
title = {{How to explain individual classification decisions}},
url = {http://arxiv.org/abs/0912.1128},
volume = {11},
year = {2010}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {::},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
month = {apr},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
url = {http://www.idsia.ch/˜juergen/DeepLearning8Oct2014.texCompleteBIBTEXfile http://arxiv.org/abs/1404.7828 http://dx.doi.org/10.1016/j.neunet.2014.09.003},
volume = {61},
year = {2015}
}
